{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 200px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies des grosses data](https://github.com/wikistat/Ateliers-Big-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistique élémentaire avec <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"Spark\"/> </a> et  [MLlib](https://spark.apache.org/mllib/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résumé**: Ce tutoriel continue l'initiation à [Spark](https://spark.apache.org/) à l'aide de commandes en Python en utilisant l'API  [`PySpark`](http://spark.apache.org/docs/latest/api/python/); échantillonnage d'une RDD ; présentation de la libriaire MLlib, statistique exploratoire rudimentaire uni et bidimensionnelles, régression logistique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Lecture des données\n",
    "Ce tutoriel s'inspire de ceux proposés par [J. A. Dianes](https://github.com/jadianes/spark-py-notebooks) pour l'utilisation des données du concours [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) concernant près de 9M d'interactions dans un réseau. Elles sont décrites en détail [ici](http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names). L'objectif est d'apprendre à détecter des intrusions dans un réseau à partir d'un ensemble de variables ou *features* déjà calculées sur chaque transaction ou ineraction avec le réseau.\n",
    "\n",
    "Un sous-échantillon est chargé localement avant de créer la RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.32.0.70:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Chargement du fichier\n",
    "#Renseignez ici le dossier ou vous souhaitez stocker le fichier téléchargé.\n",
    "DATA_PATH=\"/Users/bguillouet/Insa/TP_Insa/data/\" \n",
    "#Python 2\n",
    "#import urllib\n",
    "#f = urllib.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\",\"kddcup.data_10_percent.gz\")\n",
    "#Python 3\n",
    "import urllib.request\n",
    "f = urllib.request.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\",DATA_PATH+\"/kddcup.data_10_percent.gz\")\n",
    "data_file = DATA_PATH+\"/kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Echantillonnage de RDDs\n",
    "**Attention**, il y a deux opérations disponibles en Spark: la *transformation* `sample` et l'*action* `takeSample`. Il est donc possible de déclarer une séquence de transformations incluant un éhchantillonnage aléatoire simple ou encore d'extraire par une *action* un échantillon d'une RDD qui sera chargée en mémoire avant utilisation par une autre librairie comme par exemple `Scikit-learn` de Pyhton. Bien faire la différence entre *transformation* et *action*\n",
    "\n",
    "### 2.1 La transformation `sample`\n",
    "\n",
    "Elle admet jusqu'à trois paramètres, le premier indique si l'échantillonnage est avec remplacement ou non, le deuxième est une fraction d'échantillonnage, le troisième est optionnel pour l'initialisation du générateur (*random seed*). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size is 49493 of 494021\n"
     ]
    }
   ],
   "source": [
    "raw_data_sample = raw_data.sample(False, 0.1, 1234)\n",
    "sample_size = raw_data_sample.count()\n",
    "total_size = raw_data.count()\n",
    "print(\"Sample size is {} of {}\".format(sample_size, total_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'intérêt de `sample` est de l'inclure dans une séquence de transformations incluant des opérations (*MapReduce*) d'aggrégation ou de sélections par couples (clef, valeurs).\n",
    "\n",
    "L'exemple ci-dessous estime, sur un sous-échantillon donc plus rapidement la proportion d'interactions normales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le taux d interactions normales  est 0.195\n",
      "Calcul en 1.094 secondes\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "# transformations à appliquer\n",
    "raw_data_sample_items = raw_data_sample.map(lambda x: x.split(\",\"))\n",
    "sample_normal_tags = raw_data_sample_items.filter(lambda x: \"normal.\" in x)\n",
    "# actions + time\n",
    "t0 = time()\n",
    "sample_normal_tags_count = sample_normal_tags.count()\n",
    "tt = time() - t0\n",
    "# Calcul du taux\n",
    "sample_normal_ratio = sample_normal_tags_count / float(sample_size)\n",
    "print(\"Le taux d interactions normales  est {}\".format(round(sample_normal_ratio,3)))\n",
    "print(\"Calcul en {} secondes\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Même chose sans échantillonnage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le taux d interactions normales  est 0.197\n",
      "Calcul en 2.323 secondes\n"
     ]
    }
   ],
   "source": [
    "# transformations \n",
    "raw_data_items = raw_data.map(lambda x: x.split(\",\"))\n",
    "normal_tags = raw_data_items.filter(lambda x: \"normal.\" in x)\n",
    "# actions + time\n",
    "t0 = time()\n",
    "normal_tags_count = normal_tags.count()\n",
    "tt = time() - t0\n",
    "# Calcul\n",
    "normal_ratio = normal_tags_count / float(total_size)\n",
    "print(\"Le taux d interactions normales  est {}\".format(round(normal_ratio,3)) )\n",
    "print(\"Calcul en {} secondes\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 L'action `takeSample`\n",
    "permet d'extraire un échantillon aléatoire simple d'une RDD en le chargeant en mémoire avant utilisation par une librairie hors Spark.\n",
    "\n",
    "La syntaxe est similaire mais en spécifiant une taille d'échantillon plutôt qu'un taux d'échantillonnage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le taux d interactions normales  est 0.1967175\n",
      "Calcul en 3.257 secondes\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "raw_data_sample = raw_data.takeSample(False, 400000, 1234)\n",
    "normal_data_sample = [x.split(\",\") for x in raw_data_sample if \"normal.\" in x]\n",
    "tt = time() - t0\n",
    "\n",
    "normal_sample_size = len(normal_data_sample)\n",
    "\n",
    "normal_ratio = normal_sample_size / 400000.0\n",
    "print(\"Le taux d interactions normales  est {}\".format(normal_ratio))\n",
    "print(\"Calcul en {} secondes\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seule la phase d'échantillonnage est distribuée / parallélisée, cette procédure prend plus de temps sur un cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Présentation de [MLlib](http://spark.apache.org/mllib/)\n",
    "\n",
    "### 3.1 Préparation des données\n",
    "Comme cela est déjà largement expliqué dans le [tutoriel](https://github.com/wikistat/Intro-Python) consacré au trafic (munging) des données avec Python `pandas`, leur préparation est une étape essentielle à la qualité des analyses et modélisations qui en découlent. Extraction, filtrage, échantillonnage, complétion des données manquantes, correction, détection d'anomalies ou atypiques, jointures, agrégations ou cumuls, transformations (recodage, discrétisation, réduction, \"normalisation\"...),  sélection des variables ou *features*, recalages d'images de signaux... sont les principales procédures à mettre en oeuve et de façon itérative avec les étapes d'apprentissage visant les objectifs de l'étude.\n",
    "\n",
    "Par principe, la plupart de ces étapes, unidimensionnelles, se distribuent naturellement sur les noeuds d'un cluster en exécutant des transformations *MapReduce* et en utilisant les commandes Spark ou des fonctions spécifiques des librairies *MLlib* ou *SparkSQL*.\n",
    "\n",
    "**N.B.** Il est fréquent, qu'une fois préparées, les données ne soient pas si massives, ou encore il est pertinent d'estimer un modèle sur un échantillon plutôt que sur un corpus très volumineux. Néanmoins, il est important de savoir manipuler des volumes et flux importants, notamment dans l'environnement Spark, avant de passer à la phase d'apprentissage ou modélisation.\n",
    "\n",
    "### 3.2 Fonctionnalités de [MLlib](http://spark.apache.org/docs/latest/ml-guide.html)\n",
    "Dans un environnement en pleine évolution, seule la [documentation en ligne](https://spark.apache.org/docs/latest/mllib-guide.html) fait référence. A partir de la version 2.0 de Spark, MLlib évollue en intégrant le type `data frame` de SparkSQL, objet d'un autre tutoriel. MLllib reste néanmoins utilisée pour produire des statistiques élémentaires sur des RDDs. Les traitements plus élaborés d'apprentissage statistique sont détaillés dans les ateliers spécifiques.\n",
    "\n",
    "Fonctionnalités de MLlib:\n",
    "- *Statistique de base*: Univariée, corrélation, échantillonnage stratifié, tests d'hypothèse, générateurs aléatoires, transformation (standardisation, quantification de textes avec TF-IDF et vectorisation), sélection (chi2) de variables (*features*).\n",
    "- *Exploration multidimensionnelle* Classification non-supervisée (k-means avec version en ligne, modèles de mélanges gaussiens, LDA ou *Latent Derichlet Allocation*, réduction de dimension (SVD et ACP mais en java ou scala pas en python), factorisation non négative (NMF) par moindres carrés alternés (ALS).\n",
    "- *Apprentissage* Méthodes linéaires: SVM, régression gaussienne et binomiale ou logistique avec pénalisation l1 ou l2; estimation par gradient stochastique, ou L-BFGS; classifieur bayésien naïf, arbre de décision, forêts aléatoires, boosting (*gradient boosting machine*).\n",
    "\n",
    "\n",
    "### 3.3 Types de données\n",
    "La librairie MLlib manipule exclusivement des RDDs (des data frames avec les développements en cours) de différents types ou classes dont: vecteurs denses ou creux,  `LabeledPOint` pour les algorithmes d'apprentissage ,  `rating` pour les systèmes de recommandation, `Model` pour exploiter un modèle sur des données (prévision). L'arrivée de la classe `DataFrame`modifie en profondeur les usages des types de données. Le développement de cette section reste volontairement limité dans l'attente des nouvelles versions. \n",
    "#### Vecteurs denses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pyspark.ml.linalg import Vectors\n",
    "# vecteur \"dense\"\n",
    "# à partir de numpy\n",
    "denseVec1=array([1.0,0.0,2.0,4.0,0.0])\n",
    "# en utilisant la classe Vectors\n",
    "denseVec2=Vectors.dense([1.0,0.0,2.0,4.0,0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vecteurs creux (*sparse*)\n",
    "Seules les valeurs non nulles sont identifiées et stockées. Il faut préciser la taille du vecteur et les coordonnées de ces valeurs non nulles. C'est défini par un dictionnaire ou par une liste d'indices et de valeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparseVec1 = Vectors.sparse(5, {0: 1.0, 2: 2.0, 3: 4.0})\n",
    "sparseVec2 = Vectors.sparse(5, [0, 2, 3], [1.0, 2.0, 4.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LabeledPoint\n",
    "Ce type est spécifique aux algorithmes d'apprentissage et associe un \"label\", en fait un réel, à un vecteur dense ou creux. Ce \"label\" est soit la valeur de la variable Y quantitative à modéliser en régression, soit un code de classe: 0.0, 1.0... en classification supervisée ou discrimination. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD de vecteurs denses\n",
    "Données d'interactions représentés par des vecteurs denses à partir du type `array`de *Numpy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# relecture des données\n",
    "data_file = DATA_PATH+\"/kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def parse_interaction(line):\n",
    "    line_split = line.split(\",\")\n",
    "    # keep just numeric and logical values\n",
    "    symbolic_indexes = [1,2,3,41]\n",
    "    clean_line_split = [item for i,item in enumerate(line_split) if i not in symbolic_indexes]\n",
    "    return np.array([float(x) for x in clean_line_split])\n",
    "\n",
    "vector_data = raw_data.map(parse_interaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistiques élémentaires\n",
    "### 4.1 Tendances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib propose des statistiques unidimensionnelles par colonne d'un `RDD[Vector]` avec la fonction [`colStats`](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.stat.Statistics.colStats) accessible dans [`Statistics`](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.stat.Statistics). Cette fonction retourne un [`MultivariateStatisticalSummary`](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.stat.MultivariateStatisticalSummary), qui contient les statistiques *max*, *min*, *moyenne*, *variance*, et *nombre de non nulles*, ainsi que *nombre total*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistique des durées\n",
      " Moyenne: 47.979\n",
      " Ecart type: 707.746\n",
      " Valeur max: 58329.0\n",
      " Valeur min: 0.0\n",
      " Nombre de valeurs: 494021\n",
      " Nombre de valeurs non nulles: 12350.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "from math import sqrt \n",
    "\n",
    "# Compute column summary statistics.\n",
    "summary = Statistics.colStats(vector_data)\n",
    "\n",
    "print(\"Statistique des durées\")\n",
    "print(\" Moyenne: {}\".format(round(summary.mean()[0],3)))\n",
    "print(\" Ecart type: {}\".format(round(sqrt(summary.variance()[0]),3)))\n",
    "print(\" Valeur max: {}\".format(round(summary.max()[0],3)))\n",
    "print(\" Valeur min: {}\".format(round(summary.min()[0],3)))\n",
    "print(\" Nombre de valeurs: {}\".format(summary.count()))\n",
    "print(\" Nombre de valeurs non nulles: {}\".format(summary.numNonzeros()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Pour un *label* donné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration Statistics for label: normal\n",
      " Mean: 216.65732231336938\n",
      " St. deviation: 1359.213\n",
      " Max value: 58329.0\n",
      " Min value: 0.0\n",
      " Total value count: 97278\n",
      " Number of non-zero values: 11690.0\n"
     ]
    }
   ],
   "source": [
    "def parse_interaction_with_key(line):\n",
    "    line_split = line.split(\",\")\n",
    "    # keep just numeric and logical values\n",
    "    symbolic_indexes = [1,2,3,41]\n",
    "    clean_line_split = [item for i,item in enumerate(line_split) if i not in symbolic_indexes]\n",
    "    return (line_split[41], np.array([float(x) for x in clean_line_split]))\n",
    "\n",
    "label_vector_data = raw_data.map(parse_interaction_with_key)\n",
    "# les interactions normales sont filtrées\n",
    "normal_label_data = label_vector_data.filter(lambda x: x[0]==\"normal.\")\n",
    "# Calcul des tendances\n",
    "normal_summary = Statistics.colStats(normal_label_data.values())\n",
    "# Affichage\n",
    "print(\"Duration Statistics for label: {}\".format(\"normal\"))\n",
    "print(\" Mean: {}\".format(normal_summary.mean()[0],3))\n",
    "print(\" St. deviation: {}\".format(round(sqrt(normal_summary.variance()[0]),3)))\n",
    "print(\" Max value: {}\".format(round(normal_summary.max()[0],3)))\n",
    "print(\" Min value: {}\".format(round(normal_summary.min()[0],3)))\n",
    "print(\" Total value count: {}\".format(normal_summary.count()))\n",
    "print(\" Number of non-zero values: {}\".format(normal_summary.numNonzeros()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Pour un ensemble de  *labels*\n",
    "Edition d'une fonction avec un label comme paramètre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summary_by_label(raw_data, label):\n",
    "    label_vector_data = raw_data.map(parse_interaction_with_key).filter(lambda x: x[0]==label)\n",
    "    return Statistics.colStats(label_vector_data.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qui redonne les mêmes résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration Statistics for label: normal\n",
      " Mean: 216.65732231336938\n",
      " St. deviation: 1359.213\n",
      " Max value: 58329.0\n",
      " Min value: 0.0\n",
      " Total value count: 97278\n",
      " Number of non-zero values: 11690.0\n"
     ]
    }
   ],
   "source": [
    "normal_sum = summary_by_label(raw_data, \"normal.\")\n",
    "\n",
    "print(\"Duration Statistics for label: {}\".format(\"normal\"))\n",
    "print(\" Mean: {}\".format(normal_sum.mean()[0],3))\n",
    "print(\" St. deviation: {}\".format(round(sqrt(normal_sum.variance()[0]),3)))\n",
    "print(\" Max value: {}\".format(round(normal_sum.max()[0],3)))\n",
    "print(\" Min value: {}\".format(round(normal_sum.min()[0],3)))\n",
    "print(\" Total value count: {}\".format(normal_sum.count()))\n",
    "print(\" Number of non-zero values: {}\".format(normal_sum.numNonzeros()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquée à un type d'attaque dont une [liste](http://kdd.ics.uci.edu/databases/kddcup99/training_attack_types) est disponible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration Statistics for label: guess_password\n",
      " Mean: 2.7169811320754715\n",
      " St. deviation: 11.88\n",
      " Max value: 60.0\n",
      " Min value: 0.0\n",
      " Total value count: 53\n",
      " Number of non-zero values: 4.0\n"
     ]
    }
   ],
   "source": [
    "guess_passwd_summary = summary_by_label(raw_data, \"guess_passwd.\")\n",
    "\n",
    "print(\"Duration Statistics for label: {}\".format(\"guess_password\"))\n",
    "print(\" Mean: {}\".format(guess_passwd_summary.mean()[0],3))\n",
    "print(\" St. deviation: {}\".format(round(sqrt(guess_passwd_summary.variance()[0]),3)))\n",
    "print(\" Max value: {}\".format(round(guess_passwd_summary.max()[0],3)))\n",
    "print(\" Min value: {}\".format(round(guess_passwd_summary.min()[0],3)))\n",
    "print(\" Total value count: {}\".format(guess_passwd_summary.count()))\n",
    "print(\" Number of non-zero values: {}\".format(guess_passwd_summary.numNonzeros()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durée pour tous les types d'interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_list = [\"back.\",\"buffer_overflow.\",\"ftp_write.\",\"guess_passwd.\",\n",
    "              \"imap.\",\"ipsweep.\",\"land.\",\"loadmodule.\",\"multihop.\",\n",
    "              \"neptune.\",\"nmap.\",\"normal.\",\"perl.\",\"phf.\",\"pod.\",\"portsweep.\",\n",
    "              \"rootkit.\",\"satan.\",\"smurf.\",\"spy.\",\"teardrop.\",\"warezclient.\",\n",
    "              \"warezmaster.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistiques pour chaque type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats_by_label = [(label, summary_by_label(raw_data, label)) for label in label_list]\n",
    "duration_by_label = [ \n",
    "    (stat[0], np.array([float(stat[1].mean()[0]), float(sqrt(stat[1].variance()[0])), float(stat[1].min()[0]), float(stat[1].max()[0]), int(stat[1].count())])) \n",
    "    for stat in stats_by_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mise en forme dans un *data frame* de `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistique de la durée par label\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>back.</th>\n",
       "      <td>0.128915</td>\n",
       "      <td>1.110062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2203.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buffer_overflow.</th>\n",
       "      <td>91.700000</td>\n",
       "      <td>97.514685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>321.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ftp_write.</th>\n",
       "      <td>32.375000</td>\n",
       "      <td>47.449033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guess_passwd.</th>\n",
       "      <td>2.716981</td>\n",
       "      <td>11.879811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imap.</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>14.174240</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ipsweep.</th>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.438439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1247.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>land.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loadmodule.</th>\n",
       "      <td>36.222222</td>\n",
       "      <td>41.408869</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multihop.</th>\n",
       "      <td>184.000000</td>\n",
       "      <td>253.851006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>718.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neptune.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107201.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nmap.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>231.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>normal.</th>\n",
       "      <td>216.657322</td>\n",
       "      <td>1359.213469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58329.0</td>\n",
       "      <td>97278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perl.</th>\n",
       "      <td>41.333333</td>\n",
       "      <td>14.843629</td>\n",
       "      <td>25.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phf.</th>\n",
       "      <td>4.500000</td>\n",
       "      <td>5.744563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pod.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>264.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>portsweep.</th>\n",
       "      <td>1915.299038</td>\n",
       "      <td>7285.125159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42448.0</td>\n",
       "      <td>1040.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rootkit.</th>\n",
       "      <td>100.800000</td>\n",
       "      <td>216.185003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>708.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>satan.</th>\n",
       "      <td>0.040277</td>\n",
       "      <td>0.522433</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1589.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smurf.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280790.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spy.</th>\n",
       "      <td>318.000000</td>\n",
       "      <td>26.870058</td>\n",
       "      <td>299.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teardrop.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>979.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>warezclient.</th>\n",
       "      <td>615.257843</td>\n",
       "      <td>2207.694966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15168.0</td>\n",
       "      <td>1020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>warezmaster.</th>\n",
       "      <td>15.050000</td>\n",
       "      <td>33.385271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Mean      Std Dev    Min      Max     Count\n",
       "back.                0.128915     1.110062    0.0     14.0    2203.0\n",
       "buffer_overflow.    91.700000    97.514685    0.0    321.0      30.0\n",
       "ftp_write.          32.375000    47.449033    0.0    134.0       8.0\n",
       "guess_passwd.        2.716981    11.879811    0.0     60.0      53.0\n",
       "imap.                6.000000    14.174240    0.0     41.0      12.0\n",
       "ipsweep.             0.034483     0.438439    0.0      7.0    1247.0\n",
       "land.                0.000000     0.000000    0.0      0.0      21.0\n",
       "loadmodule.         36.222222    41.408869    0.0    103.0       9.0\n",
       "multihop.          184.000000   253.851006    0.0    718.0       7.0\n",
       "neptune.             0.000000     0.000000    0.0      0.0  107201.0\n",
       "nmap.                0.000000     0.000000    0.0      0.0     231.0\n",
       "normal.            216.657322  1359.213469    0.0  58329.0   97278.0\n",
       "perl.               41.333333    14.843629   25.0     54.0       3.0\n",
       "phf.                 4.500000     5.744563    0.0     12.0       4.0\n",
       "pod.                 0.000000     0.000000    0.0      0.0     264.0\n",
       "portsweep.        1915.299038  7285.125159    0.0  42448.0    1040.0\n",
       "rootkit.           100.800000   216.185003    0.0    708.0      10.0\n",
       "satan.               0.040277     0.522433    0.0     11.0    1589.0\n",
       "smurf.               0.000000     0.000000    0.0      0.0  280790.0\n",
       "spy.               318.000000    26.870058  299.0    337.0       2.0\n",
       "teardrop.            0.000000     0.000000    0.0      0.0     979.0\n",
       "warezclient.       615.257843  2207.694966    0.0  15168.0    1020.0\n",
       "warezmaster.        15.050000    33.385271    0.0    156.0      20.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)\n",
    "stats_by_label_df = pd.DataFrame.from_items(duration_by_label, columns=[\"Mean\", \"Std Dev\", \"Min\", \"Max\", \"Count\"], orient='index')\n",
    "print(\"Statistique de la durée par label\")\n",
    "stats_by_label_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le tout dans une fonction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_variable_stats_df(stats_by_label, column_i):\n",
    "    column_stats_by_label = [\n",
    "        (stat[0], np.array([float(stat[1].mean()[column_i]), float(sqrt(stat[1].variance()[column_i])), float(stat[1].min()[column_i]), float(stat[1].max()[column_i]), int(stat[1].count())])) \n",
    "        for stat in stats_by_label\n",
    "    ]\n",
    "    return pd.DataFrame.from_items(column_stats_by_label, columns=[\"Mean\", \"Std Dev\", \"Min\", \"Max\", \"Count\"], orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qui s'exécute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>back.</th>\n",
       "      <td>0.128915</td>\n",
       "      <td>1.110062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2203.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buffer_overflow.</th>\n",
       "      <td>91.700000</td>\n",
       "      <td>97.514685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>321.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ftp_write.</th>\n",
       "      <td>32.375000</td>\n",
       "      <td>47.449033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guess_passwd.</th>\n",
       "      <td>2.716981</td>\n",
       "      <td>11.879811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imap.</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>14.174240</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ipsweep.</th>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.438439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1247.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>land.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loadmodule.</th>\n",
       "      <td>36.222222</td>\n",
       "      <td>41.408869</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multihop.</th>\n",
       "      <td>184.000000</td>\n",
       "      <td>253.851006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>718.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neptune.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107201.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nmap.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>231.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>normal.</th>\n",
       "      <td>216.657322</td>\n",
       "      <td>1359.213469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58329.0</td>\n",
       "      <td>97278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perl.</th>\n",
       "      <td>41.333333</td>\n",
       "      <td>14.843629</td>\n",
       "      <td>25.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phf.</th>\n",
       "      <td>4.500000</td>\n",
       "      <td>5.744563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pod.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>264.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>portsweep.</th>\n",
       "      <td>1915.299038</td>\n",
       "      <td>7285.125159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42448.0</td>\n",
       "      <td>1040.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rootkit.</th>\n",
       "      <td>100.800000</td>\n",
       "      <td>216.185003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>708.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>satan.</th>\n",
       "      <td>0.040277</td>\n",
       "      <td>0.522433</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1589.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smurf.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280790.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spy.</th>\n",
       "      <td>318.000000</td>\n",
       "      <td>26.870058</td>\n",
       "      <td>299.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teardrop.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>979.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>warezclient.</th>\n",
       "      <td>615.257843</td>\n",
       "      <td>2207.694966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15168.0</td>\n",
       "      <td>1020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>warezmaster.</th>\n",
       "      <td>15.050000</td>\n",
       "      <td>33.385271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Mean      Std Dev    Min      Max     Count\n",
       "back.                0.128915     1.110062    0.0     14.0    2203.0\n",
       "buffer_overflow.    91.700000    97.514685    0.0    321.0      30.0\n",
       "ftp_write.          32.375000    47.449033    0.0    134.0       8.0\n",
       "guess_passwd.        2.716981    11.879811    0.0     60.0      53.0\n",
       "imap.                6.000000    14.174240    0.0     41.0      12.0\n",
       "ipsweep.             0.034483     0.438439    0.0      7.0    1247.0\n",
       "land.                0.000000     0.000000    0.0      0.0      21.0\n",
       "loadmodule.         36.222222    41.408869    0.0    103.0       9.0\n",
       "multihop.          184.000000   253.851006    0.0    718.0       7.0\n",
       "neptune.             0.000000     0.000000    0.0      0.0  107201.0\n",
       "nmap.                0.000000     0.000000    0.0      0.0     231.0\n",
       "normal.            216.657322  1359.213469    0.0  58329.0   97278.0\n",
       "perl.               41.333333    14.843629   25.0     54.0       3.0\n",
       "phf.                 4.500000     5.744563    0.0     12.0       4.0\n",
       "pod.                 0.000000     0.000000    0.0      0.0     264.0\n",
       "portsweep.        1915.299038  7285.125159    0.0  42448.0    1040.0\n",
       "rootkit.           100.800000   216.185003    0.0    708.0      10.0\n",
       "satan.               0.040277     0.522433    0.0     11.0    1589.0\n",
       "smurf.               0.000000     0.000000    0.0      0.0  280790.0\n",
       "spy.               318.000000    26.870058  299.0    337.0       2.0\n",
       "teardrop.            0.000000     0.000000    0.0      0.0     979.0\n",
       "warezclient.       615.257843  2207.694966    0.0  15168.0    1020.0\n",
       "warezmaster.        15.050000    33.385271    0.0    156.0      20.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_variable_stats_df(stats_by_label,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autre exemple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_bytes statistics, by label\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>back.</th>\n",
       "      <td>54156.355878</td>\n",
       "      <td>3.159360e+03</td>\n",
       "      <td>13140.0</td>\n",
       "      <td>54540.0</td>\n",
       "      <td>2203.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buffer_overflow.</th>\n",
       "      <td>1400.433333</td>\n",
       "      <td>1.337133e+03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6274.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ftp_write.</th>\n",
       "      <td>220.750000</td>\n",
       "      <td>2.677476e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guess_passwd.</th>\n",
       "      <td>125.339623</td>\n",
       "      <td>3.037860e+00</td>\n",
       "      <td>104.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imap.</th>\n",
       "      <td>347.583333</td>\n",
       "      <td>6.299260e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1492.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ipsweep.</th>\n",
       "      <td>10.083400</td>\n",
       "      <td>5.231658e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1247.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>land.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loadmodule.</th>\n",
       "      <td>151.888889</td>\n",
       "      <td>1.277453e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multihop.</th>\n",
       "      <td>435.142857</td>\n",
       "      <td>5.409604e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1412.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neptune.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107201.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nmap.</th>\n",
       "      <td>24.116883</td>\n",
       "      <td>5.941987e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>231.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>normal.</th>\n",
       "      <td>1157.047524</td>\n",
       "      <td>3.422612e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2194619.0</td>\n",
       "      <td>97278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perl.</th>\n",
       "      <td>265.666667</td>\n",
       "      <td>4.932883e+00</td>\n",
       "      <td>260.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phf.</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pod.</th>\n",
       "      <td>1462.651515</td>\n",
       "      <td>1.250980e+02</td>\n",
       "      <td>564.0</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>264.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>portsweep.</th>\n",
       "      <td>666707.436538</td>\n",
       "      <td>2.150067e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>693375640.0</td>\n",
       "      <td>1040.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rootkit.</th>\n",
       "      <td>294.700000</td>\n",
       "      <td>5.385782e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1727.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>satan.</th>\n",
       "      <td>1.337319</td>\n",
       "      <td>4.294620e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1710.0</td>\n",
       "      <td>1589.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smurf.</th>\n",
       "      <td>935.772300</td>\n",
       "      <td>2.000224e+02</td>\n",
       "      <td>520.0</td>\n",
       "      <td>1032.0</td>\n",
       "      <td>280790.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spy.</th>\n",
       "      <td>174.500000</td>\n",
       "      <td>8.838835e+01</td>\n",
       "      <td>112.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teardrop.</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>979.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>warezclient.</th>\n",
       "      <td>300219.562745</td>\n",
       "      <td>1.200905e+06</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5135678.0</td>\n",
       "      <td>1020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>warezmaster.</th>\n",
       "      <td>49.300000</td>\n",
       "      <td>2.121551e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>950.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Mean       Std Dev      Min          Max     Count\n",
       "back.              54156.355878  3.159360e+03  13140.0      54540.0    2203.0\n",
       "buffer_overflow.    1400.433333  1.337133e+03      0.0       6274.0      30.0\n",
       "ftp_write.           220.750000  2.677476e+02      0.0        676.0       8.0\n",
       "guess_passwd.        125.339623  3.037860e+00    104.0        126.0      53.0\n",
       "imap.                347.583333  6.299260e+02      0.0       1492.0      12.0\n",
       "ipsweep.              10.083400  5.231658e+00      0.0         18.0    1247.0\n",
       "land.                  0.000000  0.000000e+00      0.0          0.0      21.0\n",
       "loadmodule.          151.888889  1.277453e+02      0.0        302.0       9.0\n",
       "multihop.            435.142857  5.409604e+02      0.0       1412.0       7.0\n",
       "neptune.               0.000000  0.000000e+00      0.0          0.0  107201.0\n",
       "nmap.                 24.116883  5.941987e+01      0.0        207.0     231.0\n",
       "normal.             1157.047524  3.422612e+04      0.0    2194619.0   97278.0\n",
       "perl.                265.666667  4.932883e+00    260.0        269.0       3.0\n",
       "phf.                  51.000000  0.000000e+00     51.0         51.0       4.0\n",
       "pod.                1462.651515  1.250980e+02    564.0       1480.0     264.0\n",
       "portsweep.        666707.436538  2.150067e+07      0.0  693375640.0    1040.0\n",
       "rootkit.             294.700000  5.385782e+02      0.0       1727.0      10.0\n",
       "satan.                 1.337319  4.294620e+01      0.0       1710.0    1589.0\n",
       "smurf.               935.772300  2.000224e+02    520.0       1032.0  280790.0\n",
       "spy.                 174.500000  8.838835e+01    112.0        237.0       2.0\n",
       "teardrop.             28.000000  0.000000e+00     28.0         28.0     979.0\n",
       "warezclient.      300219.562745  1.200905e+06     30.0    5135678.0    1020.0\n",
       "warezmaster.          49.300000  2.121551e+02      0.0        950.0      20.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"src_bytes statistics, by label\")\n",
    "get_variable_stats_df(stats_by_label,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Corrélations\n",
    "La fonction `corr` propose des corrélations de Spearman (rangs) ou de Pearson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics \n",
    "correlation_matrix = Statistics.corr(vector_data, method=\"spearman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)\n",
    "col_names = [\"duration\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\n",
    "             \"urgent\",\"hot\",\"num_failed_logins\",\"logged_in\",\"num_compromised\",\n",
    "             \"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\",\n",
    "             \"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "             \"is_hot_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "             \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "             \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "             \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "             \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "             \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\"]\n",
    "\n",
    "corr_df = pd.DataFrame(correlation_matrix, index=col_names, columns=col_names)\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction des couples les plus corrélés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Une variable bouléenne est True en cas de forte corrélation\n",
    "highly_correlated_df = (abs(corr_df) > .8) & (corr_df < 1.0)\n",
    "# Extraction des noms des variables\n",
    "correlated_vars_index = (highly_correlated_df==True).any()\n",
    "correlated_var_names = correlated_vars_index[correlated_vars_index==True].index\n",
    "highly_correlated_df.loc[correlated_var_names,correlated_var_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque**: [J. A. Dianes](https://github.com/jadianes/spark-py-notebooks/blob/master/nb8-mllib-logit/nb8-mllib-logit.ipynb) utilise la procédure ci-dessus (variables les plus corrélées) ou des tests du Chi2 pour sélectionner des variables avant d'estimer une régression (logistique). C'est contraitre aux usages en Statistique qui privilégient des méthodes *pas-à-pas* (*forward, backward, both*) en minimisant un critère comme AIC (cf. un tutoriel en R) ou la prise en compte d'une *pénalisation lasso*. C'est ce dernier cas qui est privilégié, tant dans la librairie *Scikit-learn* de Python, que dans *Mllib*.\n",
    "\n",
    "Pour la suite,  l'ensemble des données dela [KDD cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) sont utilisées. Il peut être prudent de *redémarer le noyau* afin de libérer la mémoire avant de poursuivre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5.1 Lecture et préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Données d'apprentissage\n",
    "#Python 2\n",
    "import urllib\n",
    "#f = urllib.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz\", \"kddcup.data.gz\")\n",
    "#Python 3\n",
    "import urllib.request\n",
    "f = urllib.request.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz\",DATA_PATH+\"/kddcup.data.gz\")\n",
    "data_file = DATA_PATH+\"/kddcup.data.gz\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "print(\"Train data size is {}\".format(raw_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Données de test\n",
    "#Python 2\n",
    "#ft = urllib.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/corrected.gz\", \"corrected.gz\")\n",
    "#Python 3\n",
    "ft = urllib.request.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/corrected.gz\", DATA_PATH+\"/corrected.gz\")\n",
    "test_data_file = DATA_PATH+\"/corrected.gz\"\n",
    "test_raw_data = sc.textFile(test_data_file)\n",
    "\n",
    "print(\"Test data size is {}\".format(test_raw_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Labeled Points** est le format à utiliser pour un objectif d'apprentissage supervisé. Le *label* contient la variable à modéliser, classe (entier de 0 à nombre de classes - 1) ou valeur quantitative.\n",
    "\n",
    "Il s'agit de modéliser / prévoir l'occurence d'une attaque indépendamment du type de celle-ci. Il s'agit donc d'une variable binaire (0,1) à construire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from numpy import array\n",
    "# fonction à appliquer à chaque ligne ou interaction sur le réseau\n",
    "def parse_interaction(line):\n",
    "    line_split = line.split(\",\")\n",
    "    # supprime les colonnes [1,2,3,41]\n",
    "    clean_line_split = line_split[0:1]+line_split[4:41]\n",
    "    attack = 1.0\n",
    "    if line_split[41]=='normal.':\n",
    "        attack = 0.0\n",
    "    return LabeledPoint(attack, array([float(x) for x in clean_line_split]))\n",
    "# exécution sur données d'apprentissage\n",
    "training_data = raw_data.map(parse_interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Données de test\n",
    "test_data = test_raw_data.map(parse_interaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Estimation de la [régression logistique](http://wikistat.fr/pdf/st-m-app-rlogit.pdf)\n",
    "C'est la procédure classique pour prévoir une variable binaire. Mllib propose deux [algorithmes](https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression) pour l'optimisation (*mini-batch gradient descent* et L-BFGS). L-BFGS converge en principe plus vite. \n",
    "\n",
    "Comme dans *Scikit-learn* la procédure propose d'inclure une régularisation par pénalisation *l2* (ridge) ou *l1* (lasso). Le paramètre `regType` précise le type (`l1` ou `l2`) tandis que `regParam` précise la pénalisation. Par défaut, il n'y a pas de régularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from time import time\n",
    "t0 = time()\n",
    "logit_model = LogisticRegressionWithLBFGS.train(training_data)\n",
    "tt = time() - t0\n",
    "print(\"Apprentissage en {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Estimation de l'erreur\n",
    "Une fonction *map* permet de calculer la prévision de chaque observation du test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_and_preds = test_data.map(lambda p: (p.label, logit_model.predict(p.features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il suffit ensuite de calculer l'erreur de prévision sur l'échantillon test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "test_accuracy = labels_and_preds.filter(lambda (v, p): v == p).count() / float(test_data.count())\n",
    "errreur=1-test_accuracy\n",
    "tt = time() - t0\n",
    "print(\"Calcul en {} secondes. Le taux d'erreur est {}\".format(round(tt,3), round(erreur,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarques**; **exercices**\n",
    "\n",
    "- Les variables qualitatives `protocol` et `service`ont été éliminées par simplicité, il faudrait les ajouter sous la forme d'indicatrices (*dummy variables*)\n",
    "- Introduire une pénalisation lasso, [optimiser](https://spark.apache.org/docs/latest/ml-tuning.html) le paramètre par validation croisée. Le résultat est-il meilleure?\n",
    "- [J.A. Dianes](https://github.com/jadianes/spark-py-notebooks/blob/master/nb9-mllib-trees/nb9-mllib-trees.ipynb) estime également un [arbre binaire de décision](https://spark.apache.org/docs/latest/mllib-decision-tree.html) sur ces données. malheureusement, comme dans le cas de la librairie *Scikit-learn*, il n'est pas possible d'optimiser correctement l'élagage d'un arbre proposé par *Mllib*. Il se contente de construire un arbre de profondeur maximum 3, donc facile à interpréter, avec une erreur de prévision à peine supérieure à celle de la régression logistique.\n",
    "- Utiliser les [forêts aléatoires](https://spark.apache.org/docs/latest/mllib-ensembles.html#random-forests) sur ces données.\n",
    "\n",
    "Ces méthodes plus sophistiquées d'apprentissage sont testées dans les différents cas d'usage."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "PySpark (Spark 2.2.0)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "nav_menu": {
    "height": "457px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
