{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 200px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies des grosses data](https://github.com/wikistat/Ateliers-Big-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction à la librairie  [SparkML](https://spark.apache.org/docs/latest/ml-guide.html) (ou MLlib  DataFrame-based API) de  <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"Spark\"/> </a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depuis Spark 2.0 la librairie MLlib, qui maniupule éxclusivement des RDD est maintenant en maintenance. Elle peut toujours être utilisé, mais il n'y aura pas de nouveaux outils développé pour cette librairie.\n",
    "\n",
    "La principale librairie de machine learning pour spark est maintenant SparkML. SparkML est une librairie qui manipule exclusivement des DataFrame, plus facile d'utilisation que les RDD et qui permet l'utilistation d'autres services spark tel que Spark Datasources, SQL queries etc...\n",
    "Si MLlib est aujourd'hui plus complète, SparkML devrait posséder exactement les même fonctions que MLlib dans la version 2.3 de Spark (Aujourd'hui la version 2.2 est disponible). La librairie MLlib, qui manipule exclusivement des RDD n'existera plus dans Spark 3.0\n",
    "\n",
    "\n",
    "SparkML n'est pas le nom définitif que portera la librairie de machine learning de spark. C'est un nom temporaire créer pour déveloper un outil basé sur les DataFrame. MLlib sera le nom définitif de cette librairie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résumé**: Ce tutoriel continue l'initiation à [Spark](https://spark.apache.org/) à l'aide de commandes en Python en utilisant l'API  [`PySpark`](http://spark.apache.org/docs/latest/api/python/). Dans ce calepein Nous allons manier majoritairement des DataFrame telles que celles décrite dans le calepin précédent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Statistique élémentaire avec <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"Spark\"/> </a> et  [SparkML](https://spark.apache.org/docs/latest/ml-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La plupart des fonctions de statistiques élémentaires de <I>MLlib</I>, décrites dans le calepin 2 de cette introduction à pyspark ne sont pas encore disponible dans la librairie <I>SparkML</I>. Seul la fonction de correlation et le test d'hypothèse du χ² sont disponible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.32.30.48:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "La fonction <I> pyspark.ml.stat.Correlation</I> permet de calculer la correlation entre chaque colonne d'une DataFrame. Les correlations disponible sont celles de <I>Pearson</I> et <I>Spearman</I>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      "DenseMatrix([[ 1.        ,  0.05564149,         nan,  0.40047142],\n",
      "             [ 0.05564149,  1.        ,         nan,  0.91359586],\n",
      "             [        nan,         nan,  1.        ,         nan],\n",
      "             [ 0.40047142,  0.91359586,         nan,  1.        ]])\n",
      "Spearman correlation matrix:\n",
      "DenseMatrix([[ 1.        ,  0.10540926,         nan,  0.4       ],\n",
      "             [ 0.10540926,  1.        ,         nan,  0.9486833 ],\n",
      "             [        nan,         nan,  1.        ,         nan],\n",
      "             [ 0.4       ,  0.9486833 ,         nan,  1.        ]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test d'Hypothèse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark.ml supporte actuellement le test du Chi-2 de Pearson. Ce test permet d'effectuer un test d'indépendance pour chaque <I>features</I> vis à vis du <I>label</I>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pValues: [0.687289278791,0.682270330336]\n",
      "degreesOfFreedom: [2, 3]\n",
      "statistics: [0.75,1.5]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
    "        (0.0, Vectors.dense(1.5, 20.0)),\n",
    "        (1.0, Vectors.dense(1.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 40.0)),\n",
    "        (1.0, Vectors.dense(3.5, 40.0))]\n",
    "df = spark.createDataFrame(data, [\"label\", \"features\"])\n",
    "\n",
    "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librairie <I>SparkML</I>, contrairement à <I>MLlib</I>, est basé sur la notion de <B>ML Pipeline</B>. \n",
    "\n",
    "Un <B>ML Pipeline</B> permet de combiner différentes étapes de traitement, allant du nettoyage des données jusqu'a l'étape d'apprentissage en un seul objet appelé <I>pipeline<\\I> ou <I>workflow.<\\I>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator, Transformer, and Param\n",
    "\n",
    "La construction d'un <B>ML Pipeline</B> est effectué à partir de trois type d'objets décrits ci-dessous :\n",
    "\n",
    "\n",
    " * <B>Transformer</B>: Un <B>Transformer</B> est un algorithme qui permet de transformer une DataFrame en une autre DataFrame. Dans la pluspart des cas, la nouvelle DataFrame est une DataFrame identique à la première avec une colonne supplémentaire. Exemple de <B>Transformer</B>: \n",
    "     * Un modèle d'apprentissage va prendre en entrée une DataFrame de variable et retourner une nouvelle DataFrame avec les variables et une nouvelle colonne correspondant à la prédiction.\n",
    "     * Le Transformer <I>StringIndexer</I> va prendre en entrée une DataFrame possédant un colonne de texte et retourner une nouvelle DataFrame avec la même colonne texte et une nouvelle colonne ou les textes seront remplacés par une valeur numérique.\n",
    " \n",
    "\n",
    "* <B>Estimator</B>: Un <B>Estimator</B> est un algorithme qui peut-être appliquer sur une DataFrame afin de produire un <B>Transformer</B>. Exemple d'<B>Estimator</B>:\n",
    "    * Un algorithme d'apprentissage est un <B>Estimator</B>. Une fois celui-ci appliqué sur une DataFrame, il va produire un modèle d'apprentissage qui sera un <B>Transformer</B>, comme décrit précédemment.\n",
    "\n",
    "\n",
    " * <B>Parameter</B>: Chaque  <B>Transformer</B> et <B>Estimators</B> partage une même API pour spécifier leur paramètre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple : Regression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression parameters:\n",
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: label)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction, current: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability, current: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.01)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
      "\n",
      "features=[-1.0,1.5,1.3], label=1.0 -> prob=[0.00133144762578,0.998668552374], prediction=1.0\n",
      "features=[3.0,2.0,-0.1], label=0.0 -> prob=[0.981109918007,0.0188900819932], prediction=0.0\n",
      "features=[0.0,2.2,-1.5], label=1.0 -> prob=[0.0015474414135,0.998452558587], prediction=1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# DataFrame D'apprentissage\n",
    "training = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])\n",
    "\n",
    "# On crée d'abord un objet LogisticRegression en spécifiant ses paramètres. Cet objet est un Estimator\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01, featuresCol='features', labelCol='label', predictionCol='prediction', probabilityCol='probability')\n",
    "print(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")\n",
    "\n",
    "# On applique l'Estimator sur notre DataFrame d'apprentissage. L'objet qui en résulte, le modèle d'apprentissage, est un Transformer.\n",
    "model = lr.fit(training)\n",
    "\n",
    "\n",
    "# DataFrame de Test\n",
    "test = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
    "    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
    "    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])\n",
    "\n",
    "# On applique le Transformer sur la DataFrame de Test 'test'.\n",
    "prediction = model.transform(test)\n",
    "\n",
    "# Il en resulte une nouvelle DataFrame 'prediction' qui correspond a la DataFrame 'tes' auquelle on été ajoutés une colonne \"prediction\" et une colonne \"probability\".\n",
    "# Les noms de ces nouvelles colonnes ont été spécifiés dans les paramètres lors de l'instanciation de l'Estimator 'lr' puis transmis au Transformer 'model'.\n",
    "\n",
    "# Nous pouvons maintenant illustrer les résultats.\n",
    "result = prediction.select(\"features\", \"label\", \"probability\", \"prediction\") \\\n",
    "    .collect()\n",
    "\n",
    "for row in result:\n",
    "    print(\"features=%s, label=%s -> prob=%s, prediction=%s\"\n",
    "          % (row.features, row.label, row.probability, row.prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un <B> Pipeline </B> est un enchainement de plusieurs <B> Transformers </B> et <B> Estimators </B> afin de spécifier un processus entier de Machine Learning. Par exemple, pour effectuer de l'apprentissage statistique sur des données textuelles, ces différentes étapes sont appliquées les unes à la suite des autres: \n",
    "\n",
    " * Découpage du texte en liste de mot\n",
    " * Conversion en variable numérique\n",
    " * Aprentissahe sur les données numérique\n",
    " * Prédiction \n",
    "\n",
    "Toutes ces étapes peuvent être résumé dans un seul objet appelé <B> Pipeline </B>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple : Toeknize, Hash et Regression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, spark i j k) --> prob=[0.155543713844,0.844456286156], prediction=1.000000\n",
      "(5, l m n) --> prob=[0.830707735211,0.169292264789], prediction=0.000000\n",
      "(6, spark hadoop spark) --> prob=[0.0696218406195,0.93037815938], prediction=1.000000\n",
      "(7, apache hadoop) --> prob=[0.981518350351,0.018481649649], prediction=0.000000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "# DataFrame d'Apprentissage\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure un pipeline qui consiste en 3 étapes :\n",
    "\n",
    "# 1/ Tokenizer. On spécifie la colonne d'entrée 'text' et la colonne de sortie 'words'.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "# 2/ Hash. La colonne d'entrée est spécifié ici comme étant la colonne de sortie de l'étape précédente.\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "# 3/ Regression Logistique\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "\n",
    "# On configure le Pipeline comme la succession des étapes précédentes.\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# On applique toutes les étapes sur la DataFrame d'apprentissage.\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# DataFrame Test.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Prediction.\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, text: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "PySpark (Spark 2.2.0)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "nav_menu": {
    "height": "457px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
