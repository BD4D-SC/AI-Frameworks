{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 200px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IA Framework.\n",
    "## Lab 1  - Introduction to Pyspark.\n",
    "#### Part 3 La classe *DataFrame* de <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"Spark\"/> </a> [SQL](http://spark.apache.org/sql/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RÃ©sume**: This notebook continue the introduction to [Spark](https://spark.apache.org/) trough  [`PySpark`](http://spark.apache.org/docs/latest/api/python/) API. In this notebook we will learn to manipulate ***Spark's Dataframe***, which will be the standard class and will replace RDD for Spark 3.0 release. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue to used [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) dataset in this TP. \n",
    "\n",
    "In this TP we will also used the names of the columns as the `Dataframe` class will be able to handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH=\"\" \n",
    "import urllib.request\n",
    "urllib.request.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names\",DATA_PATH+\"kddcup.names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the column files and create a list of those names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = open(DATA_PATH+\"kddcup.names\",\"r\").readlines()\n",
    "col_names = [k.split(\":\")[0] for k in file_names[1:]]+[\"interactions\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  *Spark SQL*\n",
    "\n",
    "\n",
    "*Spark SQL* is a Spark library which allow to manipulate structured data, which RDD does not. Indeed Spark will see no differences between an RDD of list or an RDD of dictionnaries. \n",
    "\n",
    "With *Spark SQL* the data are organized according to a schema. A schema will define the type of variable that will be enable faster computation.\n",
    "\n",
    "They are various way to build and manipulate those structured object :SQL request, `Dataset` API  (only for java and scala) or `pyspark`' `Dataframe` that will be used all allong this TP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen in the first notebook that Spark needs a context to be used :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to manipulate `Dataframe` we will need to define a different kind of context : `SQLContext`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *DataFrame* construction\n",
    "\n",
    "A Spark's `Datafrae` is a collection of distributed data organized by column.\n",
    "\n",
    "It is vert similar to R's *Dataframe* or Python pandas' *Dataframe*.They can be build from various sources such as  : *Hive*, *json*, *xml*, *parquet*, *cassandra*... \n",
    "\n",
    "We will see how to build DataFrame from `RDD`, `.csv` and  `pandas`' *DataFrame*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From  a RDD\n",
    "\n",
    "We start by creating the rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = DATA_PATH+\"kddcup.data_10_percent.gz\"\n",
    "string_rdd = sc.textFile(data_file)\n",
    "list_rdd = string_rdd.map(lambda l: l.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying the Schema\n",
    "\n",
    "We will first specify the schema in the RDD before we convert it into a `DataFrame` *DataFrame*. \n",
    "The schema will be define as a `StructType` object composed of `StructField` which will defined the type of the different field.\n",
    "\n",
    "We will create a DataFrame that contains the following columns : *duration,rotocol_type, service, flag, src_bytes, dst_bytes, interactions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "fields = [StructField(\"duration\", IntegerType(), True),\n",
    "          StructField(\"protocol_type\", StringType(), True),\n",
    "          StructField(\"service\", StringType(), True),\n",
    "          StructField(\"flag\", StringType(), True),\n",
    "          StructField(\"src_bytes\", IntegerType(), True),\n",
    "          StructField(\"dst_bytes\", IntegerType(), True),\n",
    "          StructField(\"interactions\", StringType(), True)]\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then easily create the `DataFrame` with the `createDataFrame` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subList_rdd = list_rdd.map(lambda p: (int(p[0]), p[1], p[2], p[3], int(p[4]), int(p[5]), p[-1]))\n",
    "df_rdd = sqlContext.createDataFrame(subList_rdd, schema)\n",
    "df_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferring the Schema\n",
    "\n",
    "You can also create a `DataFrame` from a RDD by inferring the schema. \n",
    "It will require that the RDD is composed of `Row` object that are dictionnary_like object in Spark.\n",
    "The type of value from each entry of the Row is deducted from the first row which need to be correctly defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "row_rdd = list_rdd.map(lambda p: Row(\n",
    "    duration=int(p[0]), \n",
    "    protocol_type=p[1],\n",
    "    service=p[2],\n",
    "    flag=p[3],\n",
    "    src_bytes=int(p[4]),\n",
    "    dst_bytes=int(p[5]),\n",
    "    interactions = p[-1],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the rdd is created you can call the `createDataFrame` function without specifying any schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd_1 = sqlContext.createDataFrame(row_rdd)\n",
    "df_rdd_1.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A schema can be display with the `printSchema` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd_1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From a `.csv`\n",
    "\n",
    "When the source file is structured in a given format (*parquet*, *json*, *csv*),the `spark.read.load` function allow to direclty infer the schema. \n",
    "\n",
    "The *kddcup.data_10_percent.gz* file is organized as a`.csv`. The *dataframe* can then be build direclty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = spark.read.load(data_file, format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"False\")\n",
    "df_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now specify the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv=df_csv.toDF(*col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From a `pandas`' *DataFrame* \n",
    "\n",
    "\n",
    "Convert `pandas`' *DataFrame*  to a `PysSpark`'s `DataFrame` require the use of *pyarrow* library that enable to move object from *JVM* to *python*.\n",
    "\n",
    "To use it, you will need to enable its execution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pandas_df = pd.read_csv(DATA_PATH+\"kddcup.data_10_percent.gz\", sep=\",\", names=col_names)\n",
    "df = sqlContext.createDataFrame(pandas_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RequÃªte SQL\n",
    "\n",
    "\n",
    "*SparkSQL* enable to apply SQL request and return the result to a `DataFrame`.\n",
    "\n",
    "We do not have a SQL database available here. But the sql context to apply sql request on existing `DataFrame`.\n",
    "We need for that to register it on a  *SQL temporary view* format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.createOrReplaceTempView(\"interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select *tcp* interaction without transfer and with duration above 1second.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcp_interactions = sqlContext.sql(\"\"\"\n",
    "    SELECT duration, dst_bytes FROM interactions WHERE protocol_type = 'tcp' AND duration > 1000 AND dst_bytes \n",
    "    = 0\"\"\")\n",
    "tcp_interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The request is a *DataFrames* object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcp_interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are still using Spark! The above execution are lazy. You need to call action's function to actuammy apply the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcp_interactions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sortie des durÃ©es avec les dst_bytes\n",
    "tcp_interactions_out = tcp_interactions.rdd.map(lambda p: \"Duration: {}, Dest. bytes: {}\".format(p.duration, p.dst_bytes))\n",
    "for ti_out in tcp_interactions_out.collect():\n",
    "  print (ti_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Operations on *DataFrame* \n",
    "### Elementary operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*select* allow to extract columns of a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.select(\"interactions\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.select(\"interactions\",\"duration\").take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`groupBy` works like pandas' `groupby`.\n",
    "\n",
    "The example below enable to count number of interaction according to protocol type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "df_rdd.groupBy(\"protocol_type\").count().show()\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"Times to execute the request : {}\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`filter` a  *DataFrame* \n",
    "\n",
    "We count the interactions, according to protocol type , for interaction that last less thant 1 second and without data transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "df_rdd.filter(df_rdd.duration>1000).filter(df_rdd.dst_bytes==0).groupBy(\"protocol_type\").count().show()\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"Requete executee en {} secondes\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the 'DataFrame' syntax allow much more flexible syntax that RDD's one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `map` and *custom function*\n",
    "\n",
    "The  `map` function is not available on *DataFrame* object. \n",
    "You will have to convert the  `DataFrame` to and `RDD` to use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.rdd.map(lambda p: \"Duration: {}, Dest. bytes: {}\".format(p.duration, p.dst_bytes)).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way is to use the `udf` (*user defined function*).\n",
    "\n",
    "You first define the function you want to apply on each row, and convert it as an udf object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "function = udf(lambda x,y: \"Duration: {}, Dest. bytes: {}\".format(x,y))\n",
    "function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can the apply this function to the wanted columns of the *DataFrame*. The `alias` function allow you to name the output columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataframe = df_rdd.select(function(\"duration\",\"dst_bytes\").alias(\"string_output\"))\n",
    "output_dataframe.select(\"string_output\").take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column creation\n",
    "\n",
    "\n",
    "We have seen how to apply `udf` function on a *DataFrame*. The result is a one column *DataFrame*. You can add it to an existing *DataFrame* with the `withColumn` function.\n",
    "\n",
    "We will add a `label` columns to the `df` *DataFrame* which will have two possible value `attack` and `normal`. \n",
    "\n",
    "We first create the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_or_normal_func(s):\n",
    "    return \"normal\" if s == \"normal.\" else \"attack\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is convert in a *user defined function*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_or_normal = udf(attack_or_normal_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create the `label` column from the `df` *DataFrame* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_label = df.withColumn(\"label\", attack_or_normal(df.interactions) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can know check that the columns has been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_label.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**  Display number of attack and normal interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise3_1.py \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**  Display number of attack and normal interaction for each protocol_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise3_2.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check all the functionality on the online [documentation](http://spark.apache.org/docs/latest/sql-programming-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas `udf` function\n",
    "\n",
    "A `pandas udf` function is similar to `udf` function previously defined. As the latest, it allow to apply transformation to a *DataFrame* column but this columns will be treated as a *pandas*' *SÃ©ries*.\n",
    "\n",
    "This will allow better [performances](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html) and also to apply native function of *pandas*. \n",
    "\n",
    "They are two type of `pandas udf`: `Scalar` and `Grouped Map`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Scalar`\n",
    "\n",
    "*Scalar Pandas UDFs* are used to easily vectorized scalar operation. It will handle p*andas* *Series* as an argument and return *Series* de *pandas* of same size.\n",
    "\n",
    "Here we will apply *pandas* `cumsum` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Declare the function and create the UDF\n",
    "def cum_sum(x):\n",
    "    return x.cumsum()\n",
    "\n",
    "cum_sum_udf = pandas_udf(cum_sum, returnType=IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_sum_duration = df.select(cum_sum_udf(col(\"duration\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction *Grouped Map*\n",
    "\n",
    " *Grouped map Pandas UDFs* need to be used along with `groupBy().apply()` function allowing to apply `split-apply-combine` pattern. \n",
    " This one is a three steps job:\n",
    " \n",
    "\n",
    " * *Split* data with `DataFrame.groupBy`.\n",
    " * *Apply* function on each group. Input and output will be *pandas*' *DataFrames* output. L\n",
    " * *Combine* results in a new *DataFrame*.\n",
    "\n",
    "Before using `groupBy().apply()`, you have to define\n",
    "\n",
    "* A python function which will be applied on each group\n",
    "* A `StructType`  or a  `string` which will specify the schema of the output.\n",
    "\n",
    "In the following example we will substract mean `duration` according to the label `attack` or `normal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"label string, duration int\", PandasUDFType.GROUPED_MAP)\n",
    "def substract_mean(pdf):\n",
    "    # pdf is a pandas.DataFrame\n",
    "    duration = pdf.duration\n",
    "    return pdf.assign(duration=duration - duration.mean())\n",
    "\n",
    "df_with_label.select(\"label\",\"duration\").groupby(\"label\").apply(substract_mean).show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (PySpark)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
