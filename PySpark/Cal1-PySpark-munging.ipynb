{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 200px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IA Framework.\n",
    "## Lab 1  - Introduction to Pyspark.\n",
    "#### Part 1 Data munging with <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"Spark\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resume**: The objective of this notebook is to discover [Spark](https://spark.apache.org/) framework and its python API [`PySpark`](http://spark.apache.org/docs/latest/api/python/). \n",
    "We will see the main motivation to use these frameworks that allow to apply distributed task on clusters and understand the concept of **RDD**(*Resilient Distributed Datasets*), main abstraction Spark provides , and how we can use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### When using Spark ?\n",
    "\n",
    "When data becomes to big for either RAM or Disk memory and/or where computation time are to high for your computer.  \n",
    "**Spark** allow you do parallelize your tasks to different clusters and provides an interface to do it easily. \n",
    "\n",
    "Machine learning algorithm (supervised or unsupervised) are iterative algorithm. Using them with *Hadoop* technology requires to read and write at each iteration on disk which will make learning really slow. [Spark](http://spark.apache.org/)' (*Resilient Distributed Dataset* or **RDD** [Zaharia et al. 2012](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf). We won't get deeply intoo Hadoop details. See [Besse et al. 2016](https://hal.archives-ouvertes.fr/hal-01350099v2/document) for an introduction to *MapReduce*, its limitation. \n",
    "\n",
    "Spark allow to stock data in each cluster trought **RDD** and use them only in read-only mode which allow faster run of different algorithms.\n",
    "\n",
    "\n",
    "#### About spark\n",
    "\n",
    "* **Spark** can be used trough *Java*, *Scala*, *R* and *Python* API. We will see  [`PySpark`](http://spark.apache.org/docs/latest/api/python/) API all along this TP. \n",
    "* **Spark** has four main librairies:\n",
    " \n",
    "    * [`SparlSQL`](https://spark.apache.org/docs/latest/sql-programming-guide.html) which allows to access really big and various kind of data, structred or not by executing SQL syntax. (Part 3).\n",
    "    * [`MLlib`](http://spark.apache.org/docs/latest/ml-guide.html) which contains statistical and machine learning algorithm (Part 2 and 4).\n",
    "    * [`SparkStreaming`](https://spark.apache.org/docs/latest/streaming-programming-guide.html) for live data stream processing (No cover in this TP).\n",
    "    * [`GraphX`](https://spark.apache.org/docs/latest/graphx-programming-guide.html) for graphs and graph-parallel computation (No cover in this TP).\n",
    "\n",
    "\n",
    "\n",
    "#### Spark locally\n",
    "**Spark** is designed to run on cluster to take advantage of it.\n",
    "In this TP we will run Spark locally to understand how to use *Pyspark* API, but we won't be able to realize the all potential of Spark.\n",
    "\n",
    "\n",
    "#### Warnings\n",
    "\n",
    "For the training phase of a machine learning model estimation, it's often easier to use bigger ressources (more Ram and CPU), than using **Spark** on distributed cluster. Using **Spark** (or **Hadoop**) will be more efficient for extracting sampling, preprocessing the data (See *Cdiscount* application in [Besse et al. 2016](https://hal.archives-ouvertes.fr/hal-01350099v2/document). \n",
    "\n",
    "\n",
    "#### References\n",
    "Official [documentation](https://spark.apache.org/docs/latest/). [Karau et al. (2015)](http://index-of.co.uk/Big-Data-Technologies/Learning%20Spark%20%20Lightning-Fast%20Big%20Data%20Analysis%20.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "**Spark** needs a configuration to be used. We create for that a `SparkConf` object that contains information about your application (where is the master, the cluster, etc..) and  `SparkContext` object which tell us how to access a cluster.\n",
    "\n",
    "Here, these objects have already been defined within the pyspark kernel of your notebook, and can be directly used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://insa-20480.insa-toulouse.fr:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "This notebook has been inspired by the one build by [J. A. Dianes](https://github.com/jadianes/spark-py-notebooks) which use the dataset used in the  [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) competition. It contains 9M of interactions within a newtork (see description [here](http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names)). The objective of this contest was to detect attack in a networf from various features build or computed on transaction or interaction with this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "# Download the file from URL\n",
    "DATA_PATH=\"\"  #<-- Put here where you want to stock the file.\n",
    "f = urllib.request.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\",DATA_PATH+\"kddcup.data_10_percent.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From  a python object\n",
    "You can convert a python object to a RDD with the `parallelize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = range(100)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize(l)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From a file\n",
    "The other way is to read a file from your computer as a RDD with the `textfile` function. You will read here the dataset previously defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = DATA_PATH+\"kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "raw_data.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transformation** and **Actions** on RDD.\n",
    "\n",
    "They are two type of operations you can apply on RDD **Transformation** and **Actions**. \n",
    "\n",
    "* **Transformation** allow to create a new RDD from an existing one.\n",
    "* **Actions** apply computation on the RDD and return a value (like the .count function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.map` function is one of the simplest **Transformation**. It allows to apply the same function to each entries of the rdd.\n",
    "Below we apply a function to split each string entry of the RDD to get a list of elements :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time execution :0.0019 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "ts = time.time()\n",
    "csv_data = raw_data.map(lambda x : x.split(\",\"))\n",
    "te= time.time()\n",
    "print(\"Time execution :%.4f seconds\" %(te-ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now observe the new RDD produced by the transformation : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take\n",
    "The `.take` function is one of the simpliest action. It convert the rdd to a python list of n element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0', 'tcp', 'http', 'SF', '181', '5450', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '8', '8', '0.00', '0.00', '0.00', '0.00', '1.00', '0.00', '0.00', '9', '9', '1.00', '0.00', '0.11', '0.00', '0.00', '0.00', '0.00', '0.00', 'normal.'], ['0', 'tcp', 'http', 'SF', '239', '486', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '8', '8', '0.00', '0.00', '0.00', '0.00', '1.00', '0.00', '0.00', '19', '19', '1.00', '0.00', '0.05', '0.00', '0.00', '0.00', '0.00', '0.00', 'normal.']]\n",
      "Time execution :0.0488 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "ts = time.time()\n",
    "csv_take  =csv_data.take(2)\n",
    "print(csv_take)\n",
    "te =time.time()\n",
    "print(\"Time execution :%.4f seconds\" %(te-ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** \n",
    "* What can you say about the time execution fo the two cells above? Is it normal? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More complex map function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the above cell, the function apply on each entry is defined within the map function thanks to the `lambda` operator. \n",
    "\n",
    "You can also define first a python function and then apply on the RDD. This will allow you to define more complex and more readable function.\n",
    "\n",
    "Below is a function that convert element to float, if the string can be converted to a float, and separe features from the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([0.0, 'tcp', 'http', 'SF', 181.0, 5450.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 8.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 9.0, 9.0, 1.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0], 'normal.')]\n"
     ]
    }
   ],
   "source": [
    "# Fontion qui sépare les champ (elems=valeur) et extrait la 41ème = clef.\n",
    "def parse_interaction(l):\n",
    "    elems = l\n",
    "    features =[]\n",
    "    for e in  elems[:-1]:\n",
    "        try:\n",
    "            e=float(e)\n",
    "        except ValueError:\n",
    "            e=e\n",
    "        features.append(e)\n",
    "    y = elems[-1]\n",
    "    return (features, y)\n",
    "# Affichage des 5 premiers\n",
    "key_csv_data = csv_data.map(parse_interaction)\n",
    "print(key_csv_data.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `count` function is a **actions** which allow to count number of elements on a rdd. \n",
    "\n",
    "Let's compute the number of element in the three RDD created so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494021\n",
      "Time execution :0.4217 seconds\n",
      "494021\n",
      "Time execution :0.9110 seconds\n",
      "494021\n",
      "Time execution :4.9017 seconds\n"
     ]
    }
   ],
   "source": [
    "ts = time.time()\n",
    "print(raw_data.count())\n",
    "te= time.time()\n",
    "print(\"Time execution :%.4f seconds\" %(te-ts))\n",
    "\n",
    "\n",
    "ts = time.time()\n",
    "print(csv_data.count())\n",
    "te= time.time()\n",
    "print(\"Time execution :%.4f seconds\" %(te-ts))\n",
    "\n",
    "ts = time.time()\n",
    "print(key_csv_data.count())\n",
    "te= time.time()\n",
    "print(\"Time execution :%.4f seconds\" %(te-ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** \n",
    "* What can you say about the results of the three application of the count function?\n",
    "* What can you say about time execution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distinct\n",
    "the `distinct` function is a **transformation** that build a RDD where all duplicated element are remove\n",
    "\n",
    "In the cell below we build a list of all protocol in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tcp', 'udp', 'icmp']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protocol = csv_data.map(lambda x : x[1]).distinct()\n",
    "protocol.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter\n",
    "Another very used **Transformation** is the `filter` function. It wil create a smaller dataset based on a custom condition function.\n",
    "\n",
    "The cell below, will build a smaller RDD which contains only '.normal' entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_csv_data = csv_data.filter(lambda x: 'normal.' == x[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They are 97278 normal interactions (over 494021)\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "normal_count = normal_csv_data.count()\n",
    "total_count = csv_data.count()\n",
    "tt = time.time() - t0\n",
    "print(\"They are %d normal interactions (over %d)\" %(normal_count, total_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect\n",
    "`collect` is an ***action** similare to `.take` **action**,  except that it will convert the ALL dataset to a python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected in 2.292 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "all_raw_data = raw_data.collect()\n",
    "tt = time.time() - t0\n",
    "print(\"Data collected in %.3f seconds\" %tt)\n",
    "all_raw_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494021\n",
      "Time runing : 0.000\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "print(len(all_raw_data))\n",
    "tt = time.time() - t0\n",
    "print(\"Time runing : %.3f\" %tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining transformation\n",
    "\n",
    "You've seen above that transformation are *Lazy* Operators, combining **transformations** on various RDD are also *Lazy*. The all pipeline will be run only when a **action** is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time runing : 0.019\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "data_file = DATA_PATH+\"kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "key_csv_data = raw_data.map(parse_interaction)\n",
    "normal_key_interactions = key_csv_data.filter(lambda x: x[0] == \"normal.\")\n",
    "tt = time.time() - t0\n",
    "print(\"Time runing : %.3f\" %tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time runing : 25.913\n",
      "There are 0 'normal' interactions\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "count_normal_interaction= normal_key_interactions.count()\n",
    "tt = time.time() - t0\n",
    "print(\"Time runing : %.3f\" %tt)\n",
    "print(\"There are %d 'normal' interactions\" %count_normal_interaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation on rdd.\n",
    "Operation are **Transformation** that can by apply on different RDD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Substract\n",
    "`Substract` operation enable to create a RDD *C* from RDDs *A* and *B* by taking all entries from *A* that are not in *B*.\n",
    "\n",
    "The cells below allow to create a RDD with no normal data in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_raw_data = raw_data.filter(lambda x: \"normal.\" in x) #<- RDD with normal data\n",
    "attack_raw_data = raw_data.subtract(normal_raw_data) #<-- RDD with 'anormal' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All count in 0.397 secs\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "raw_data_count = raw_data.count()\n",
    "tt = time.time() - t0\n",
    "print(\"All count in {} secs\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal count in 0.464 secs\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "normal_raw_data_count = normal_raw_data.count()\n",
    "tt = time.time() - t0\n",
    "print(\"Normal count in {} secs\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack count in 2.01 secs\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "attack_raw_data_count = attack_raw_data.count()\n",
    "tt = time.time() - t0\n",
    "print(\"Attack count in {} secs\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 97278 interactions normales et 396743 attaques, pour un total de 494021 interactions\n"
     ]
    }
   ],
   "source": [
    "print(\"Il y a {} interactions normales et {} attaques, pour un total de {} interactions\".format(normal_raw_data_count,attack_raw_data_count,raw_data_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cartesian\n",
    "\n",
    "The cartesian product (`.cartesian`) returns every possible pair between elements of two rdd. (To be used with caution if RDDs are really big)\n",
    "\n",
    "**Exercise**\n",
    "Write function do display all possible pair Protocol (Second columns) and Services (Third column) within the dataset using  `.cartesian` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise1_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More **Actions** on RDDs\n",
    "So far we have used only *native* action function (count, distinct, take etc..) We will now define custom action with `reduce` and `aggregate` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reduce` function take a function as an argument that will describe how elements from the RDD are combined.\n",
    "\n",
    "The code below will build two rdd containings duration time from normal and attack data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_csv_data =csv_data.filter(lambda x : x[-1]==\"normal.\")\n",
    "attack_csv_data =csv_data.filter(lambda x : x[-1]!=\"normal.\")\n",
    "\n",
    "normal_duration_data = normal_csv_data.map(lambda x: int(x[0]))\n",
    "attack_duration_data = attack_csv_data.map(lambda x: int(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use them `reduce` function to compute the total duration of all these actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duration for 'normal' interactions is 21075991\n",
      "Total duration for 'attack' interactions is 2626792\n"
     ]
    }
   ],
   "source": [
    "total_normal_duration = normal_duration_data.reduce(lambda x, y: x + y)\n",
    "total_attack_duration = attack_duration_data.reduce(lambda x, y: x + y)\n",
    "\n",
    "print(\"Total duration for 'normal' interactions is {}\".\\\n",
    "    format(total_normal_duration))\n",
    "print(\"Total duration for 'attack' interactions is {}\".\\\n",
    "    format(total_attack_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we compute the means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean duration for 'normal' interactions is 216.657\n",
      "Mean duration for 'attack' interactions is 6.621\n"
     ]
    }
   ],
   "source": [
    "normal_count = normal_duration_data.count()\n",
    "attack_count = attack_duration_data.count()\n",
    "\n",
    "print(\"Mean duration for 'normal' interactions is {}\".\\\n",
    "    format(round(total_normal_duration/float(normal_count),3)))\n",
    "print(\"Mean duration for 'attack' interactions is {}\".\\\n",
    "    format(round(total_attack_duration/float(attack_count),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above looks quite complicated for such a simple operation. For example the reduce function does not allow to define a \"start\" value, like in Python. \n",
    "\n",
    "**Question** : How would you implement the `count` function with `reduce` function in Python?  Why this can't be applied directly on Spark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load solutions/exercise1_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### aggregate\n",
    "\n",
    "The `aggregate` function allows to overcome the problem. The function takes as arguement : \n",
    "1. The initialization\n",
    "2. A function that describes how element of the rdd are combined\n",
    "3. A function that describes how element from different cluster will be combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21075991"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_sum_duration = normal_duration_data.aggregate(0, # valeurs initiales à 0\n",
    "    lambda acc, value: acc + value, # Somme des durées et cumul des interactions\n",
    "    lambda acc1, acc2: acc1+ acc2 # cumul des accumualteurs\n",
    "    )\n",
    "normal_sum_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregate function could be used to compute both total sum of duration and count of element in one call of the function.\n",
    "\n",
    "**Exercise** Write a function that return total duraction of normal attack AND the count of normal attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durée moyenne des interactions agressives 6.621\n"
     ]
    }
   ],
   "source": [
    "attack_sum_count = attack_duration_data.aggregate(\n",
    "    (0,0), # the initial value\n",
    "    #todo;\n",
    "    #todo\n",
    "    )\n",
    "\n",
    "print(\"Durée moyenne des interactions agressives {}\".\\\n",
    "    format(round(attack_sum_count[0]/float(attack_sum_count[1]),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% load solutions/exercise1_3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReducebyKey & CombineByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ReducebyKey` & `CombineByKey` are function that allow to apply reduce or aggregate function by key (which is the base of MapReduce function.\n",
    "\n",
    "We need for that, to define a RDD where for each entry, the first element is the key, and the second element is the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('normal.', 0.0)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_value_duration = csv_data.map(lambda x: (x[41], float(x[0]))) # x[41] contient le type normal ou non\n",
    "key_value_duration.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducee By Key\n",
    "\n",
    "\n",
    "The `reduceByKey` function is the used to apply reduce function for each key on the first columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('normal.', 21075991.0),\n",
       " ('buffer_overflow.', 2751.0),\n",
       " ('loadmodule.', 326.0),\n",
       " ('perl.', 124.0),\n",
       " ('neptune.', 0.0),\n",
       " ('smurf.', 0.0),\n",
       " ('guess_passwd.', 144.0),\n",
       " ('pod.', 0.0),\n",
       " ('teardrop.', 0.0),\n",
       " ('portsweep.', 1991911.0),\n",
       " ('ipsweep.', 43.0),\n",
       " ('land.', 0.0),\n",
       " ('ftp_write.', 259.0),\n",
       " ('back.', 284.0),\n",
       " ('imap.', 72.0),\n",
       " ('satan.', 64.0),\n",
       " ('phf.', 18.0),\n",
       " ('nmap.', 0.0),\n",
       " ('multihop.', 1288.0),\n",
       " ('warezmaster.', 301.0),\n",
       " ('warezclient.', 627563.0),\n",
       " ('spy.', 636.0),\n",
       " ('rootkit.', 1008.0)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "durations_by_key = key_value_duration.reduceByKey(lambda x, y: x + y)\n",
    "durations_by_key.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that `reduceByKey` is a **Transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'normal.': 97278,\n",
       "             'buffer_overflow.': 30,\n",
       "             'loadmodule.': 9,\n",
       "             'perl.': 3,\n",
       "             'neptune.': 107201,\n",
       "             'smurf.': 280790,\n",
       "             'guess_passwd.': 53,\n",
       "             'pod.': 264,\n",
       "             'teardrop.': 979,\n",
       "             'portsweep.': 1040,\n",
       "             'ipsweep.': 1247,\n",
       "             'land.': 21,\n",
       "             'ftp_write.': 8,\n",
       "             'back.': 2203,\n",
       "             'imap.': 12,\n",
       "             'satan.': 1589,\n",
       "             'phf.': 4,\n",
       "             'nmap.': 231,\n",
       "             'multihop.': 7,\n",
       "             'warezmaster.': 20,\n",
       "             'warezclient.': 1020,\n",
       "             'spy.': 2,\n",
       "             'rootkit.': 10})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_by_key = key_value_data.countByKey()\n",
    "counts_by_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combineByKey\n",
    "\n",
    "`combineByKey` if for `aggregate` whate `reduceByKey` is to `reduce` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('normal.', (21075991.0, 97278)),\n",
       " ('buffer_overflow.', (2751.0, 30)),\n",
       " ('loadmodule.', (326.0, 9)),\n",
       " ('perl.', (124.0, 3)),\n",
       " ('neptune.', (0.0, 107201)),\n",
       " ('smurf.', (0.0, 280790)),\n",
       " ('guess_passwd.', (144.0, 53)),\n",
       " ('pod.', (0.0, 264)),\n",
       " ('teardrop.', (0.0, 979)),\n",
       " ('portsweep.', (1991911.0, 1040)),\n",
       " ('ipsweep.', (43.0, 1247)),\n",
       " ('land.', (0.0, 21)),\n",
       " ('ftp_write.', (259.0, 8)),\n",
       " ('back.', (284.0, 2203)),\n",
       " ('imap.', (72.0, 12)),\n",
       " ('satan.', (64.0, 1589)),\n",
       " ('phf.', (18.0, 4)),\n",
       " ('nmap.', (0.0, 231)),\n",
       " ('multihop.', (1288.0, 7)),\n",
       " ('warezmaster.', (301.0, 20)),\n",
       " ('warezclient.', (627563.0, 1020)),\n",
       " ('spy.', (636.0, 2)),\n",
       " ('rootkit.', (1008.0, 10))]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_counts = key_value_duration.combineByKey(\n",
    "    (lambda x: (x, 1)), # valeur initiale x and compteur 1\n",
    "    (lambda acc, value: (acc[0]+value, acc[1]+1)), # Combiner une paire  avec une paires d'accumulateurs (somme et incrément)\n",
    "    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combinaison des accumulateurs\n",
    "     )\n",
    "sum_counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('normal.', 216.657),\n",
       " ('buffer_overflow.', 91.7),\n",
       " ('loadmodule.', 36.222),\n",
       " ('perl.', 41.333),\n",
       " ('neptune.', 0.0),\n",
       " ('smurf.', 0.0),\n",
       " ('guess_passwd.', 2.717),\n",
       " ('pod.', 0.0),\n",
       " ('teardrop.', 0.0),\n",
       " ('portsweep.', 1915.299),\n",
       " ('ipsweep.', 0.034),\n",
       " ('land.', 0.0),\n",
       " ('ftp_write.', 32.375),\n",
       " ('back.', 0.129),\n",
       " ('imap.', 6.0),\n",
       " ('satan.', 0.04),\n",
       " ('phf.', 4.5),\n",
       " ('nmap.', 0.0),\n",
       " ('multihop.', 184.0),\n",
       " ('warezmaster.', 15.05),\n",
       " ('warezclient.', 615.258),\n",
       " ('spy.', 318.0),\n",
       " ('rootkit.', 100.8)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_means_by_type = sum_counts.map(lambda lambda_args: (lambda_args[0], round(lambda_args[1][0]/lambda_args[1][1],3)))\n",
    "duration_means_by_type.collect()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (PySpark)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "492px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
