{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" width=400, style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 5 Introduction to Recommendation System with Collaborative Filtering  -  Part 3 : Latent Vector-Based Methods with `Keras` Python Library.\n",
    "\n",
    "The objectives of this notebook are the following : \n",
    "\n",
    "* Build Keras models to learn embedding space for user and item data.\n",
    "* Visualize these space.\n",
    "* Use results of algorithm to apply recommendation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.models as km\n",
    "import sklearn.metrics as sm\n",
    "import sklearn.decomposition as sdec\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We download the updated ratings data generated in the first notebook. `1-Python-Neighborhood-MovieLens.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"movielens_small/\"\n",
    "rating = pd.read_csv(DATA_DIR + \"ratings_updated.csv\")\n",
    "nb_entries = rating.shape[0]\n",
    "print(\"Number of entries : %d \" %nb_entries)\n",
    "rating.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create two new columns. The column **user_id** (resp. **item_id**) rearange the userId (resp. MovieId) columns in order that these columns lies in the range(0,609) (resp. range(0,0723))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userIdToNormUserId = {k:v for v,k in enumerate(rating.userId.unique())}\n",
    "rating[\"user_id\"] = [userIdToNormUserId[x] for x in rating.userId.values]\n",
    "itemIdToNormItemId = {k:v for v,k in enumerate(rating.movieId.unique())}\n",
    "rating[\"item_id\"] = [itemIdToNormItemId[x] for x in rating.movieId.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(DATA_DIR + \"movies.csv\")\n",
    "id_movie_to_title = dict(movies[[\"movieId\",\"title\"]].values)\n",
    "id_item_to_title = {itemIdToNormItemId[k]:v for k,v in id_movie_to_title.items() if k in itemIdToNormItemId}\n",
    "print(\"Number of movies in the dictionary : %d\" %(len(id_item_to_title)))\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the same train/test dataset that the one in the first notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = rating[rating.test_train==\"train\"]\n",
    "user_id_train = train['user_id']\n",
    "item_id_train = train['item_id']\n",
    "rating_train = train['rating']\n",
    "print(train.shape)\n",
    "\n",
    "test = rating[rating.test_train==\"test\"]\n",
    "user_id_test = test['user_id']\n",
    "item_id_test = test['item_id']\n",
    "rating_test = test['rating']\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Recommender System\n",
    "\n",
    "We first build a very simple recommender according to this architecture:\n",
    "\n",
    "![alt text](images/simple_architecture.png)\n",
    "\n",
    "Let's decompose the construction of this network.\n",
    "\n",
    "\n",
    "We first create the inputs layer, which will take as entry the id of the user and the id of the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sample we input the integer identifiers of a single user and a single item\n",
    "user_id_input = kl.Input(shape=[1], name='user')\n",
    "item_id_input = kl.Input(shape=[1], name='item')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This id we will then be converted in their embedding space. This can be easily done with the `Embedding` layer object of Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_user_id= rating.user_id.max()\n",
    "max_item_id= rating.item_id.max()\n",
    "embedding_size = 30\n",
    "user_embedding = kl.Embedding(output_dim=embedding_size, input_dim=max_user_id + 1,\n",
    "                           input_length=1, name='user_embedding')(user_id_input)\n",
    "item_embedding = kl.Embedding(output_dim=embedding_size, input_dim=max_item_id + 1,\n",
    "                           input_length=1, name='item_embedding')(item_id_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the dot product of the two vectors which are the vectors representation in the embedding space of the user and the item given in input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape from shape: (batch_size, input_length, embedding_size)\n",
    "# to shape: (batch_size, input_length * embedding_size) which is\n",
    "# equal to shape: (batch_size, embedding_size)\n",
    "user_vecs = kl.Flatten()(user_embedding)\n",
    "item_vecs = kl.Flatten()(item_embedding)\n",
    "\n",
    "y = kl.Dot(axes=1)([user_vecs, item_vecs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the complete model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = km.Model(inputs=[user_id_input, item_id_input], outputs=y)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction can now be applied by giving the list of user and item ids that we want to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_train_preds = model.predict([user_id_train, item_id_train])\n",
    "initial_train_preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, as the model has not been traine, the Model error is quite bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random init MSE: %0.3f\" % sm.mean_squared_error(initial_train_preds, rating_train))\n",
    "print(\"Random init MAE: %0.3f\" % sm.mean_absolute_error(initial_train_preds, rating_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([user_id_train, item_id_train], rating_train,\n",
    "                    batch_size=64, epochs=10, validation_split=0.1,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "\n",
    "- Why is the train loss higher than the first loss in the first few epochs?\n",
    "- Why is Keras not computing the train loss on the full training set at the end of each epoch as it does on the validation set?\n",
    "\n",
    "\n",
    "Now that the model is trained, the model MSE and MAE look nicer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict([user_id_test, item_id_test])\n",
    "print(\"Final test MSE: %0.3f\" % sm.mean_squared_error(test_preds, rating_test))\n",
    "print(\"Final test MAE: %0.3f\" % sm.mean_absolute_error(test_preds, rating_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict([user_id_train, item_id_train])\n",
    "print(\"Final train MSE: %0.3f\" % sm.mean_squared_error(train_preds, rating_train))\n",
    "print(\"Final train MAE: %0.3f\" % sm.mean_absolute_error(train_preds, rating_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** What do you think about those results? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Deep recommender model\n",
    "\n",
    "Let's know compute a deeper architecture in order to improve those results.\n",
    "\n",
    "![alt text](images/deep_architecture.png)\n",
    "\n",
    "\n",
    "**Exercise** : Implement a model similar to the previous one with:\n",
    "\n",
    "* A concatenate layer (look at the kl.Concatenate function)\n",
    "* A dropout layer (rate=0.5) after the concatenate layer.\n",
    "* only one Hidden layer with 64 neurons and relu activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise_3_1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([user_id_train, item_id_train], rating_train,\n",
    "                    batch_size=64, epochs=5, validation_split=0.1,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict([user_id_train, item_id_train])\n",
    "print(\"Final train MSE: %0.3f\" % sm.mean_squared_error(train_preds, rating_train))\n",
    "print(\"Final train MAE: %0.3f\" % sm.mean_absolute_error(train_preds, rating_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict([user_id_test, item_id_test])\n",
    "print(\"Final test MSE: %0.3f\" % sm.mean_squared_error(test_preds, rating_test))\n",
    "print(\"Final test MAE: %0.3f\" % sm.mean_absolute_error(test_preds, rating_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What can you say about those results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploiting the model\n",
    "\n",
    "In this section we will see how to explore both the model and the embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding similar items and user.\n",
    "\n",
    "We want to find the K closest element of an item or a user. The model e build can't be used directly as it take into account a user and a item and not two user nor two items.\n",
    "\n",
    "But we can't easily build a method based on the constructed embedding space. Let's first get the embedding matices of the user and the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()\n",
    "user_embeddings = weights[0]\n",
    "print(\"User embedding matrix dimension : %s\" %str(user_embeddings.shape))\n",
    "item_embeddings = weights[1]\n",
    "print(\"item embedding matrix dimension : %s\" %str(item_embeddings.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the id of an item we compute the distance (*cosine*, *euclidean*, etc.) of its embedding vector to all embedding vectors of the items. \n",
    "\n",
    "(The procedure would be the same for the user, but the results are easier to interpreted with the movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1027\n",
    "X = np.expand_dims(item_embeddings[idx],axis=0)\n",
    "distX = sm.pairwise_distances(X, item_embeddings, metric=\"cosine\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 10 items of the item \"idx\" are then the ten items that are the closest to this items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 items similar to movies %s\" %str(id_item_to_title[idx]))\n",
    "mostSimilarItem = pd.DataFrame([[id_item_to_title[x], distX[x],x] for x in distX.argsort()[:10]])\n",
    "mostSimilarItem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What do you think of these results?  Unfortunalty the dataset is to small to really get good meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaItems = sdec.PCA(n_components=2)\n",
    "items_pca_embeddings = pcaItems.fit_transform(item_embeddings)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(items_pca_embeddings[:,0], item_embeddings[:,1], linestyle=\"None\", marker=\".\")\n",
    "ax.plot(items_pca_embeddings[mostSimilarItem[2].values,0], item_embeddings[mostSimilarItem[2].values,1], linestyle=\"None\", marker=\".\", markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A recommendation function for a given user\n",
    "\n",
    "Once the model is trained, the system can be used to recommend a few items for a user, that he/she hasn't already seen:\n",
    "\n",
    "First let's select a user and display the movies he likes or dislikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 0\n",
    "rating_user = rating[rating[\"user_id\"]==user_id]\n",
    "rating_user_sorted = rating_user.sort_values(\"rating\")\n",
    "print(\"10 best rated movies by user %d\" %user_id)\n",
    "display(rating_user_sorted[-10:][[\"movie\",\"rating\"]])\n",
    "print(\"10 worst rated movies by user %d\" %user_id)\n",
    "display(rating_user_sorted[:10][[\"movie\",\"rating\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Use the model to compute the estimated rates that the user would give to the movies he hasn't seen. Display the 10 movies you would recommend to him."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise_3_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete dataset\n",
    "\n",
    "The following code perform the same model on the complete dataset. \n",
    "It would take too much time if you don't have a GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"ml-25/\"\n",
    "rating = pd.read_csv(DATA_DIR + \"ratings_updated.csv\")\n",
    "nb_entries = rating.shape[0]\n",
    "print(\"Number of entries : %d \" %nb_entries)\n",
    "rating.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(DATA_DIR + \"movies.csv\")\n",
    "id_movie_to_title = dict(movies[[\"movieId\",\"title\"]].values)\n",
    "id_item_to_title = {itemIdToNormItemId[k]:v for k,v in id_movie_to_title.items() if k in itemIdToNormItemId}\n",
    "print(\"Number of movies in the dictionary : %d\" %(len(id_item_to_title)))\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userIdToNormUserId = {k:v for v,k in enumerate(rating.userId.unique())}\n",
    "rating[\"user_id\"] = [userIdToNormUserId[x] for x in rating.userId.values]\n",
    "itemIdToNormItemId = {k:v for v,k in enumerate(rating.movieId.unique())}\n",
    "rating[\"item_id\"] = [itemIdToNormItemId[x] for x in rating.movieId.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = rating[rating.test_train==\"train\"]\n",
    "user_id_train = train['user_id']\n",
    "item_id_train = train['item_id']\n",
    "rating_train = train['rating']\n",
    "print(train.shape)\n",
    "\n",
    "test = rating[rating.test_train==\"test\"]\n",
    "user_id_test = test['user_id']\n",
    "item_id_test = test['item_id']\n",
    "rating_test = test['rating']\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_input = kl.Input(shape=[1], name='user')\n",
    "item_id_input = kl.Input(shape=[1], name='item')\n",
    "\n",
    "embedding_size = 30\n",
    "max_user_id= rating.user_id.max()\n",
    "max_item_id= rating.item_id.max()\n",
    "user_embedding = kl.Embedding(output_dim=embedding_size, input_dim=max_user_id + 1,\n",
    "                           input_length=1, name='user_embedding')(user_id_input)\n",
    "item_embedding = kl.Embedding(output_dim=embedding_size, input_dim=max_item_id + 1,\n",
    "                           input_length=1, name='item_embedding')(item_id_input)\n",
    "\n",
    "# reshape from shape: (batch_size, input_length, embedding_size)\n",
    "# to shape: (batch_size, input_length * embedding_size) which is\n",
    "# equal to shape: (batch_size, embedding_size)\n",
    "user_vecs = kl.Flatten()(user_embedding)\n",
    "item_vecs = kl.Flatten()(item_embedding)\n",
    "\n",
    "input_vecs = kl.Concatenate()([user_vecs, item_vecs])\n",
    "input_vecs = kl.Dropout(0.5)(input_vecs)\n",
    "\n",
    "x = kl.Dense(64, activation='relu')(input_vecs)\n",
    "y = kl.Dense(1)(x)\n",
    "\n",
    "model = km.Model(inputs=[user_id_input, item_id_input], outputs=y)\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([user_id_train, item_id_train], rating_train,\n",
    "                    batch_size=2048, epochs=10, validation_split=0.1,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()\n",
    "user_embeddings = weights[0]\n",
    "print(\"User embedding matrix dimension : %s\" %str(user_embeddings.shape))\n",
    "item_embeddings = weights[1]\n",
    "print(\"item embedding matrix dimension : %s\" %str(item_embeddings.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 283\n",
    "X = np.expand_dims(item_embeddings[idx],axis=0)\n",
    "distX = sm.pairwise_distances(X, item_embeddings, metric=\"cosine\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 items similar to movies %s\" %str(id_item_to_title[idx]))\n",
    "mostSimilarItem = pd.DataFrame([[id_item_to_title[x], distX[x],x] for x in distX.argsort()[:10]])\n",
    "mostSimilarItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaItems = sdec.PCA(n_components=2)\n",
    "items_pca_embeddings = pcaItems.fit_transform(item_embeddings)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(items_pca_embeddings[:,0], item_embeddings[:,1], linestyle=\"None\", marker=\".\")\n",
    "ax.plot(items_pca_embeddings[mostSimilarItem[2].values,0], item_embeddings[mostSimilarItem[2].values,1], linestyle=\"None\", marker=\".\", markersize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 1\n",
    "rating_user = rating[rating[\"user_id\"]==user_id]\n",
    "rating_user_sorted = rating_user.sort_values(\"rating\")\n",
    "print(\"10 best rated movies by user %d\" %user_id)\n",
    "display(rating_user_sorted[-10:][[\"movie\",\"rating\"]])\n",
    "print(\"10 worst rated movies by user %d\" %user_id)\n",
    "display(rating_user_sorted[:10][[\"movie\",\"rating\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run prediction for all movies\n",
    "prediction = model.predict([[user_id for _ in range(max_item_id)], [x for x in range(max_item_id)]])\n",
    "#Concatenate results with id of the movie\n",
    "prediction_with_id = zip(prediction, [x for x in range(max_item_id)])\n",
    "# Filter on unseen movie, get the title and sort the results according to predicted rate\n",
    "prediction_of_unseen_movie = sorted([[p[0],id_item_to_title[x]] for p,x in prediction_with_id if not(x in seen_movie)], key=lambda x :x[0], reverse = True)\n",
    "#Display it.\n",
    "pd.DataFrame(prediction_of_unseen_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
