{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 200px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [High Dimensional Statistics](https://github.com/wikistat/HDStat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Reconnaissance de caractères manuscrits](https://github.com/wikistat/Ateliers-Big-Data/2-MNIST) ([MNIST](http://yann.lecun.com/exdb/mnist/)) par *deep learning* avec <a href=\"https://keras.io/\"><img src=\"https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png\" style=\"max-width: 100px; display: inline\" alt=\"Keras\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résumé\n",
    "\n",
    "## 1  Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Objectif\n",
    "Ce calepin reprend le même objectif que les calepins de l'[Atelier MNIST](https://github.com/wikistat/Ateliers-Big-Data/tree/master/2-MNIST) sur les mêmes données cette fois avec les librairies Keras et tensorFlow pour aborder l'apprentissage profond. Il est une adpatation du tutoriel de Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import keras.utils as ku\n",
    "import keras.models as km\n",
    "import keras.layers as kl\n",
    "import keras.optimizers as ko\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Lecture des données d'apprentissage et de test\n",
    "\n",
    "Les données peuvent être préalablement téléchargées ou directement lues. Ce sont celles originales du site [MNIST DataBase](http://yann.lecun.com/exdb/mnist/) mais préalablement converties au format .csv, certes plus volumineux mais plus facile à lire. Attention le fichier `mnist_train.zip` présent dans le dépôt est compressé. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture des données d'apprentissage\n",
    "N_classes = 10\n",
    "\n",
    "# path=\"\" # Si les données sont dans le répertoire courant sinon:\n",
    "path=\"../1-MNIST/\"\n",
    "Dtrain=pd.read_csv(path+\"mnist_train.csv\",header=None)\n",
    "X_train = Dtrain.values[:,:-1]\n",
    "Y_train = Dtrain.values[:,-1]\n",
    "\n",
    "Dtest=pd.read_csv(path+\"mnist_test.csv\",header=None)\n",
    "X_test = Dtest.values[:,:-1]\n",
    "Y_test = Dtest.values[:,-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Attention*, avec Keras, la variable réponse doit être une matrice binaire où chaque classe est représentée par une indicatrice: pour chaque individu, l'élément de la colone correspondant à la classe à laquelle il appartient est à 1, sinon il est à 0. \n",
    "\n",
    "Keras possède une fonction `to_catergorical` permettant de convertir directement le vecteur de variable `Y_train`, de réponse en matrice indicatrice`Y_train_cat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train_cat = ku.to_categorical(Y_train, N_classes)\n",
    "Y_test_cat = ku.to_categorical(Y_test, N_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Apprentissage et prévision du test\n",
    "\n",
    "### 2.1 MLP Classifier\n",
    "Première tentative d'appliquer un réseaux de neurone MultiPerceptron classique constitué de 4 couches: \n",
    "* Dense: 52 neurones + Foncton d'activation \"relu\"\n",
    "* Dropout: 20% des neurones tiré aléatoirement sont desactivés\n",
    "* Dense: 52 neurones + Foncton d'activation \"relu\"\n",
    "* Dropout: 20% des neurones tiré aléatoirement sont desactivés\n",
    "\n",
    "Une couche softmax permettant la classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "model = km.Sequential()\n",
    "model.add(kl.Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(kl.Dropout(0.2))\n",
    "model.add(kl.Dense(512, activation='relu'))\n",
    "model.add(kl.Dropout(0.2))\n",
    "model.add(kl.Dense(N_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=ko.RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "ts = time.time()\n",
    "history = model.fit(X_train, Y_train_cat,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, Y_test_cat))\n",
    "te = time.time()\n",
    "t_train_mpl = te-ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats sont assez médiocres puisque l'on obtient seulement 19,33% d'images bien classée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mpl = model.evaluate(X_test, Y_test_cat, verbose=0)\n",
    "predict_mpl = model.predict(X_test)\n",
    "print('Test loss:', score_mpl[0])\n",
    "print('Test accuracy:', score_mpl[1])\n",
    "print(\"Time Running: %.2f seconds\" %t_train_mpl )\n",
    "pd.DataFrame(confusion_matrix(Y_test, predict_mpl.argmax(1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 MLP With normalized Data\n",
    "\n",
    "Même modèle mais avec normalisation des données en divisant par leur valeurs maximal (ici 255)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "X_train_norm = X_train/255\n",
    "X_test_norm = X_test/255\n",
    "\n",
    "model = km.Sequential()\n",
    "model.add(kl.Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(kl.Dropout(0.2))\n",
    "model.add(kl.Dense(512, activation='relu'))\n",
    "model.add(kl.Dropout(0.2))\n",
    "model.add(kl.Dense(N_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=ko.RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "ts=time.time()\n",
    "history = model.fit(X_train_norm, Y_train_cat,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test_norm, Y_test_cat))\n",
    "te=time.time()\n",
    "t_train_mpl_norm = te-ts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le résultat est cette fois de 98% d'image bien classé!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mpl_norm = model.evaluate(X_test, Y_test_cat, verbose=0)\n",
    "predict_mpl_norm = model.predict(X_test)\n",
    "print('Test loss:', score_mpl_norm[0])\n",
    "print('Test accuracy:', score_mpl_norm[1])\n",
    "print(\"Time Running: %.2f seconds\" %t_train_mpl_norm )\n",
    "pd.DataFrame(confusion_matrix(Y_test, predict_mpl_norm.argmax(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Convolutional Network\n",
    "\n",
    "Test d'un réseau de convolution constitué de 7 couches: \n",
    "\n",
    "* Une couche de convolution 2D, avec fenêtre de convolution de taille 3x3 et une fonction d'activation de type \"relu\"\n",
    "* Une couche de convolution 2D, avec fenêtre de convolution de taille 3x3 et une fonction d'activation de type \"relu\"\n",
    "* Une couche max pooling de fenêtre 2x2\n",
    "* Une couche dropout où 25% des neurones sont desactivés\n",
    "* Une couche \"Flatten\" permetant de \"remettre  à plat\" les images de taille $NxN$ en vecteur de tailles $N^2$.\n",
    "* Une couche de 128 neurones classiques\n",
    "* Une couche dropout ou 50% des neurones sont desactivés\n",
    "\n",
    "Une couche softmax permettant la classification\n",
    "\n",
    "#### Format des données\n",
    "\n",
    "Dans les exemples précédents. Les données était \"applaties\". Une imade de $28\\times 28=784$ pixels est considérée comme un vecteur. \n",
    "\n",
    "Pour pouvoir utiliser le principe de la convolution la structure des images est conservée. Une image n'est pas un vecteur de tailles $784\\times 1$ mais une matrice de taille $28\\times 28$. Ainsi `X_train` est réorganisée en cube ou multitableau de dimensions $60000\\times 28\\times 28$ pour être utilisé dans un réseau de convolution.\n",
    "\n",
    "Avec **Keras** `X_train` doit même être de dimensions $60000\\times 28\\times 28\\times 1$. La dernière dimension, de taille 1 peut paraitre inutile. Elle l'est dans le cas des données *MNIST* car les pixels ne sont décrits qu'avec un seul niveau de gris. Cependant, des images couleurs en RGB sont généralement codées avec trois niveaux d'intensité (Rouge, Vert et Bleus) correspondant à la quatrième dimension. \n",
    "\n",
    "Noter également que l'utilisation des couches de convolution rend inutile la normalisation préalable des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_conv = X_train.reshape(60000, 28, 28, 1)\n",
    "X_test_conv = X_test.reshape(10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = km.Sequential()\n",
    "model.add(kl.Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28,28, 1), data_format=\"channels_last\"))\n",
    "model.add(kl.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(kl.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(kl.Dropout(0.25))\n",
    "model.add(kl.Flatten())\n",
    "model.add(kl.Dense(128, activation='relu'))\n",
    "model.add(kl.Dropout(0.5))\n",
    "model.add(kl.Dense(N_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=ko.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "ts=time.time()\n",
    "model.fit(X_train_conv, Y_train_cat,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test_conv, Y_test_cat))\n",
    "te=time.time()\n",
    "t_train_conv = te-ts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_conv = model.evaluate(X_test_conv, Y_test_cat, verbose=0)\n",
    "predict_conv = model.predict(X_test_conv)\n",
    "print('Test loss:', score_conv[0])\n",
    "print('Test accuracy:', score_conv[1])\n",
    "print(\"Time Running: %.2f seconds\" %t_train_conv )\n",
    "pd.DataFrame(confusion_matrix(Y_test, predict_conv.argmax(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeErreur1 = pd.read_csv(\"/Users/bguillouet/Insa/TP_Insa/data/data_erreur_mnist_scikit_learn_100_trees.csv\")\n",
    "score_RF_100 = dataframeErreur1[\"Erreur\"].values[-1]\n",
    "temps_RF_100 = dataframeErreur1[\"Temps\"].values[-1]\n",
    "dataframeErreur= pd.read_csv(\"/Users/bguillouet/Insa/TP_Insa/data/data_erreur_mnist_scikit_learn_250_trees.csv\")\n",
    "score_RF_250 = dataframeErreur[\"Erreur\"].values[-1]\n",
    "temps_RF_250 = dataframeErreur[\"Temps\"].values[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap(\"Set1\")\n",
    "color = [cmap(i) for i in range(4)]\n",
    "index = np.arange(4)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "y_score = [score_RF_100, score_RF_250, 1-score_mpl_norm[1], 1-score_conv[1]]\n",
    "rects = ax.bar(index, y_score, 0.9, color=color)\n",
    "ax.set_xlabel('Method',fontsize=20)\n",
    "ax.set_ylabel('Error',fontsize=20)\n",
    "ax.set_title('Error Per Method', fontsize=30)\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(('Random Forest (100 Trees)', 'Random Forest (250 Trees)', 'MLP classifier', 'Convolutional'), \n",
    "                   fontsize=15, rotation=20, ha=\"right\")\n",
    "\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "y_temps = [temps_RF_100, temps_RF_250, t_train_mpl_norm, t_train_conv]\n",
    "rects = ax2.bar(index, y_temps, 0.9, color=color)\n",
    "ax2.set_xlabel('Method',fontsize=20)\n",
    "ax2.set_ylabel('Learning time',fontsize=20)\n",
    "ax2.set_title('Learning time Per Method', fontsize=30)\n",
    "ax2.set_xticks(index)\n",
    "ax2.set_xticklabels(('Random Forest (100 Trees)', 'Random Forest (250 Trees)', 'MLP classifier', 'Convolutional'), \n",
    "                    fontsize=15, rotation=20, ha=\"right\")\n",
    "ax2.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
