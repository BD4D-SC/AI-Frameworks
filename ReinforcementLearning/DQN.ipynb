{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies de l'intelligence Artificielle](https://github.com/wikistat/AI-Frameworks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" width=400, style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 4 Introduction to Reinforcement Learning  -  Part 2 : Q-Learning & Deep Q-Network\n",
    "The objectives of this noteboks are the following : \n",
    "\n",
    "* Q-Learning on simple Markov Decision Process\n",
    "* DQN\n",
    " \n",
    "Source : [https://github.com/ageron/handson-ml](https://github.com/ageron/handson-ml) and https://github.com/breeko/Simple-Reinforcement-Learning-with-Tensorflow/blob/master/Part%202%20-%20Policy-based%20Agents%20with%20Keras.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be using [OpenAI gym](https://gym.openai.com/), a great toolkit for developing and comparing Reinforcement Learning algorithms. It provides many environments for your learning *agents* to interact with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brendanguillouet/anaconda3/envs/INSA/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/brendanguillouet/anaconda3/envs/INSA/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/brendanguillouet/anaconda3/envs/INSA/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/brendanguillouet/anaconda3/envs/INSA/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/brendanguillouet/anaconda3/envs/INSA/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/brendanguillouet/anaconda3/envs/INSA/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot figures and animations\n",
    "%matplotlib inline\n",
    "%matplotlib nbagg\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "\n",
    "import tensorflow.keras.models as km\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.initializers as ki\n",
    "import tensorflow.keras.optimizers as ko\n",
    "import tensorflow.keras.losses as klo\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "# Gym Librairy\n",
    "import gym\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=400):\n",
    "    plt.close()  # or else nbagg sometimes plots in the previous cell\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    return animation.FuncAnimation(fig, update_scene, fargs=(frames, patch), frames=len(frames), repeat=repeat, interval=interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Q-Learning on Markov Decision Process\n",
    "\n",
    "### Definition\n",
    "\n",
    "We will first define a simple markov process on wich we will apply Q-learning algorithm.\n",
    "\n",
    "here is an illustration of the MDP that we will define.\n",
    "\n",
    "![images](images/mdp.png)\n",
    "\n",
    "\n",
    "We store the different action and transition probabilities within a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities = [\n",
    "        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]], \n",
    "        [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "        [None, [0.8, 0.1, 0.1], None],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use pandas DataFrame visualisation in order to display this table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Actions</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.7, 0.3, 0.0]</td>\n",
       "      <td>[1.0, 0.0, 0.0]</td>\n",
       "      <td>[0.8, 0.2, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 1.0, 0.0]</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>[0.8, 0.1, 0.1]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Actions                0                1                2\n",
       "State                                                     \n",
       "0        [0.7, 0.3, 0.0]  [1.0, 0.0, 0.0]  [0.8, 0.2, 0.0]\n",
       "1        [0.0, 1.0, 0.0]             None  [0.0, 0.0, 1.0]\n",
       "2                   None  [0.8, 0.1, 0.1]             None"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_probabilities_df = pd.DataFrame(transition_probabilities).rename_axis('Actions', axis=1)\n",
    "transition_probabilities_df.index.name=\"State\"\n",
    "transition_probabilities_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the rewards, each actions couple state/action will lead to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Actions</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[10, 0, 0]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[0, 0, -50]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[40, 0, 0]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Actions           0           1            2\n",
       "State                                       \n",
       "0        [10, 0, 0]   [0, 0, 0]    [0, 0, 0]\n",
       "1         [0, 0, 0]   [0, 0, 0]  [0, 0, -50]\n",
       "2         [0, 0, 0]  [40, 0, 0]    [0, 0, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = [\n",
    "        [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "        [[0, 0, 0], [+40, 0, 0], [0, 0, 0]],\n",
    "    ]\n",
    "\n",
    "rewards_df = pd.DataFrame(rewards).rename_axis('Actions', axis=1)\n",
    "rewards_df.index.name=\"State\"\n",
    "rewards_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the list of possible action that can be taken at each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_actions = [[0, 1, 2], [0, 2], [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we define now a class that will act as a Gym environment. \n",
    "\n",
    "* The environent is the MDP.\n",
    "* The observation is the current step.\n",
    "* The action possible are the three actions we previously define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDPEnvironment(object):\n",
    "    def __init__(self, start_state=0):\n",
    "        self.start_state=start_state\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.total_rewards = 0\n",
    "        self.state = self.start_state\n",
    "    def step(self, action):\n",
    "        next_state = np.random.choice(range(3), p=transition_probabilities[self.state][action])\n",
    "        reward = rewards[self.state][action][next_state]\n",
    "        self.state = next_state\n",
    "        self.total_rewards += reward\n",
    "        return self.state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(policy, n_steps, start_state=0):\n",
    "    env = MDPEnvironment()\n",
    "    for step in range(n_steps):\n",
    "        action = policy(env.state)\n",
    "        state, reward = env.step(action)\n",
    "    return env.total_rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Coded Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: mean=-19.8, std=84.815932, min=-310, max=210\n"
     ]
    }
   ],
   "source": [
    "def policy_random(state):\n",
    "    return np.random.choice(possible_actions[state])\n",
    "\n",
    "\n",
    "all_score = []\n",
    "for episode in range(1000):\n",
    "    all_score.append(run_episode(policy_random, n_steps=100))\n",
    "print(\"Summary: mean={:.1f}, std={:1f}, min={}, max={}\".format(np.mean(all_score), np.std(all_score), np.min(all_score), np.max(all_score)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Which policy would be the safest? The more risky? Implement it and test it. What can you say about their results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: mean=127.5, std=136.299303, min=-300, max=520\n",
      "Summary: mean=23.3, std=26.702193, min=0, max=180\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/exercise2_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's know implement Q-learning algorithm to learn a bette policy!\n",
    "\n",
    "Q-Learning works by watching an agent play (e.g., randomly) and gradually improving its estimates of the Q-Values. \n",
    "Once it has accurate Q-Value estimates (or close enough), then the optimal policy consists in choosing the action that has the highest Q-Value (i.e., the greedy policy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.],\n",
       "       [  0., -inf,   0.],\n",
       "       [-inf,   0., -inf]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_states = 3\n",
    "n_actions = 3\n",
    "n_steps = 20\n",
    "alpha = 0.01  #<-- Learning Rate\n",
    "gamma = 0.99  #<-- The discount rate\n",
    "\n",
    "\n",
    " \n",
    "exploration_policy = policy_random #<-- Policy that we will play during exploration\n",
    "q_values = np.full((n_states, n_actions), -np.inf) #<-- Policy that we will be updated\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    q_values[state][actions]=0\n",
    "q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.29997   ,  0.00785021,  0.0019601 ],\n",
       "       [ 0.        ,        -inf, -1.95835969],\n",
       "       [       -inf,  1.1939401 ,        -inf]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = MDPEnvironment()\n",
    "for step in range(n_steps):    \n",
    "    action = exploration_policy(env.state) \n",
    "    state = env.state\n",
    "    next_state, reward = env.step(action)\n",
    "    next_value = np.max(q_values[next_state]) # greedy policy\n",
    "    q_values[state, action] = (1-alpha)*q_values[state, action] + alpha*(reward + gamma * next_value)\n",
    "q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** How do we defined the optimal policy from the computed Q_values? Implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(state):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise2_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: mean=24.2, std=28.309405, min=0, max=230\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_totals = []\n",
    "for episode in range(1000):\n",
    "    all_totals.append(run_episode(optimal_policy, n_steps=100))\n",
    "print(\"Summary: mean={:.1f}, std={:1f}, min={}, max={}\".format(np.mean(all_totals), np.std(all_totals), np.min(all_totals), np.max(all_totals)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** We use the Q learning iteration algorithm to learn the best policy. Would it have been possible to use a different algorithm here? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEEP Q Learning on *Pacman-Like* Game\n",
    "\n",
    "We have seen how to learn an optimal policy on a Markov Decision Process. \n",
    "\n",
    "Let's now try to learn how to play a Video Game Like Pacman. \n",
    "\n",
    "We won't use directly Pacman here (Even if it's available on Gym environment (see below)). The Pacman game requires GPU to be trained and may need some try and parameter optimization to easily converged. \n",
    "\n",
    "Hence, we will start with and simpler environment which looks like Pacman : the [gridworld](https://github.com/awjuliani/DeepRL-Agents/blob/master/gridworld.py) environment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid World environment\n",
    "\n",
    "We have a 5x5 blocs where blue, green and red squares. In the environment the agent controls a blue square, and the goal is to navigate to the green squares (reward +1) while avoiding the red squares (reward -1).\n",
    "\n",
    "#### Observation\n",
    "\n",
    "The observation is the image itself which is a 84x84x3 images.\n",
    "\n",
    "#### Actions\n",
    "\n",
    "Num | Action\n",
    "--- | ---\n",
    "0 | Go Up\n",
    "1 | Go Down\n",
    "2 | Go Left\n",
    "3 | Go Right\n",
    "\n",
    "\n",
    "#### Reward\n",
    "Reward is 1 for every green square taken and -1 when a red square is taken\n",
    "\n",
    "#### End epsiode\n",
    "There are no condition limit for an episode to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initiate the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAADFklEQVR4nO3dsQrCQBBF0az4/7+8tqKBVDoXck6phRq4DNi8tfc+gJ7H9BcAzokTosQJUeKEKHFC1PPifX/lwu+tsxddTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQtTVBOCYtU5X0eAn9u6tXbqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEqOwE4DG5yGZ9cEZvhW+UywlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTojKTgDuwRm+ZX5wxuRvD84PupwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUJUdp9zdKbyzhuZZLicECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEqOwE4B787HsvAA4++X3vJ//J5YQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0RlJwCPPTdFNzk/eG+e/DuXE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcEHU1Abj+8i2ALy4nRIkTosQJUeKEKHFClDgh6gUzXxTZ84WrNgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gridworld import gameEnv\n",
    "env = gameEnv(partial=False, size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Let's play a game manually to understand how it works. Fill the actions_list with step in order to win points according to the intialised environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"432\" height=\"288\" controls autoplay>\n",
       "  <source type=\"video/mp4\" src=\"data:video/mp4;base64,AAAAHGZ0eXBNNFYgAAACAGlzb21pc28yYXZjMQAAAAhmcmVlAAAKlG1kYXQAAAKfBgX//5vcRem9\n",
       "5tlIt5Ys2CDZI+7veDI2NCAtIGNvcmUgMTUyIC0gSC4yNjQvTVBFRy00IEFWQyBjb2RlYyAtIENv\n",
       "cHlsZWZ0IDIwMDMtMjAxNyAtIGh0dHA6Ly93d3cudmlkZW9sYW4ub3JnL3gyNjQuaHRtbCAtIG9w\n",
       "dGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFseXNlPTB4MzoweDExMyBtZT1o\n",
       "ZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVkX3JlZj0xIG1lX3JhbmdlPTE2\n",
       "IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MSBjcW09MCBkZWFkem9uZT0yMSwxMSBmYXN0\n",
       "X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD0tMiB0aHJlYWRzPTkgbG9va2FoZWFkX3RocmVhZHM9\n",
       "MSBzbGljZWRfdGhyZWFkcz0wIG5yPTAgZGVjaW1hdGU9MSBpbnRlcmxhY2VkPTAgYmx1cmF5X2Nv\n",
       "bXBhdD0wIGNvbnN0cmFpbmVkX2ludHJhPTAgYmZyYW1lcz0zIGJfcHlyYW1pZD0yIGJfYWRhcHQ9\n",
       "MSBiX2JpYXM9MCBkaXJlY3Q9MSB3ZWlnaHRiPTEgb3Blbl9nb3A9MCB3ZWlnaHRwPTIga2V5aW50\n",
       "PTI1MCBrZXlpbnRfbWluPTIgc2NlbmVjdXQ9NDAgaW50cmFfcmVmcmVzaD0wIHJjX2xvb2thaGVh\n",
       "ZD00MCByYz1jcmYgbWJ0cmVlPTEgY3JmPTIzLjAgcWNvbXA9MC42MCBxcG1pbj0wIHFwbWF4PTY5\n",
       "IHFwc3RlcD00IGlwX3JhdGlvPTEuNDAgYXE9MToxLjAwAIAAAAUGZYiEABT//vfHT8Cm6Plmy51F\n",
       "PSEwitj6SCi9WOzQMnUAAAMAAETAxxFtXcuPAeYAAGEAAJYNmXp7/iq//AALROfwDBjGaEVDoK8q\n",
       "09TZK4+sjOoEhnoXlVY8Ugzz+CRqnjAlUXDHxbFZjE5UobkAFH56bnfmWmQYbUJP1/fUI7hCU5bt\n",
       "D8ZEV4iMKi2ymvQf2K4k7Wx0JTHPVir2ko/SKsdzHhVp9ftlIGwkJj9wvIx0iUdjgKYP2fKMO/h1\n",
       "SRrx0N3WplDRrLEd0m9xZQlz54+pYD/VO7fbnNuP5k5iJW7DKYL5EXNn7tpywg8vrkPz3+55ySTU\n",
       "VQVVHnfhHhMXYJzBV70vgf0GzAnd9/O7zEYzNuXC0EDjYbA0HpLApxr2pILggSdv5l2JM8lfPQ7/\n",
       "44ZlJ7kzWVvck5Hlp4m0xfSiC2BN79Eqx/pxeK0tJQSEZPOqriYf/lS4FS+CZxsyx/X4+DqQqu+F\n",
       "jq1eU8tF13uc05bZS7hiatBwNrsYueZr7oc2ueOoxLdlcV1D97PPOmPiflilEGAU1Xz9/stTZgTm\n",
       "6pV3MhvcA420oQeBo4lL/UB+HKgZWrqld09qLY8Tp2acy541ApH1D8NJm1xX1XTkos6T/84iyqFe\n",
       "kNTBoRf3FPlp5n9D9PD/paOO4UR3K4aFK+dMyENuZOmP0yaDGTydXNVL6184mq/TtcVOewMIW37m\n",
       "0niXheyGpxUMRihjycbmDF5Y9bl3o2Fnu85RlFfsjd7Niws9JH6njf65xLCjxNqPwVwhVkGZ/qce\n",
       "Nzi1krKlhyilsKzdaVmgWMGGisaQMkv3YtJVVpx5XvJSpa9e9TPW4XwNIVHQmsMWLCJZfoJXmEj7\n",
       "3xBjepdxbBr/LxjJGC2qOIovaTAvpVQUDFe0z71ULy7bm8nWye7n0W4YFpkXHx4vnxbwopW/ta7m\n",
       "IiibksYcTWnEHtH2OjuY+2vMNA2Ek+Qp31YMHsk/i0SeoQuCtQndO+OicvbL8JX7WGcPMJxW5sHO\n",
       "pTvxqNBw9MqpZshscroNNJuR/8nVHrVRS+5wjKREE+hhi13Oh9HkJkm70LK31jJWQo2cEf1HsCS9\n",
       "ZGa2UP03IA/A4DCx0lC+TDRSyEpkdNYsgZQTIJ0eOq6YGAtefEtYEqq4PniqocusalQxftKWlj+f\n",
       "w9HVh4evUxOq3wVZH75AIUOsW4s09S/oWgO9OD4pOzXAJ1qpXfqVQnyzsz2qq2Xjg2PQCZn/HY2n\n",
       "izjtfmruSc8TlcuD2WKYRn5n03UIT3x9v/RmeXlll0dG3zNKFn697R1bP7MsmAzn9mY0s74WFGLz\n",
       "cgL1tMgcEO+78Rume38Ur3AdGTlmGe3DOoHh9eI7OU1bnYnsFfDSINTTWZ247ASldyI0FihETuig\n",
       "Aq/uE8kvkR2PBIY9digFyKrF1hV5UalEh9XubzzVHtbrTT9+T9s1AlA2eoC7Qls+DZPBnWGJL7MO\n",
       "G3lZnxN1CcpntlD8tpY9wwpG7cEK8aOFf/yKfkYImaAWeeZ3u0rewunaEdV0Nzu3xH3w+6Q2SeRP\n",
       "/LBcwHlTIz7AkZTw9seJSC/Ybf77qGteFw02xx8jPnnHfIXq5Z6VFGT/TxpV0M3H9ctpj4ZSiWcW\n",
       "u9sdZADyjNcGZw+e7MTJL4fxV1gu7dmpJJQhRnnEEcLtkAC21bAJvWaBc67jorokNteqc8aWqNYW\n",
       "ZAIFoRsITEEAo9kABgUAAAEcQZohbEFP/taMsADFZT2ETBzPgATNlB+RISv7wmfu2Nxl+WCKfkGW\n",
       "6auoeq8jRkUzMeeRPWpjohVq4yGX3Ky3wvFfOB/pierqPyrV8nmFrS+Td8C0mKhUlAzDzNvtPiEK\n",
       "qNfM9GUIzaKdtGzvs95/oE1JLaJF0SznmMl4G7IAF//alkXFyUiz72NkKQhctcnuLCy1aDew2T19\n",
       "5nswusr/v6lc9wHkX6qHMoE+wR9z1xoIeiFbR/di8mFdhseuC8t8ynAKiEkEQJN52P2Lv1JGcusX\n",
       "pyUSpJXKT0FS7hlH7bT3cjs+d9aQ/GCPtlQ3c57He7uQDsk3fILftW/hHvMc1hqtq5A0TEjmTCji\n",
       "ssPS1dFswiTzQ3EHdtQUr5UAAAD7QZpDPCGTKYQS//61KoAC3E4iHpHMUoAFm7LpC9QCLdxOsHbx\n",
       "/+oFxpdq7rz2RDBVVYdXN5pWJ+AU6ud5PZ//9hgDBSgL+HeMRdtSLUPvMFK2XMRURYUQJiCtDeHW\n",
       "2velw/HEeZy6s6HWkd6m53jenlPL4f1Fn2wtxBMz4HXVAxKHr1E8c73wRSx4b1NVK4EFlrnmKzy2\n",
       "vjsyysTkMQyolT12Cd2oYWYXHdvnrr14NKNPBZNNE8offPgVBcqb2CoVLIDVGd/QQ/T2mo1fheaC\n",
       "Kq748iWpnEgm0fKEIoeTCF2nLXRANTtr3MtEcVyFhy0esG1OwTFTeZYR2vEAAAC8AZ5iakEvAAFq\n",
       "7c7t34okboh9gL6f6qCi6aP7A1D5mPNqL2wABdW6RdNLLSqd7jFS4gZOXoBUuSwZueNxHv0dWPyy\n",
       "N9aPrxksBdZKNrYvByqfLYs2YieCdp9RTXz/2iF6w9K9gOuC0Ej5pJXOA0e1Bz+Ul9AXtdtSGKsV\n",
       "yT9husYmHBOWWjgpM7hmSzkUIsTPlYSogCYMgQwA/lcosm+eOcZA+Losa5piryl4ab55dObbxxNo\n",
       "UYmmQ+gVm4AAAANWbW9vdgAAAGxtdmhkAAAAAAAAAAAAAAAAAAAD6AAABkAAAQAAAQAAAAAAAAAA\n",
       "AAAAAAEAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
       "AAAAAAAAAAAAAgAAAoB0cmFrAAAAXHRraGQAAAADAAAAAAAAAAAAAAABAAAAAAAABkAAAAAAAAAA\n",
       "AAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAABAAAAAAbAAAAEgAAAAAAAk\n",
       "ZWR0cwAAABxlbHN0AAAAAAAAAAEAAAZAAAAgAAABAAAAAAH4bWRpYQAAACBtZGhkAAAAAAAAAAAA\n",
       "AAAAAAAoAAAAQABVxAAAAAAALWhkbHIAAAAAAAAAAHZpZGUAAAAAAAAAAAAAAABWaWRlb0hhbmRs\n",
       "ZXIAAAABo21pbmYAAAAUdm1oZAAAAAEAAAAAAAAAAAAAACRkaW5mAAAAHGRyZWYAAAAAAAAAAQAA\n",
       "AAx1cmwgAAAAAQAAAWNzdGJsAAAAs3N0c2QAAAAAAAAAAQAAAKNhdmMxAAAAAAAAAAEAAAAAAAAA\n",
       "AAAAAAAAAAAAAbABIABIAAAASAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
       "AAAAGP//AAAAMWF2Y0MBZAAV/+EAGGdkABWs2UGwloQAAAMACAAAAwAoPFi2WAEABmjr48siwAAA\n",
       "ABx1dWlka2hA8l8kT8W6OaUbzwMj8wAAAAAAAAAYc3R0cwAAAAAAAAABAAAABAAAEAAAAAAUc3Rz\n",
       "cwAAAAAAAAABAAAAAQAAAChjdHRzAAAAAAAAAAMAAAACAAAgAAAAAAEAADAAAAAAAQAAEAAAAAAc\n",
       "c3RzYwAAAAAAAAABAAAAAQAAAAQAAAABAAAAJHN0c3oAAAAAAAAAAAAAAAQAAAetAAABIAAAAP8A\n",
       "AADAAAAAFHN0Y28AAAAAAAAAAQAAACwAAABidWR0YQAAAFptZXRhAAAAAAAAACFoZGxyAAAAAAAA\n",
       "AABtZGlyYXBwbAAAAAAAAAAAAAAAAC1pbHN0AAAAJal0b28AAAAdZGF0YQAAAAEAAAAATGF2ZjU4\n",
       "LjIwLjEwMA==\n",
       "\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAADF0lEQVR4nO3dQWrDUBAFQX2T+195cgALtFKmIVVLe2EL0wx4887MXEDPZ/sLAPfECVHihChxQpQ4Iern4X1/5cL7zt2LLidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCHqaQJwzTm3q2jwipne2qXLCVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiMpOAF6bi2yr64PbU3SLD7/96DEuJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4ISo7ATiLS3RndX5wdX9w1+ajB+cHXU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCEqu8+5ulL5jycy6XA5IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUdkJwFn8bAuASzZ/9CCXE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcEJWdALxmbw/OEh0FLidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCHqaQLw/Mm3AL64nBAlTogSJ0SJE6LECVHihKhfCKsT2zSGXBQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "frames = []\n",
    "cum_reward=0\n",
    "actions_list=[0,2,0,0]\n",
    "for step in actions_list:\n",
    "    state, reward , end = env.step(step)\n",
    "    frames.append(state)\n",
    "    cum_reward+=reward\n",
    "cum_reward\n",
    "HTML(plot_animation(frames).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay Buffer\n",
    "\n",
    "The second major addition to make DQNs work is Experience Replay. The basic idea is that by storing an agent’s experiences, and then randomly drawing batches of them to train the network, we can more robustly learn to perform well in the task. By keeping the experiences we draw random, we prevent the network from only learning about what it is immediately doing in the environment, and allow it to learn from a more varied array of past experiences. Each of these experiences are stored as a tuple of <state,action,reward,next state>. The Experience Replay buffer stores a fixed number of recent memories, and as new ones come in, old ones are removed. When the time comes to train, we simply draw a uniform batch of random memories from the buffer, and train our network with them. For our DQN, we will build a simple class that handles storing and retrieving memories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(self, buffer_size=50000):\n",
    "        \"\"\" Data structure used to hold game experiences \"\"\"\n",
    "        # Buffer will contain [state,action,reward,next_state,done]\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def add(self, experience):\n",
    "        \"\"\" Adds list of experiences to the buffer \"\"\"\n",
    "        # Extend the stored experiences\n",
    "        self.buffer.extend(experience)\n",
    "        # Keep the last buffer_size number of experiences\n",
    "        self.buffer = self.buffer[-self.buffer_size:]\n",
    "\n",
    "    def sample(self, size):\n",
    "        \"\"\" Returns a sample of experiences from the buffer \"\"\"\n",
    "        sample_idxs = np.random.randint(len(self.buffer), size=size)\n",
    "        sample_output = [self.buffer[idx] for idx in sample_idxs]\n",
    "        sample_output = np.reshape(sample_output, (size, -1))\n",
    "        return sample_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture and loss of network\n",
    "\n",
    "#### Q-Learning\n",
    "\n",
    "\n",
    "$$target = R(s,a,s')+\\gamma \\max\\limits_{a'}Q_k(s',a')$$\n",
    "$$Q_{k+1}(s,a)\\leftarrow(1-a)Q_k(s,a)+\\alpha[target]$$\n",
    "\n",
    "#### Deep Q-learning\n",
    "\n",
    "However, if the combinations of states and actions are too large, the memory and the computation requirement for Q will be too high. To address that, we switch to a deep network Q (DQN) to approximate Q(s, a). The learning algorithm is called Deep Q-learning. With the new approach, we generalize the approximation of the Q-value function rather than remembering the solutions.\n",
    "\n",
    "$$target = R(s,a,s')+\\gamma \\max\\limits_{a'}Q_k(s',a')$$\n",
    "$$\\theta_{k+1} \\leftarrow \\theta_k - \\alpha\\nabla_{\\theta}\\mathbb{E}_{s\\sim'P(s'|s,a)} [(Q_{\\theta}(s,a)-target(s'))^2]_{\\theta=\\theta_k} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self):\n",
    "        # The input image of the game is 84 x 84 x 3 (RGB)\n",
    "        self.inputs = kl.Input(shape=[84, 84, 3], name=\"main_input\")\n",
    "\n",
    "        # There will be four layers of convolutions performed on the image input\n",
    "        # A convolution take a portion of an input and matrix multiplies\n",
    "        # a filter on the portion to get a new input (see below)\n",
    "        self.model = kl.Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=[8, 8],\n",
    "            strides=[4, 4],\n",
    "            activation=\"relu\",\n",
    "            padding=\"valid\",\n",
    "            name=\"conv1\")(self.inputs)\n",
    "        self.model = kl.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[4, 4],\n",
    "            strides=[2, 2],\n",
    "            activation=\"relu\",\n",
    "            padding=\"valid\",\n",
    "            name=\"conv2\")(self.model)\n",
    "        self.model = kl.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[1, 1],\n",
    "            activation=\"relu\",\n",
    "            padding=\"valid\",\n",
    "            name=\"conv3\")(self.model)\n",
    "        self.model = kl.Conv2D(\n",
    "            filters=512,\n",
    "            kernel_size=[7, 7],\n",
    "            strides=[1, 1],\n",
    "            activation=\"relu\",\n",
    "            padding=\"valid\",\n",
    "            name=\"conv4\")(self.model)\n",
    "\n",
    "        # We then separate the final convolution layer into an advantage and value\n",
    "        # stream. The value function is how well off you are in a given state.\n",
    "        # The advantage is the how much better off you are after making a particular\n",
    "        # move. Q is the value function of a state after a given action.\n",
    "        # Advantage(state, action) = Q(state, action) - Value(state)\n",
    "        self.model = kl.Flatten()(self.model)\n",
    "        self.model = kl.Dense(256, activation=\"relu\")(self.model)\n",
    "        self.model = kl.Dense(4, activation = \"linear\")(self.model)\n",
    "        self.model = km.Model(self.inputs, self.model)\n",
    "        self.model.compile(\"adam\", \"mse\")\n",
    "        self.model.optimizer.lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/brendanguillouet/anaconda3/envs/INSA/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/brendanguillouet/anaconda3/envs/INSA/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 84, 84, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 20, 20, 32)        6176      \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 1, 1, 512)         1606144   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 1,814,436\n",
      "Trainable params: 1,814,436\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "main_qn = Qnetwork()\n",
    "main_qn.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Class\n",
    "\n",
    "\n",
    "#### Pseudo code\n",
    "\n",
    "\n",
    "#### Separated Target network\n",
    "\n",
    "$$target = R(s,a,s')+\\gamma \\max\\limits_{a'}Q_k(s',a';\\theta) $$\n",
    "\n",
    "\n",
    "We build a deep network to learn the values of Q but its target values are changing as we know things better. As shown below, the target values for Q depends on Q itself, we are chasing a non-stationary target. So we use a different network to generate the target, that will be updated from time to time\n",
    "\n",
    "\n",
    "$$target = R(s,a,s')+\\gamma \\max\\limits_{a'}Q_k(s',a';\\theta_{target}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64  # How many experiences to use for each training step\n",
    "        self.num_epochs = 20  # How many epochs to train\n",
    "        self.update_freq = 5  # How often you update the network\n",
    "        self.y = 0.99  # Discount factor\n",
    "        self.prob_random_start = 0.6  # Starting chance of random action\n",
    "        self.prob_random_end = 0.1  # Ending chance of random action\n",
    "        self.annealing_steps = 1000.  # Steps of training to reduce from start_e -> end_e\n",
    "        self.num_episodes = 10000  # How many episodes of game environment to train\n",
    "        self.pre_train_episodes = 100  # Number of episodes of random actions\n",
    "        self.max_num_step = 50  # Maximum allowed episode length\n",
    "        self. goal = 15\n",
    "\n",
    "        # Reset everything\n",
    "        K.clear_session()\n",
    "\n",
    "        # Setup our Q-networks\n",
    "        self.main_qn = Qnetwork()\n",
    "        self.target_qn = Qnetwork()\n",
    "\n",
    "        # Setup our experience replay\n",
    "        self.experience_replay = ExperienceReplay()\n",
    "\n",
    "    def update_target_graph(self):\n",
    "        updated_weights = np.array(self.main_qn.model.get_weights())\n",
    "        self.target_qn.model.set_weights(updated_weights)\n",
    "\n",
    "\n",
    "    def run_one_episode(self, num_episode, prob_random):\n",
    "        # Create an experience replay for the current episode\n",
    "        episode_buffer = ExperienceReplay()\n",
    "\n",
    "        # Get the game state from the environment\n",
    "        state = env.reset()\n",
    "\n",
    "        done = False  # Game is complete\n",
    "        sum_rewards = 0  # Running sum of rewards in episode\n",
    "        cur_step = 0  # Running sum of number of steps taken in episode\n",
    "\n",
    "        while cur_step < self.max_num_step and not done:\n",
    "            cur_step += 1\n",
    "            if np.random.rand() < prob_random or \\\n",
    "                    num_episode < self.pre_train_episodes:\n",
    "                # Act randomly based on prob_random or if we\n",
    "                # have not accumulated enough pre_train episodes\n",
    "                action = np.random.randint(env.actions)\n",
    "            else:\n",
    "                # Decide what action to take from the Q network\n",
    "                action = np.argmax(self.main_qn.model.predict(np.array([state])))\n",
    "\n",
    "            # Take the action and retrieve the next state, reward and done\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            # Setup the episode to be stored in the episode buffer\n",
    "            episode = np.array([[state], action, reward, [next_state], done])\n",
    "            episode = episode.reshape(1, -1)\n",
    "\n",
    "            # Store the experience in the episode buffer\n",
    "            episode_buffer.add(episode)\n",
    "\n",
    "            # Update the running rewards\n",
    "            sum_rewards += reward\n",
    "\n",
    "            # Update the state\n",
    "            state = next_state\n",
    "\n",
    "        return episode_buffer, sum_rewards, cur_step\n",
    "\n",
    "    def train_one_step(self):\n",
    "        # Train batch is [[state,action,reward,next_state,done],...]\n",
    "        train_batch = self.experience_replay.sample(self.batch_size)\n",
    "\n",
    "        # Separate the batch into its components\n",
    "        train_state, train_action, train_reward, \\\n",
    "        train_next_state, train_done = train_batch.T\n",
    "\n",
    "        # Convert the action array into an array of ints so they can be used for indexing\n",
    "        train_action = train_action.astype(np.int)\n",
    "\n",
    "        # Stack the train_state and train_next_state for learning\n",
    "        train_state = np.vstack(train_state)\n",
    "        train_next_state = np.vstack(train_next_state)\n",
    "\n",
    "        # Our predictions (actions to take) from the main Q network\n",
    "        target_q = self.target_qn.model.predict(train_state)\n",
    "\n",
    "        # Tells us whether game over or not\n",
    "        # We will multiply our rewards by this value\n",
    "        # to ensure we don't train on the last move\n",
    "        train_gameover = train_done == 0\n",
    "\n",
    "        # Q value of the next state based on action\n",
    "        target_q_next_state = self.target_qn.model.predict(train_next_state)\n",
    "        train_next_state_values = np.max(target_q_next_state[range(self.batch_size)], axis=1)\n",
    "\n",
    "        # Reward from the action chosen in the train batch\n",
    "        actual_reward = train_reward + (self.y * train_next_state_values * train_gameover)\n",
    "        target_q[range(self.batch_size), train_action] = actual_reward\n",
    "\n",
    "        # Train the main model\n",
    "        loss = self.main_qn.model.train_on_batch(train_state, target_q)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # Make the networks equal\n",
    "        self.update_target_graph()\n",
    "\n",
    "        # We'll begin by acting complete randomly. As we gain experience and improve,\n",
    "        # we will begin reducing the probability of acting randomly, and instead\n",
    "        # take the actions that our Q network suggests\n",
    "        prob_random = self.prob_random_start\n",
    "        prob_random_drop = (self.prob_random_start - self.prob_random_end) / self.annealing_steps\n",
    "\n",
    "        num_steps = []  # Tracks number of steps per episode\n",
    "        rewards = []  # Tracks rewards per episode\n",
    "        print_every = 50  # How often to print status\n",
    "        losses = [0]  # Tracking training losses\n",
    "        num_episode = 0\n",
    "\n",
    "        while num_episode < self.num_episodes:\n",
    "\n",
    "            episode_buffer, sum_rewards, cur_step = self.run_one_episode(num_episode, prob_random)\n",
    "\n",
    "            if num_episode > self.pre_train_episodes:\n",
    "                # Training the network\n",
    "\n",
    "                if prob_random > self.prob_random_end:\n",
    "                    # Drop the probability of a random action\n",
    "                    prob_random -= prob_random_drop\n",
    "\n",
    "                if num_episode % self.update_freq == 0:\n",
    "                    for num_epoch in range(self.num_epochs):\n",
    "                        loss = self.train_one_step()\n",
    "                        losses.append(loss)\n",
    "\n",
    "                    # Update the target model with values from the main model\n",
    "                    self.update_target_graph()\n",
    "\n",
    "            # Increment the episode\n",
    "            num_episode += 1\n",
    "\n",
    "            self.experience_replay.add(episode_buffer.buffer)\n",
    "            num_steps.append(cur_step)\n",
    "            rewards.append(sum_rewards)\n",
    "\n",
    "            if num_episode % print_every == 0:\n",
    "                # Print progress\n",
    "                mean_loss = np.mean(losses[-(print_every * self.num_epochs):])\n",
    "\n",
    "                print(\"Num episode: {} Mean reward: {:0.4f} Prob random: {:0.4f}, Loss: {:0.04f}\".format(\n",
    "                    num_episode, np.mean(rewards[-print_every:]), prob_random, mean_loss))\n",
    "                if np.mean(rewards[-print_every:]) >= self.goal:\n",
    "                    print(\"Training complete!\")\n",
    "                    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num episode: 50 Mean reward: 1.8800 Prob random: 0.6000, Loss: 0.0000\n",
      "Num episode: 100 Mean reward: 1.3600 Prob random: 0.6000, Loss: 0.0000\n",
      "WARNING:tensorflow:From /Users/brendanguillouet/anaconda3/envs/INSA/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Num episode: 150 Mean reward: 1.5200 Prob random: 0.5755, Loss: 5.5301\n",
      "Num episode: 200 Mean reward: 1.6200 Prob random: 0.5505, Loss: 2.7259\n",
      "Num episode: 250 Mean reward: 1.6400 Prob random: 0.5255, Loss: 1.8145\n",
      "Num episode: 300 Mean reward: 2.0000 Prob random: 0.5005, Loss: 1.3653\n",
      "Num episode: 350 Mean reward: 1.9200 Prob random: 0.4755, Loss: 1.0976\n",
      "Num episode: 400 Mean reward: 2.9800 Prob random: 0.4505, Loss: 0.0856\n",
      "Num episode: 450 Mean reward: 3.3400 Prob random: 0.4255, Loss: 0.0577\n",
      "Num episode: 500 Mean reward: 3.8600 Prob random: 0.4005, Loss: 0.0526\n",
      "Num episode: 550 Mean reward: 4.3200 Prob random: 0.3755, Loss: 0.0515\n",
      "Num episode: 600 Mean reward: 4.5000 Prob random: 0.3505, Loss: 0.0528\n",
      "Num episode: 650 Mean reward: 5.7800 Prob random: 0.3255, Loss: 0.0572\n",
      "Num episode: 700 Mean reward: 6.1600 Prob random: 0.3005, Loss: 0.0603\n",
      "Num episode: 750 Mean reward: 6.8200 Prob random: 0.2755, Loss: 0.0627\n",
      "Num episode: 800 Mean reward: 7.3000 Prob random: 0.2505, Loss: 0.0666\n",
      "Num episode: 850 Mean reward: 8.7400 Prob random: 0.2255, Loss: 0.0700\n",
      "Num episode: 900 Mean reward: 8.2400 Prob random: 0.2005, Loss: 0.0708\n",
      "Num episode: 950 Mean reward: 8.5200 Prob random: 0.1755, Loss: 0.0735\n",
      "Num episode: 1000 Mean reward: 8.2400 Prob random: 0.1505, Loss: 0.0774\n"
     ]
    }
   ],
   "source": [
    "dqn = DQN()\n",
    "dqn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_sum = 0\n",
    "num_games = 100       \n",
    "num_game = 0\n",
    "all_reward_sum = []\n",
    "obs = env.reset()\n",
    "while num_game < num_games:\n",
    "    p_action = dqn.main_model_predict.predict(np.expand_dims(obs,axis=0))\n",
    "    action = 0 if p_left[0][0]>0.5 else 1 \n",
    "    obs, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        if num_game %10 == 0:\n",
    "            print(\"Game played : %d. Reward for the last 10 episode: %s\" %(num_game,all_reward_sum[-10:]) )\n",
    "        all_reward_sum.append(reward_sum)\n",
    "        reward_sum = 0\n",
    "        num_game += 1\n",
    "        env.reset()\n",
    "print(\"Over %d episodes, mean reward: %d, std : %d\" %(num_games, np.mean(all_reward_sum), np.std(all_reward_sum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "cum_reward=0\n",
    "actions_list=[0,2,0,0]\n",
    "for step in actions_list:\n",
    "    state, reward , end = env.step(step)\n",
    "    frames.append(state)\n",
    "    cum_reward+=reward\n",
    "cum_reward\n",
    "HTML(plot_animation(frames).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dueling\n",
    "\n",
    "See the dueling architecture : \n",
    "\n",
    "\n",
    "In order to explain the reasoning behind the architecture changes that Dueling DQN makes, we need to first explain some a few additional reinforcement learning terms. The Q-values that we have been discussing so far correspond to how good it is to take a certain action given a certain state. This can be written as Q(s,a). This action given state can actually be decomposed into two more fundamental notions of value. The first is the value function V(s), which says simple how good it is to be in any given state. The second is the advantage function A(a), which tells how much better taking a certain action would be compared to the others. We can then think of Q as being the combination of V and A. More formally:\n",
    "$$Q(s,a) =V(s) + A(a)$$\n",
    "The goal of Dueling DQN is to have a network that separately computes the advantage and value functions, and combines them back into a single Q-function only at the final layer. It may seem somewhat pointless to do this at first glance. Why decompose a function that we will just put back together? The key to realizing the benefit is to appreciate that our reinforcement learning agent may not need to care about both value and advantage at any given time. For example: imagine sitting outside in a park watching the sunset. It is beautiful, and highly rewarding to be sitting there. No action needs to be taken, and it doesn’t really make sense to think of the value of sitting there as being conditioned on anything beyond the environmental state you are in. We can achieve more robust estimates of state value by decoupling it from the necessity of being attached to specific actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self):\n",
    "        # The input image of the game is 84 x 84 x 3 (RGB)\n",
    "        self.inputs = kl.Input(shape=[84, 84, 3], name=\"main_input\")\n",
    "\n",
    "        # There will be four layers of convolutions performed on the image input\n",
    "        # A convolution take a portion of an input and matrix multiplies\n",
    "        # a filter on the portion to get a new input (see below)\n",
    "        self.model = kl.Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=[8, 8],\n",
    "            strides=[4, 4],\n",
    "            activation=\"relu\",\n",
    "            padding=\"valid\",\n",
    "            name=\"conv1\")(self.inputs)\n",
    "        self.model = kl.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[4, 4],\n",
    "            strides=[2, 2],\n",
    "            activation=\"relu\",\n",
    "            padding=\"valid\",\n",
    "            name=\"conv2\")(self.model)\n",
    "        self.model = kl.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[1, 1],\n",
    "            activation=\"relu\",\n",
    "            padding=\"valid\",\n",
    "            name=\"conv3\")(self.model)\n",
    "        self.model = kl.Conv2D(\n",
    "            filters=512,\n",
    "            kernel_size=[7, 7],\n",
    "            strides=[1, 1],\n",
    "            activation=\"relu\",\n",
    "            padding=\"valid\",\n",
    "            name=\"conv4\")(self.model)\n",
    "\n",
    "        # We then separate the final convolution layer into an advantage and value\n",
    "        # stream. The value function is how well off you are in a given state.\n",
    "        # The advantage is the how much better off you are after making a particular\n",
    "        # move. Q is the value function of a state after a given action.\n",
    "        # Advantage(state, action) = Q(state, action) - Value(state)\n",
    "        self.stream_AC = kl.Lambda(lambda layer: layer[:, :, :, :256], name=\"advantage\")(self.model)\n",
    "        self.stream_VC = kl.Lambda(lambda layer: layer[:, :, :, 256:], name=\"value\")(self.model)\n",
    "\n",
    "        # We then flatten the advantage and value functions\n",
    "        self.stream_AC = kl.Flatten(name=\"advantage_flatten\")(self.stream_AC)\n",
    "        self.stream_VC = kl.Flatten(name=\"value_flatten\")(self.stream_VC)\n",
    "\n",
    "        # We define weights for our advantage and value layers. We will train these\n",
    "        # layers so the matmul will match the expected value and advantage from play\n",
    "        self.Advantage = kl.Dense(env.actions, name=\"advantage_final\")(self.stream_AC)\n",
    "        self.Value = kl.Dense(1, name=\"value_final\")(self.stream_VC)\n",
    "\n",
    "        # To get the Q output, we need to add the value to the advantage.\n",
    "        # The advantage to be evaluated will bebased on how good the action\n",
    "        # is based on the average advantage of that state\n",
    "        self.model = kl.Lambda(lambda val_adv: val_adv[0] + (val_adv[1] - K.mean(val_adv[1], axis=1, keepdims=True)),\n",
    "                            name=\"final_out\")([self.Value, self.Advantage])\n",
    "        self.model = km.Model(self.inputs, self.model)\n",
    "        self.model.compile(\"adam\", \"mse\")\n",
    "        self.model.optimizer.lr = 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 84, 84, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 20, 20, 32)   6176        main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 9, 9, 64)     32832       conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 7, 7, 64)     36928       conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4 (Conv2D)                  (None, 1, 1, 512)    1606144     conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "value (Lambda)                  (None, 1, 1, 256)    0           conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "advantage (Lambda)              (None, 1, 1, 256)    0           conv4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "value_flatten (Flatten)         (None, 256)          0           value[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "advantage_flatten (Flatten)     (None, 256)          0           advantage[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "value_final (Dense)             (None, 1)            257         value_flatten[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "advantage_final (Dense)         (None, 4)            1028        advantage_flatten[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "final_out (Lambda)              (None, 4)            0           value_final[0][0]                \n",
      "                                                                 advantage_final[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,683,365\n",
      "Trainable params: 1,683,365\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "main_qn = Qnetwork()\n",
    "main_qn.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double DQN\n",
    "\n",
    "\n",
    "This lead to often overestimates the Q-values of the potential actions to take in a given state. While this would be fine if all actions were always overestimates equally, there was reason to believe this wasn’t the case. You can easily imagine that if certain suboptimal actions regularly were given higher Q-values than optimal actions, the agent would have a hard time ever learning the ideal policy. In order to correct for this, the authors of DDQN paper propose a simple trick: instead of taking the max over Q-values when computing the target-Q value for our training step, we use our primary network to chose an action, and our target network to generate the target Q-value for that action. By decoupling the action choice from the target Q-value generation, we are able to substantially reduce the overestimation, and train faster and more reliably. Below is the new DDQN equation for updating the target value.\n",
    "\n",
    "\n",
    "$$target = R(s,a,s')+\\gamma Q_k(s',argmax_aQ(s',a;\\theta);\\theta_{target}) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64  # How many experiences to use for each training step\n",
    "        self.num_epochs = 20  # How many epochs to train\n",
    "        self.update_freq = 5  # How often you update the network\n",
    "        self.y = 0.99  # Discount factor\n",
    "        self.prob_random_start = 0.6  # Starting chance of random action\n",
    "        self.prob_random_end = 0.1  # Ending chance of random action\n",
    "        self.annealing_steps = 1000.  # Steps of training to reduce from start_e -> end_e\n",
    "        self.num_episodes = 10000  # How many episodes of game environment to train\n",
    "        self.pre_train_episodes = 100  # Number of episodes of random actions\n",
    "        self.max_num_step = 50  # Maximum allowed episode length\n",
    "        self. goal = 15\n",
    "\n",
    "        # Reset everything\n",
    "        K.clear_session()\n",
    "\n",
    "        # Setup our Q-networks\n",
    "        self.main_qn = Qnetwork()\n",
    "        self.target_qn = Qnetwork()\n",
    "\n",
    "        # Setup our experience replay\n",
    "        self.experience_replay = ExperienceReplay()\n",
    "\n",
    "    def update_target_graph(self):\n",
    "        updated_weights = np.array(self.main_qn.model.get_weights())\n",
    "        self.target_qn.model.set_weights(updated_weights)\n",
    "\n",
    "\n",
    "    def run_one_episode(self, num_episode, prob_random):\n",
    "        # Create an experience replay for the current episode\n",
    "        episode_buffer = ExperienceReplay()\n",
    "\n",
    "        # Get the game state from the environment\n",
    "        state = env.reset()\n",
    "\n",
    "        done = False  # Game is complete\n",
    "        sum_rewards = 0  # Running sum of rewards in episode\n",
    "        cur_step = 0  # Running sum of number of steps taken in episode\n",
    "\n",
    "        while cur_step < self.max_num_step and not done:\n",
    "            cur_step += 1\n",
    "            if np.random.rand() < prob_random or \\\n",
    "                    num_episode < self.pre_train_episodes:\n",
    "                # Act randomly based on prob_random or if we\n",
    "                # have not accumulated enough pre_train episodes\n",
    "                action = np.random.randint(env.actions)\n",
    "            else:\n",
    "                # Decide what action to take from the Q network\n",
    "                action = np.argmax(self.main_qn.model.predict(np.array([state])))\n",
    "\n",
    "            # Take the action and retrieve the next state, reward and done\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            # Setup the episode to be stored in the episode buffer\n",
    "            episode = np.array([[state], action, reward, [next_state], done])\n",
    "            episode = episode.reshape(1, -1)\n",
    "\n",
    "            # Store the experience in the episode buffer\n",
    "            episode_buffer.add(episode)\n",
    "\n",
    "            # Update the running rewards\n",
    "            sum_rewards += reward\n",
    "\n",
    "            # Update the state\n",
    "            state = next_state\n",
    "\n",
    "        return episode_buffer, sum_rewards, cur_step\n",
    "\n",
    "    def train_one_step(self):\n",
    "        # Train batch is [[state,action,reward,next_state,done],...]\n",
    "        train_batch = self.experience_replay.sample(self.batch_size)\n",
    "\n",
    "        # Separate the batch into its components\n",
    "        train_state, train_action, train_reward, \\\n",
    "        train_next_state, train_done = train_batch.T\n",
    "\n",
    "        # Convert the action array into an array of ints so they can be used for indexing\n",
    "        train_action = train_action.astype(np.int)\n",
    "\n",
    "\n",
    "        # Stack the train_state and train_next_state for learning\n",
    "        train_state = np.vstack(train_state)\n",
    "        train_next_state = np.vstack(train_next_state)\n",
    "\n",
    "        # Our predictions (actions to take) from the target Q network\n",
    "        target_q = self.target_qn.model.predict(train_state)\n",
    "\n",
    "        # The Q values from our target network from the next state\n",
    "        main_q_next_state = self.main_qn.model.predict(train_next_state)\n",
    "        train_next_state_action = np.argmax(main_q_next_state, axis=1)\n",
    "        train_next_state_action = train_next_state_action.astype(np.int)\n",
    "\n",
    "        # Tells us whether game over or not\n",
    "        # We will multiply our rewards by this value\n",
    "        # to ensure we don't train on the last move\n",
    "        train_gameover = train_done == 0\n",
    "\n",
    "        # Q value of the next state based on action\n",
    "        target_q_next_state = self.target_qn.model.predict(train_next_state)\n",
    "        train_next_state_values = target_q_next_state[range(self.batch_size), train_next_state_action]\n",
    "\n",
    "        # Reward from the action chosen in the train batch\n",
    "        actual_reward = train_reward + (self.y * train_next_state_values * train_gameover)\n",
    "        target_q[range(self.batch_size), train_action] = actual_reward\n",
    "\n",
    "        # Train the main model\n",
    "        loss = self.main_qn.model.train_on_batch(train_state, target_q)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # Make the networks equal\n",
    "        self.update_target_graph()\n",
    "\n",
    "        # We'll begin by acting complete randomly. As we gain experience and improve,\n",
    "        # we will begin reducing the probability of acting randomly, and instead\n",
    "        # take the actions that our Q network suggests\n",
    "        prob_random = self.prob_random_start\n",
    "        prob_random_drop = (self.prob_random_start - self.prob_random_end) / self.annealing_steps\n",
    "\n",
    "        num_steps = []  # Tracks number of steps per episode\n",
    "        rewards = []  # Tracks rewards per episode\n",
    "        print_every = 50  # How often to print status\n",
    "        losses = [0]  # Tracking training losses\n",
    "        num_episode = 0\n",
    "\n",
    "        while num_episode < self.num_episodes:\n",
    "\n",
    "            episode_buffer, sum_rewards, cur_step = self.run_one_episode(num_episode, prob_random)\n",
    "\n",
    "            if num_episode > self.pre_train_episodes:\n",
    "                # Training the network\n",
    "\n",
    "                if prob_random > self.prob_random_end:\n",
    "                    # Drop the probability of a random action\n",
    "                    prob_random -= prob_random_drop\n",
    "\n",
    "                if num_episode % self.update_freq == 0:\n",
    "                    for num_epoch in range(self.num_epochs):\n",
    "                        loss = self.train_one_step()\n",
    "                        losses.append(loss)\n",
    "\n",
    "                    # Update the target model with values from the main model\n",
    "                    self.update_target_graph()\n",
    "\n",
    "            # Increment the episode\n",
    "            num_episode += 1\n",
    "\n",
    "            self.experience_replay.add(episode_buffer.buffer)\n",
    "            num_steps.append(cur_step)\n",
    "            rewards.append(sum_rewards)\n",
    "\n",
    "            if num_episode % print_every == 0:\n",
    "                # Print progress\n",
    "                mean_loss = np.mean(losses[-(print_every * self.num_epochs):])\n",
    "\n",
    "                print(\"Num episode: {} Mean reward: {:0.4f} Prob random: {:0.4f}, Loss: {:0.04f}\".format(\n",
    "                    num_episode, np.mean(rewards[-print_every:]), prob_random, mean_loss))\n",
    "                if np.mean(rewards[-print_every:]) >= self.goal:\n",
    "                    print(\"Training complete!\")\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num episode: 50 Mean reward: 2.1200 Prob random: 0.6000, Loss: 0.0000\n",
      "Num episode: 100 Mean reward: 2.6200 Prob random: 0.6000, Loss: 0.0000\n",
      "Num episode: 150 Mean reward: 1.3600 Prob random: 0.5755, Loss: 24.2187\n",
      "Num episode: 200 Mean reward: 1.8000 Prob random: 0.5505, Loss: 11.8405\n",
      "Num episode: 250 Mean reward: 1.5800 Prob random: 0.5255, Loss: 7.8375\n",
      "Num episode: 300 Mean reward: 1.9800 Prob random: 0.5005, Loss: 5.8696\n",
      "Num episode: 350 Mean reward: 2.1200 Prob random: 0.4755, Loss: 4.6989\n",
      "Num episode: 400 Mean reward: 1.9600 Prob random: 0.4505, Loss: 0.2479\n",
      "Num episode: 450 Mean reward: 1.7800 Prob random: 0.4255, Loss: 0.1395\n",
      "Num episode: 500 Mean reward: 2.1800 Prob random: 0.4005, Loss: 0.1151\n",
      "Num episode: 550 Mean reward: 2.2400 Prob random: 0.3755, Loss: 0.1016\n",
      "Num episode: 600 Mean reward: 2.2000 Prob random: 0.3505, Loss: 0.0916\n",
      "Num episode: 650 Mean reward: 2.6200 Prob random: 0.3255, Loss: 0.0845\n",
      "Num episode: 700 Mean reward: 2.6200 Prob random: 0.3005, Loss: 0.0797\n",
      "Num episode: 750 Mean reward: 3.2200 Prob random: 0.2755, Loss: 0.0745\n",
      "Num episode: 800 Mean reward: 3.2600 Prob random: 0.2505, Loss: 0.0699\n",
      "Num episode: 850 Mean reward: 3.8200 Prob random: 0.2255, Loss: 0.0670\n",
      "Num episode: 900 Mean reward: 3.8400 Prob random: 0.2005, Loss: 0.0644\n",
      "Num episode: 950 Mean reward: 3.4000 Prob random: 0.1755, Loss: 0.0624\n",
      "Num episode: 1000 Mean reward: 3.5400 Prob random: 0.1505, Loss: 0.0622\n",
      "Num episode: 1050 Mean reward: 3.9600 Prob random: 0.1255, Loss: 0.0627\n",
      "Num episode: 1100 Mean reward: 3.4800 Prob random: 0.1005, Loss: 0.0634\n",
      "Num episode: 1150 Mean reward: 3.0200 Prob random: 0.0995, Loss: 0.0653\n",
      "Num episode: 1200 Mean reward: 3.9600 Prob random: 0.0995, Loss: 0.0645\n",
      "Num episode: 1250 Mean reward: 6.3200 Prob random: 0.0995, Loss: 0.0652\n",
      "Num episode: 1300 Mean reward: 5.0800 Prob random: 0.0995, Loss: 0.0641\n",
      "Num episode: 1350 Mean reward: 6.3200 Prob random: 0.0995, Loss: 0.0642\n",
      "Num episode: 1400 Mean reward: 6.2200 Prob random: 0.0995, Loss: 0.0635\n",
      "Num episode: 1450 Mean reward: 6.0200 Prob random: 0.0995, Loss: 0.0651\n",
      "Num episode: 1500 Mean reward: 8.0400 Prob random: 0.0995, Loss: 0.0691\n",
      "Num episode: 1550 Mean reward: 7.2000 Prob random: 0.0995, Loss: 0.0743\n",
      "Num episode: 1600 Mean reward: 8.0000 Prob random: 0.0995, Loss: 0.0746\n",
      "Num episode: 1650 Mean reward: 7.1600 Prob random: 0.0995, Loss: 0.0746\n",
      "Num episode: 1700 Mean reward: 9.1800 Prob random: 0.0995, Loss: 0.0756\n",
      "Num episode: 1750 Mean reward: 7.1200 Prob random: 0.0995, Loss: 0.0739\n",
      "Num episode: 1800 Mean reward: 9.3600 Prob random: 0.0995, Loss: 0.0734\n",
      "Num episode: 1850 Mean reward: 10.6800 Prob random: 0.0995, Loss: 0.0748\n",
      "Num episode: 1900 Mean reward: 10.4400 Prob random: 0.0995, Loss: 0.0802\n",
      "Num episode: 1950 Mean reward: 11.8000 Prob random: 0.0995, Loss: 0.0854\n",
      "Num episode: 2000 Mean reward: 9.2800 Prob random: 0.0995, Loss: 0.0889\n",
      "Num episode: 2050 Mean reward: 10.7600 Prob random: 0.0995, Loss: 0.0928\n",
      "Num episode: 2100 Mean reward: 12.1600 Prob random: 0.0995, Loss: 0.0975\n",
      "Num episode: 2150 Mean reward: 12.0400 Prob random: 0.0995, Loss: 0.1039\n",
      "Num episode: 2200 Mean reward: 14.7800 Prob random: 0.0995, Loss: 0.1119\n",
      "Num episode: 2250 Mean reward: 15.5800 Prob random: 0.0995, Loss: 0.1113\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "dqn = DQN()\n",
    "dqn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Real Pacman!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the MsPacman environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"MsPacman-v0\")\n",
    "obs = env.reset()\n",
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the images is optional but greatly speeds up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 80, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mspacman_color = 210 + 164 + 74\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "    img = obs[1:176:2, ::2] # crop and downsize\n",
    "    img = img.sum(axis=2) # to greyscale\n",
    "    img[img==mspacman_color] = 0 # Improve contrast\n",
    "    img = (img // 3 - 128).astype(np.int8) # normalize from -128 to 127\n",
    "    return img.reshape(88, 80, 1)/128\n",
    "\n",
    "img = preprocess_observation(obs)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the `preprocess_observation()` function is slightly different from the one in the book: instead of representing pixels as 64-bit floats from -1.0 to 1.0, it represents them as signed bytes (from -128 to 127). The benefit is that the replay memory will take up roughly 8 times less RAM (about 6.5 GB instead of 52 GB). The reduced precision has no visible impact on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApEAAAGgCAYAAADy5TxeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xm8JFV99/HvV0VQZ4YBB1QWIYCIiBsJAY3ro7jiCiIogyQa82BcRk2iQRNwwxUyRJS4AxcBBQQVgYiPorigRAQVFWUVZpB9ZlgGA/h7/jinmZqe7r73NN19q+p+3q8XL+5U1alz6nTde3/3d06dckQIAAAAKHG/2W4AAAAAmocgEgAAAMUIIgEAAFCMIBIAAADFCCIBAABQjCASAAAAxQgiR8D2QbY/N+pjZ3CusL3dEOUOsX3cKNowSbbPtP3aMZ37ubZPG8e5kdj+qe3HznY7AACjQRDZxfYBtn9p+w7bf7R9lO2Fg8pExKER8fqZnL/k2LmsV6AbES+IiGPGVOWhkj5cqf/9+T642/YhPdq3ie3jba+wfYvtL1X2rW/7C7ZX5Xvo7aWNsb2b7bNt32z7Btsn2X5EZf+zbH/X9krbV/Yov3Xef4ft39p+zoC6jrb9v7Zvy/WdbXuHrmMeYfuztpfn4y7P5Xao1Bd53222r7P9KdvrVU7zcUnvK+0LAEA9EURW2H6HpI9I+mdJG0raTdJWks62/cA+ZR4wuRY2R5P6xfYukjaMiPMqmy+V9C+Svtmn2Fcl/VHp/thUKUDqOETSo/K+Z0n6F9vPr9S3he1Nutpg20+qbNpI0mckbZ3Pc6ukL1b23y7pC0r3ai8nSPq5pIdKerekk7vr7PLRiJgnaXNJyyR9vtK2h0r6kaQHS3qapPmSdpb0PUm7d51nYT7P4yQ9WdI/VvZ9XdKzqsEwAKC5CCIz2wskvVfSmyPirIi4KyKulLS30i/x/fJxh9g+2fZxtldJOqA7a2Z7f9tX2b7J9r/ZvrKTCaoeW8nevNb2H2zfaPvdlfP8te0f52zXtbaP7BfM9riezWx/PWeWLrX9912HbGD7y7ZvtX2B7SdUyr7T9rK87xLbz87b72f7XbYvy9f2Fdsbd13L62z/QdJ3bJ9l+01d7brI9ivy10fYvjpn7H5m+2l5+/MlHSTpVTmrdVHefo7t11fa8p7cz9fbPtb2hjPp1x5eoBQQ3SsijomIM5WCt+6+fa6kLSX9c0SszPfKzyuH7C/p/RFxS0T8RtJnJR1Q2f8qpT9MNqpsWyrpiEr9Z0bESRGxKiLukHSkpL+p7P9pRExJurxH+7ZXCvIOjojVEXGKpF9K2nNAH3TOu1rSVyQ9sbL5bZJWSVocEZdFsiIivhgRn+hznuslnS1px8q2OyX9TNJzp2sHAKD+CCLXeIqkDZQyTPeKiNsknam1My4vlXSypIWSvlQ93vaOkj4l6TWSHqGU0dx8mrqfKunRkp4t6d9tPyZvv0fpF/gipazOsyW9cYbXc4KkayRtJmkvSYd2gsHKNZwkaWNJx0s6zfZ6th8t6U2SdomI+ZKeJ+nKXOYtkl4m6Rn5vLdI+mRXvc+Q9Jhc7nhJ+3Z25L7ZSmuye+crBSudNpxke4OIOEtpePnLETEvIp6gdR2Q/3uWpG0kzVMKtKr69Wu3x0m6pM++XnbLxx+Tg+nzbT8jX+NGSn1zUeX4iyTdOxcwIg6TdK6ks2zPt/1hpX576YA6ny7p4hm277GSLo+IagC8Vhv6sf0Qpc/s0srm50g6NSL+PMP6ZXszpXvgvK5dv5HU6/MEADQMQeQaiyTdGBF399h3bd7f8eOIOC0i/pwzN1V7SfpGRPwgIv5X0r9Lmu4F5e/NGaOLlH7ZP0GSIuJnEXFeRNyds6KfVgo2BrK9pVIA9c6IuDMiLpT0OUmLK4f9LCJOjoi7JB2uFEDvphS4ri9pR9vrRcSVEXFZLvMPkt4dEddExJ+Uhm336hq6PiQibs/9cqqkJ9reKu97jaSv5rKKiOMi4qZ8fYfleh893fVVznV4RFyeA/1/lbRPV1t69msPC9Uj4zjAFkrZtO9KerikwyR9zfYipWBWklZWjl+pNARc9RZJv1YKDF8qafeIuKVXZbYfr3Qf9Ru67javq/5+baj6J9srlPrhqVr7XlmkNHTfac9Lcnb8Vtvf6jrPjfk8y5SG3E/u2n+rUn8DABqOIHKNGyUt6jOX7xF5f8fVA86zWXV/Hoq8aZq6/1j5+g7lQMT29rZPd3o4Y5VSdm5RrxP0aMPNXZmoq7R2RrTaxj8rZy0j4lJJS5QCxOttn5izSlLKIp6aA4gVSlmleyQ9rM95b1XKOu6TN+2jSubW9jts/8bp4ZAVSlnbmVxf5xqv6rq+B3S1pWe/9nCLBgdY3VZLujIiPp+Hsk9Uuu6/kXRbPmZB5fgF6gpSIyKU+m8Tpb5f1asip6fvz5T01og4d4btu62r/p5t6PLxiFioNAdztdYO5m9S+h7otP3r+di3SeqeXrEo73uwpB9KOqtr/3xJK2Z2GQCAOiOIXOPHkv4k6RXVjXl47wWS/l9l86DM4rVKmapO+QcpPdwwjKMk/VbSoyJigdI8Qc+g3HJJG9uuBkaPVMoOdWxZaeP9cpuXS1JEHB8RT1UKGkPpYSMpBUoviIiFlf82iIjqebv75gRJ+9p+sqQHKWXvlOc/vlNpzulGOfBYWbm+6bK3y3P7qtd3t6TrpinXyy8kbV94fM/25WzitVo76/kEdQ1F236jpAOV5gyuUBrKX6/rmK0kfVtpfuVUQfsulrRN1+e/Thv6tP8Pkt4q6Yh870rp3n9Zvk9mJGeij5b05Jyh7XiM1h7qBwA0FEFkFhErlR6s+YTt5+f5gVsrzRu8RtJMf4mfLOnFtp+SH4J5r2YW+PUyXylDdZvTUioHzqRQRFyt9DTth2xvkIdDX6e152/+pe1X5MzrEqUA+jzbj7b9f2yvL+lOpazUPbnMf0n6YGd42mmZm0Hz+CTpDKVg731Kcxw78+rmKwV9N0h6gO1/19rZs+skbT0gcDlB0tts/4XteVozh7LXdITpnKGuaQL5899A6XvkAbkf7593nyppo/zgzv1t76WU5f1h3n+spPfY3ih/bn+vFFB1zr1Yafj9ORFxhaRXK90j1Szt5pK+I+mTEfFf3Q3ODxZtIGm99E9vkO83RcTvJF0o6eC8/eWSHi/plJl0RkScrRSkvyFvOlzpafEp29s6ma+1H77pbt/6SkPif1TOxOdtf6n0wA0AoOEIIisi4qNK2b6PKwVvP1HKvj27M49vBue4WNKbJZ2olJG6VdL1SkFaqX9SCjBuVXrC98sFZfdVGppcrhT0HJyDg46vKT0lfIvSL/tX5PmR6yutl3ijUgCwqVKfSOnp4a9L+pbtW5Uemth1UCNyv31V6eGM4yu7/ltpmPZ3SkPRd2rtaQIn5f/fZPuCHqf+glJg/31JV+Tybx7UlgFtvEDSStvVa/msUgC9r9ISOauV5wlGxM2SXqL0+ayU9C5JL42IzpSHgyVdlq/re5I+lh8W6rhYaQ7kZfl8dynNpa1+vq9XemDoYK9Ze/G2yv6n5zadoZSFXS2pOj9xH0l/pfT5fljSXhFxQ0G3fExpaaL183XtptTHP1C6Hy9U+kOg+w+bFbmd1yk9DPaSPHQvpT47JyKWF7QDAFBTXvPzHeOQs2QrlIakr5jt9qC3vGzPGyPiZbPdlray/RNJr4uIX812WwAA9x1B5BjYfrHSPDIrPbm7q6Sdg84GAAAtwXD2eLxUaRh5udKbS/YhgAQAAG1CJhIAAADFyEQCAACgGEEkAAAAivV6O8uss80YO4CRi4hh12wFAHSpZRB5zVvfOttNAAAAwAC1DCIH2eKUR0x/UEtds+e1fffN5X6Zq7gfehvULwCA0WFOJAAAAIoRRAIAAKAYQSQAAACKEUQCAACgGEEkAAAAihFEAgAAoBhBJAAAAIoRRAIAAKBY4xYbH2TYxZebUm5YTbm+UfdLU9rP/TDacgCAySATCQAAgGIEkQAAAChGEAkAQGb7mbavGbLsObZfP+o2jZvt22xvM6Zzf8j2knGcG4PZ3tp22J526qLtl9g+sbiOiBiudWO0bMmSvo2ay3OhmCOGKu6H3gb1y+ZLl3qCTZl1tq+U9DBJ90i6XdIZkt4cEbfNZrvqzPYzJR0XEVsMUfacXPZzo27XqEyyjbY3kXShpO0iYnXetrek90raQtLVkg6KiNPyPkt6v6S/lTRP0s8l/WNEXFxY78aSjpL07LzpvyUdGBGr8v6tJX1R0q6S/iDpTRHx7aEvtKbydV4hab2IuHsGx/9K0qsj4hczrYNMJAC024sjYp6knSXtIuk93Qc4Gdnvg1GfD2vMJKtUIwdIOqMSQG4u6ThJb5e0QNI/Szre9qb5+FdK+jtJT5O0saQfS5qqntD2zt2V2N7B9oMrmz4gaSNJ20jaVukPqUMq+09QClAfKundkk7OAe990rDPppcTJL2hpADf5AAwB0TEMklnStpJunfo9YO2fyjpDknb2N7Q9udtX2t7me0P2L5/Pv4A2z+0/QnbK23/1nYn09PvfJvZ/rrtm21favvvK8ff3/ZBti+zfavtn9neMu/bwfbZudwlOXvVKfdC27/OZZbZ/qe8fZHt022vyOXO7QSyuR2n2L7B9hW231I534NsH237Ftu/Vgq0+7L9FNvn5z443/ZTug7Z1vZP8/6v5ayYbG9g+zjbN+U2nm/7YXnfTPr9P2zfLOn9ufxOlTZtYnu17U1tb5T74YZ8Tafb3iIf90GlAO1IpyHsI/P2sL1dpS3H5vJX2X5PpR8PsP0D2x/P577C9gsGdNcLJH2v8u8tJK2IiDMj+aZShnzbvP8vJP0gIi6PiHuUAs4dK9e5QNLXbL+usm0HSd+V9DeVev5C0mkRsSoiVko6VdJj8/HbK/1BdXBErI6IUyT9UtKevS7A9kNtf8P2qvyZfcD2Dyr7w/Y/2v69pN932tTr/rW9i+3rXAk2be9p+8L89V/b/p9c13W2D68c91TbP8qf/dW2D8jbX2T757nM1bYP6fdhDLrPsnMkvahf+V4IIgFgDsgB2guVMjAdi5UyD/MlXSXpGEl3S9pO0pMkPVdSdY7frpIul7RI0sGSvtoJkvqc7wRJ10jaTNJekg71msDz7ZL2zW1aoJSBusP2QySdLel4SZvmYz5l+7G53Ocl/UNEzFcKiL+Tt78j17WJUubpIEmRA6BvSLpI0uZKQ5xLbD8vlztYKYjZVtLzJL12QB9uLOmbkv5TKYt1uKRv2n5o5bD987VslvvyP/P210raUNKWuez/lbQ675tpv28q6X2Svpr7pWNvSd+LiOuVfq9/UdJWkh6Z6zhSkiLi3ZLOVRq+nRcRb+pxmZ/I7dxG0jPy9fxtV1suUboHPirp87b7TRN5XD62438k/cZp/t39bb9M0p8kdYZPT5S0ne3tba+X++ysTuE8HP1cSR+0/Wrb20r6tqT3RMTZlXo+KWmPHFBvpBQgnpn3PVbS5RFxa+X4i/L2Xj6pFOg+PLen1/3xstwvOw66fyPifEk3Sdq9UnY/rcm2HiHpiIhYoHQ/fkWSbD8yt/8TSvf3E5WmCSi3bX9JC5UCwANzv/Yy3X32G0lb52B9RggiAaDdTrO9QtIPlLJCh1b2HR0RF+f5UhsrZY6WRMTtOSD5D0n7VI6/XtLSiLgrIr6sFCC8qM/5Hi7pqZLeGRF3RsSFkj6nFGhK6ZfXeyLikpyVuigibpK0h6QrI+KLEXF3RFwg6RSlIFSS7lL6Zb0gIm7J+zvbHyFpq9y+cyNN+t9F0iYR8b6I+N+IuFzSZyvXtbekD0bEzRFxtdYEfb28SNLvI2Iqt+0ESb+V9OLKMVMR8auIuF3Sv0naO2d77lIKHreLiHsi4mcRsSpnI6fr9+UR8Ylc52qlAKUaRL46b1NE3BQRp0TEHTlQ+qBSMDit3M5XSfrXiLg1Iq6UdJjWfGaSdFVEfDZnCo9R6vOH9TnlQkn3Bmu5zLG5rX/K//+H3FeSdK1SkHuJUvD7Sklvq54wIn6j1F9H5GM/EhGf76r3AkkPVArYblKaE/ypvG+epJVdx69U+sOnV3/sqZS1vCMifp2vuduH8v2zWtPfv8coBY6dP0qel/tBSvfIdrYXRcRtEXFe3v4aSd+OiBPyvX1T/n5SRJwTEb+MiD/nuYwnqMfnPcP7rPNZLexxjT0RRAJAu70sIhZGxFYR8cbO/LTs6srXW0laT9K1echshaRPK2VTOpbF2k9jXqWUcet1vs0k3dyV8blKKRsopYzcZT3au5WkXTttyO14jVJQKqVf6i+UdJXt79l+ct7+MUmXSvqW7cttv6tyvs26zneQ1gQ+m3W1+6oebapeU/f+6jWpx7nWU8raTSk94HGi7eW2P5qzbTPp9+o5pZR9fZDtXW1vpZSZOlWSbD/Y9qfzUPQqSd+XtLBr2LKfRUrBV/Uau6/vj50vIuKO/OW8Pue7RZXgzPZzlLKXz8z1PEPS52w/MR9ysFLQv6WkDZQewPmO157vKKVgc2Wu93c96j0pb5+vlOW+TGloXJJuy9uqFqgS7FZsovRSlmr/d38W3dumu3+Pk/Ri2/OU/oA5NyI6TwO+TtL2kn6bh873yNv7fa8o3wPfzdMPVipluBf1OHQm91nns1rRq65eCCIBYO6qBoRXK2WHFuWgc2FELIiI6jDf5l1Dl4+UtLzP+ZZL2tj2/K7jl1Xq21brulppaHZh5b95EXGgJEXE+RHxUqVffqcpD/nlzNk7ImIbpczg2/PQ+dWSrug63/yIeGGu71qlX9LVNvazXOmXcVX1mtTjXHdJujFnkN4bETtKeopSxmp/zazf11qxJCL+nK97X6Us5OmVYP0dkh4tadc8LPr0vN29ztXlxtze6jV2X1+JXygFRR1PlPT9iPifnDk7X9JPJD0n73+CpC9HxDU5i3e00gMy1XmRi5SGsI+V9HxJU05P1Fc9QdKnc8btNkn/pfSHhyRdrDRfd37X8b2eAL9Bafi3+qT+lj2O6/4+GnT/LlN6YOjlShneex8ciojfR8S+Svf2R5Qe+HmI+n+vSCmL+XVJW0bEhvlae00vmMl99hilLOqqPnWtgyASAKCcDfmWpMNsL7B9P9vb2q4OjW0q6S2217P9SqVfOmf0Od/Vkn4k6UNOD5U8XinT8qV8yOeUHhJ5lJPH57mFp0va3vbiXM96+YGEx9h+oO3X2N4wIu6StEppqFK297C9XQ5yO9vvkfRTSatsv9PpIZr7297JducBmq9I+tc8f24LSW8e0E1n5La92vYDbL9KKcA5vXLMfrZ3zNmz90k6OSLusf0s24/LGcFVSsHaPTPs916OVxp6fo3WDIdKKZu0WtKKPFx6cFe565TmO64jDzd/RWnO4fyc5Xy71mTxSp2htYdWz5f0tE7m0faTlB70+UVl/yttPyz3w2Kl7Nml+fh5Sn11ep6e8COl4diTbP91Vz2vz5/3g5Tm6V6Ur/F3SvMJD8735cslPV5pyLlXf3xV0iE5w7uDUuA/SN/7t3LMsZL+RWnO6Kmdjbb3s71J/iOhkw28R+l75jm298733UMr2dv5Shn/O3MfvLpXo2Z4nz1Da+aOzkjTH0cfiUHryg2rDev0jaNf5iruh97a0C8ts7+kD0v6tdIvp8uVMiIdP5H0KKWM1XWS9oo0j7GffZUyI8uVhjYPjjUPQBwuaX2lX2yLlOYWvjwibrL93Lz/cKVkx0VKwYyUsjdH5mDsEuX5ZbldRyoNQd4i6VMRcY4k2X6x0ty+K3Kdl2jNUkfvzW28Irfzi5Le2utictv2UJqPd5RScLNHRNxYOWxK0tGSdlCag3pg3v7wXM8WSkOqX9aa4Gy6fu/Vlp/Yvl1piL36i3+pUlB5Y76ew5Qe/Og4QtIxtg9Umr/5Fq3tzUoPcFwu6U6l+aNfGNSWAY6VdKHtB0V6Evp7Tk8Pn+w0R+8GSYdGxLfy8R9R+kPlQkkPUerfPSOiE1DdLuljkeaidvrhOzmY/0Ol3r9Tmtt6jVJW7qdKyw117KP0Gd2Sy+0VETf0uYY35WP/qHTfnCDpr/pdcETcOs39K6XA8ShJp1bmg0ops3p4/gPkKkn7RMSdkv5g+4WSPq70x9dKpfv3QklvVAoMj1S6376i/nMap7vP9tWa76cZYbFxNeeX46QXlyaIHB3uh94m3c65ttj4KDktKfL6iHjqbLcFzWH7UEnXR8TS2W7LKNj+iKSHR0Tfp/hneJ7LlB4qqsUi5/kPrcURsfe0B1e0KhPJGzwAAKiPiDhotttwX+Qh7AcqrSW5i9KUjPv0akvbeyrNo/zOdMdOSkR8Q2kprCKtCiIBAABGaL7SEPZmSktcHSbpa8OezOm1kzsqZf3+PIoGziaCSADAtPKTskfPcjOAicpPkG83wvM9c1TnqgOezgYAAEAxMpEA0B71e1ISQFus82AimUgAAAAUI4gEAABAMYazZ0Gd1vdj6aPZx/2ASVi8ePFsN2HWTE1N9d03l/tlLuOe6G1Qv/RCJhIAAADFCCIBAABQjCASAAAAxZgTOQsmPe+MeW71xv0AAGiiVgWR/HIEAACYDIazAQAAUKxVmUgAQLlhlztpSrlhNeX6xtEvTbkG7onRlitFJhIAAADFCCIBAABQjCASAAAAxRwRs92GdSxbsqRvoyb9GrhhteF1dePol7mK+6G3Sbdz86VLPfIK66Xvz05e5dbbXO6XuYx7ordpXnu4zs9PHqxRc5YGYj1BVHE/AABmE8PZAAAAKEYQCQAAgGIEkQAAAChGEAkAAIBiBJEAAAAoRhAJAACAYgSRAAAAKEYQCQAAgGKtWmx82Dd4UK7Z5erSDsrVoxzKDfv2jmnebjGUNrwtZBz9MpdxT/RWh34hEwkAAIBiBJEAAAAoRhAJAACAYq2aEznsPCnKNbtcXdpBuXqUAwBMBplIAAAAFCOIBAAAQLFWDWcDANpr2KWImlIfynFPzC4ykQAAAChGEAkAAIBiBJEAAAAoxpxIAEAjTHrO2Vyc49Y03BOzq3FB5KD36Q4yjjXnJv1u32GvfZCmtLMumtJf3O8AgHFjOBsAAADFGpeJBACMFkN0AIZBJhIAAADFCCIBAABQjCASAAAAxQgiAQAAUIwgEgAAAMUIIgEAAFCMJX5Ur0WUx1HfsJrSzrpoSn9xv2NUmrI0EG81QTfuidEgEwkAAIBiBJEAAAAoRhAJAACAYsyJ1OTnZTVlHlhT2lkXTekv7ncAwCiQiQQAAEAxgkgAAAAUYzgbAOa4qampvvsGLU1CuWaXG6Qp10C50ZYrRSYSAAAAxQgiAQAAUIwgEgAAAMUcEbPdhnUsW7Jk5I1qw+vcBtU3rKa0sy6a0l/c771tvnSpR37Seun7s7Otr10DMDqD5lJKWufnZy0frGFdud6a0i9NaWdd0F+9jaNfYunITwkAcxbD2QAAAChWy0wkMMh5O53Td99uv3rmxNoBAMBcRiYSAAAAxchEojEGZSC7jyEjCQDAeJGJBAAAQDEykWiE83Y6Z53sYr9t/fYBAIDRIROJxjhvp3PWGdLutQ0AAIwfQSQAAACKtWo4e9g3atTpTRzjaGcbyl1zSe+HZQYNWdep/W0oN6ymtLPtpnkTRV/jeNPNoLZMur5hNaWdddKUPuOenzkykQAAACjWqkwk2q3X3EfmQwIAMDvIRAIAAKBYqzKRw86TmvT8qkm3sw3lztvpklq0Yy6XG1ZT2gkAKEMmEgAAAMUIIgEAAFCsVcPZAIDJqdOyJeOob1hNaWedNKXPuOfXRiYSAAAAxQgi0QjVRcV3+9Uz7/139ete/wYAAONBEAkAAIBizIlEY3RnGLszkIOOBTB6k56TVYc5YDPRlHbWSVP6jHt+bbUMIge9M3dYk15zbhzXMA6Tfldy09Ffk0O/AEC9MZwNAACAYgSRAAAAKEYQCQAAgGIEkQAAAChGEAkAAIBiBJEAAAAoRhAJAACAYgSRAAAAKFbLxcYnbdCixpNepLxOxtEvTejPYRe55j7qjX6ZnLq/3WK2NKVfxt3ON7zhDX33feYznxlr3ePSlM920sbRL1NTU+tsIxMJAACAYgSRAAAAKEYQCQAAgGLMiRTzsvqhX8rQX73RL8Dsqc6DHDTvsXu+ZFPnSGKyyEQCAACgGEEkAAAAijGcDQBzXK+lOzoGLRUybLlhTbqdbSh37rnn9t03SJ2uoQ3lhlX3dpKJBAAAQDGCSAAAABQjiAQAAEAxR8Rst2Edy5YsGXmjxrHMyLCvx6uTSfdLE5Z7mXT7uY96G0e/bL50qUd+0hpZvHhx/X6g9zDpeWVz2aBXHQ7CEj+j1YZ7fmpqap2fn7V8sGbYX0iT/mXchIAI9cd9VG7YPoulI24IAMxhDGcDAACgGEEkAAAAihFEAgAAoBhBJAAAAIoRRAIAAKBYLZ/OBgAA9x1L9WCcyEQCAACgGEEkAAAAijGcreHfUEI5VDXl82lKOZQb9FaMYU36bRrjuIZxGEe/NOXah0WfTc6k+oVMJAAAAIoRRAIAAKAYQSQAAACKMSdSw8/LotzsOH3/E/vu2+PYfSbYkrU15fNpSjkAQL0RRKIxBgWP3cfMZjAJAMBcwHA2AAAAipGJRCOcvv+J62QX+23rtw/AaA1aRmTSSwPVyTj6pSn9OezSMtxLvdW9X8hEAgAAoBhBJBrj9P1PXGdeZK9tAABg/AgiAQAAUIw5kWiMXnMcmfcIzJ46zMmqI/qlHH3WW937pZZB5KB37dZJU9o5SJPW8Os1bN2GoWzuo3Jt6DMAaDqGswEAAFCMIBIAAADFCCIBAABQjCASAAAAxQgiAQAAUIwgEgAAAMUIItEI1fUg9zh2n3v/Xf26178BAMB4EEQCAACgWC0XG5+0QQsXN2kx7lGrW790Zxi7M5CDjp2EuvVXXdAvkzPs2y2mpqZG3JLB6v4WDjQH91K5Uf6cIBMJAACAYgSRAAAAKEYQCQAAgGLMiRTzsvoZR78Mmh/XdNxHvdEvANBOZCIBAABQjCASAAAAxRjOBgAMZdDSQIOWEaEcujXlM2pKuUkhEwnAqEwzAAAK20lEQVQAAIBiBJEAAAAoRhAJAACAYsyJBAAMZdg5WZSbHQceeGDffUcdddQEW7KupnxGTSk3KbUMItuwrlwbrmFYc/nah9GU/pr0Gp/j6JdYOvJTAsCcxXA2AAAAitUyEwkM8osPf3+dbY9/19PX2tf5NwDMZdUh7EFD1t1D3bM9vI1mIBMJAACAYmQi0Ri9MpDd+6oZSbKRAACMD5lIAAAAFCMTiUbrngvJnEgAACaDIBKN0R0Y/uLD3x84xA0AAMaH4WwAAAAUa1UmctBiyIMWLh623LAm3c62lqtmJntlJOve/qaVG1ZT2tkGU1NTs92EGWlKOwep+5tEehn0xpqm4l4qN8o+IxMJAACAYgSRAAAAKNaq4exhh7gmPTQ26Xa2tdx0D9XUvf1NKzesprQTAFCGTCQAAACKtSoTiXZjOR8AAOqDTCQAAACKkYlEq/CmGmByBi0V0sQlcEalTv1y1FFHTbS+YdWpz+qk7v1CJhIAAADFyESiMbrfk91rHwAAmAyCSDQOASNQD3UYTqujcfRLG97MMgj3Um917xeGswEAAFCslpnIQe/MHVYbFi5uyjuPx1HfqLW9v9p+vwMAZh+ZSAAAABQjiAQAAEAxgkgAAAAUI4gEAABAMYJIAAAAFCOIBAAAQDGCSAAAABQjiAQAAECxWi42PmltXyS67fWNWtv7q+31zWV1f0XaTLThGoY17ms/7rjjZnzsfvvtN8aWjE5T7pdJv7ZyUq/eJBMJAACAYgSRAAAAKMZwtiY/pEZ99db2/mp7fQBmpjpkXTLUDXSQiQQAAEAxgkgAAAAUI4gEAABAMeZEAsAcN2j5kUFLhQxbbliTbmfbypUs21PXa2hquWHVvZ1kIgEAAFCMIBIAAADFGM4GgDlu2OGtSb8tZNLtbHu5Qcv6NOUamlJuWHVvJ5lIAAAAFGtcJrJOCxfXqS1tQH/WW1MWKR/0rm4AwOiQiQQAAECxxmUiAQBAuUFzIEuW/wE6yEQCAACgGEEkAAAAijGcDQDAHMCQNUaNTCQAAACKEUQCAACgGEEkAAAAirVqTuSgRYYHLVzclHLDakp9o25nXdoxnabU15Ryc9nU1NTIzznp17yNw6B+Gcf11am+cWh7n7X9nh8lMpEAAAAoRhAJAACAYq0azh52iKsp5YbVlPpG3c66tKMt9TWlHABgMshEAgAAoBhBJAAAAIoRRAIAAKBYq+ZEAgAmp+3LsrS9vnFoe5+1vb5SZCIBAABQjCASAAAAxRjOBgAMZdLDadRXf23vs7bXV4pMJAAAAIqRidTgd/QOqw0LJU/6GsbxOfQzjmtrw2c+CN8nAIAqMpEAAAAoRhAJAACAYgSRAAAAKEYQCQAAgGIEkQAAAChGEAkAAIBiBJEAAAAoRhAJAACAYq1abHzQYsh1WtR40u0ctr5Jl6uLtvdXUz6fprSzKer0+rQ6taUN6M/6a8rrEqempoqOJxMJAACAYgSRAAAAKNaq4eymDHFNup3D1jfpcnXR9v5qyufTlHYCwFxFJhIAAADFCCIBAABQjCASAAAAxVo1JxIAUG7Qsh6DlgppSrlhNaW+cbSzTm0ZpCn1NaVcKTKRAAAAKEYQCQAAgGIMZwPAHDfs8FZTyg2rKfWNo511aksb6mtKuVJkIgEAAFCMTGRLDHrP8DiwEPS6Jv0ZAAAwm8hEAgAAoFirMpGDMkFkzgAAAEaHTCQAAACKEUQCAACgGEEkAAAAihFEAgAAoBhBJAAAAIoRRAIAAKBYq5b4aYq2L0XU9Otrevun0/brw+RMTU2N/JyTfo3dOEz6GsbxOQxSp9csNkVbv1fIRAIAAKAYQSQAAACKMZw9C9o+ZNj062t6+6fT9usDAEwGmUgAAAAUa1UmkgwLAADAZJCJBAAAQLFWZSIBAOUGLT9Sh2VEOibdzmHrm3S5Oml7nzXlM5pUO8lEAgAAoBhBJAAAAIoxnA0Ac1ydhuEGmXQ7h61v0uXqpO191pTPaFLtJBMJAACAYmQi1ZylgZrSzmE1/fqa3v7ptP36AABlyEQCAACgGEEkAAAAihFEAgAAoBhBJAAAAIoRRAIAAKAYQSQAAACKEUQCAACgGEEkAAAAirVqsfFr9ry2775BCyVTrtnl6tIOytWjHMpNTU313deU17wNa9C1j0Pb+3NYk/4cMBpkIgEAAFCMIBIAAADFWjWcPewQF+WaXa4u7aBcPcoBACaDTCQAAACKEUQCAACgGEEkAAAAirVqTiQAoL3avhRRG66vDdcwSNuvrxSZSAAAABQjiAQAAEAxhrMBAI3Q9uHCNlxfG65hkLZfXykykQAAACjWuEzkoPfpAgAAYDLIRAIAAKBY4zKRAIDRYp4XgGGQiQQAAECxWmYitzjiiNluAoAWiqVLZ7sJANAatQwix+nss3fR7ruff+/X3Tr7mlofMIyzdt55rX8//4ILZqklAICmYDgbAAAAxeZMJrKTBdx99/N7ZgR7Hdek+oBhnbXzzutkHquZSbKSAIBeyEQCAACgmCNittuwDttja1Q1KziJuYqTrg8YhU4m8vkXXLDW100XEZ7tNoxZ35+dLOMDYDpTU1ODdq/z85NMJAAAAIrNmTmRvXRnBMedDZx0fcAodDKQbcpIAgDuuzkdRE46iCNoRN31ChS7l/8BAEBiOBsAAABDmNOZSABJv6HqXllIhrUBABKZSAAAAAxhTmciebAGWBtZxrlp0LIeg5YGolyzyw3SlGug3GjLlSITCQAAgGJzOhPZrdfC4G2qD+hlunmP3fvJUgIApDn4xhqp9xtjuo0yqJt0fUCJQUv4tC1g5I01ANAfb6wBAADA2M3J4exJv8Oad2ajzhiyBgAMg0wkAAAAis3JTGQHrz0EEjKPAIBSZCIBAABQjCASAAAAxQgiAQAAUIwgEgAAAMUIIgEAAFCMIBIAAADF5uRrD4H76tylT1tn29OWnDsLLUGJufzaQwC4j3jtIQAAAO47MpFAge4MZCf7WN1ORrK+yEQCwNDW+flJEAnM0HSBYr8AE/XR9iBy0j87V61ade/XCxYsaF19QImjjz763q8POOCAWWvHuPT6+clwNgAAAIoRRAIAAKAYQSQAAACKPWC2GwAAaIbqnMTp9o1izuKk6wPuizbOg5wOmUgAAAAU4+lsoABL/DQbT2ffN4Myg93GnYkcR30A+uv185MgEhgCb6xpprYHkQAwSQxnAwAAoBiZSABzBplIABgdMpEAAAAoRhAJAACAYgSRAAAAKEYQCQAAgGIEkQAAAChGEAkAAIBiBJEAAAAoRhAJAACAYgSRAAAAKEYQCQAAgGIEkQAAAChGEAkAAIBiBJEAAAAoRhAJAACAYgSRAAAAKEYQCQAAgGIEkQAAAChGEAkAAIBiBJEAAAAoRhAJAACAYgSRAAAAKEYQCQAAgGIEkQAAAChGEAkAAIBiBJEAAAAoRhAJAACAYgSRAAAAKOaImO02AAAAoGHIRAIAAKAYQSQAAACKEUQCAACgGEEkAAAAihFEAgAAoBhBJAAAAIoRRAIAAKAYQSQAAACKEUQCAACgGEEkAAAAihFEAgAAoBhBJAAAAIoRRAIAAKAYQSQAAACKEUQCAACgGEEkAAAAihFEAgAAoBhBJAAAAIoRRAIAAKAYQSQAAACKEUQCAACgGEEkAAAAihFEAgAAoBhBJAAAAIoRRAIAAKAYQSQAAACK/X/UmaVpkuh0/QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 792x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(11, 7))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Original observation (160×210 RGB)\")\n",
    "plt.imshow(obs)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(122)\n",
    "plt.title(\"Preprocessed observation (88×80 greyscale)\")\n",
    "plt.imshow(img.reshape(88, 80), interpolation=\"nearest\", cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
