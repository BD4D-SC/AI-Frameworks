{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" width=400, style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 4 Introduction to Reinforcement Learning  -  Part 1 : Policy Gradient Algorithm\n",
    "\n",
    "The objectives of this noteboks are the following : \n",
    "\n",
    "* Discover AI Gym librairy and environment\n",
    "* Implement Hard-Coded & And Neural network policy\n",
    "* Implement Policy gradient algorithm\n",
    " \n",
    "Source : [https://github.com/ageron/handson-ml](https://github.com/ageron/handson-ml) and https://github.com/breeko/Simple-Reinforcement-Learning-with-Tensorflow/blob/master/Part%202%20-%20Policy-based%20Agents%20with%20Keras.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot figures and animations\n",
    "%matplotlib inline\n",
    "%matplotlib nbagg\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "#Tensorflow/Keras utils\n",
    "import tensorflow.keras.models as km\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.initializers as ki\n",
    "import tensorflow.keras.optimizers as ko\n",
    "import tensorflow.keras.losses as klo\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "# Gym Library\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# AI Gym Librairie\n",
    "<a href=\"https://gym.openai.com/\" ><img src=\"https://gym.openai.com/assets/dist/home/header/home-icon-54c30e2345.svg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be using [OpenAI gym](https://gym.openai.com/), a great toolkit for developing and comparing Reinforcement Learning algorithms. It provides many environments for your learning *agents* to interact with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple environment: the Cart-Pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n",
    "\n",
    "### Observation\n",
    "\n",
    "Num | Observation | Min | Max\n",
    "---|---|---|---\n",
    "0 | Cart Position | -2.4 | 2.4\n",
    "1 | Cart Velocity | -Inf | Inf\n",
    "2 | Pole Angle | ~ -41.8&deg; | ~ 41.8&deg;\n",
    "3 | Pole Velocity At Tip | -Inf | Inf\n",
    "\n",
    "### Actions\n",
    "\n",
    "Num | Action\n",
    "--- | ---\n",
    "0 | Push cart to the left\n",
    "1 | Push cart to the righ&t\n",
    "\n",
    "Note: The amount the velocity is reduced or increased is not fixed as it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n",
    "\n",
    "### Reward\n",
    "Reward is 1 for every step taken, including the termination step\n",
    "\n",
    "### Starting State\n",
    "All observations are assigned a uniform random value between ±0.05\n",
    "\n",
    "### Episode Termination\n",
    "1. Pole Angle is more than ±12°\n",
    "2. Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)\n",
    "3. Episode length is greater than 200\n",
    "\n",
    "### Solved Requirements\n",
    "Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials.\n",
    "\n",
    "The description above if part of the official description of this environemtn. Read full description [here](https://github.com/openai/gym/wiki/CartPole-v0).\n",
    "\n",
    "The following command will load the `CartPole` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reset` command initialize the environement and return the first observation which are a 1D array of size 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "env.observation_space, obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** What are the four output above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `render` command allow to generate the environment which is here a 400X600 pixels with RGB channel. \n",
    "\n",
    "The `render` command for the `CartPole`environment also open another window that we will close directly with the `env.close`command bu can produce disturbing behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = env.render(mode = \"rgb_array\")\n",
    "env.close()\n",
    "print(\"Environemnt is a %dx%dx%d images\" %img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment can then easily be displayed with matplotlib function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space is composed of two actions push to the left (0), push to the right (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `step function`allow to apply one of this actions and return several information : \n",
    "\n",
    "* The new observation after applying this action\n",
    "* The reward this action has produce\n",
    "* A boolean that indicate if the experience is over or not.\n",
    "* Extra information that depend of the environment (CartPole environment does not provide anything).\n",
    "\n",
    "Let's push the cart pole to the left!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, done, info = env.step(0)\n",
    "print(\"New observation : %s\" %str(obs))\n",
    "print(\"Reward : %s\" %str(reward))\n",
    "print(\"Is the experience over? : %s\" %str(done))\n",
    "print(\"Extra information : %s\" %str(info))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = env.render(mode = \"rgb_array\")\n",
    "env.close()\n",
    "plt.imshow(img)\n",
    "axs =  plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** : What can you see? Does it seems normal to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** : Reset the environment, and push the car to the left untill the experience is over then display the final environment. \n",
    "**Q** : Why do the environment ends? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise_1_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard coded policy\n",
    "\n",
    "How can we make the poll remain upright? We will need to define a _policy_ for that. This is the strategy that the agent will use to select an action at each step. It can use all the past actions and observations to decide what to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a completly random policy and see how much time the poll will remain upright over 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env.reset()           \n",
    "num_games = 100       \n",
    "reward_sum = 0        \n",
    "all_reward_sum = []   \n",
    "num_game = 0          \n",
    "while num_game < num_games:\n",
    "    observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        if num_game %10 == 0:\n",
    "            print(\"Game played : %d. Reward for the last 10 episode: %s\" %(num_game,all_reward_sum[-10:]) )\n",
    "        all_reward_sum.append(reward_sum)\n",
    "        reward_sum = 0\n",
    "        num_game += 1\n",
    "        env.reset()\n",
    "        \n",
    "print(\"Over %d episodes, mean reward: %d, std : %d\" %(num_games, np.mean(all_reward_sum), np.std(all_reward_sum)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize a complete game\n",
    "\n",
    "Let's run one pisode with his random policy and save all images representing the environment at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "env.reset()    \n",
    "observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "reward_sum = 0        \n",
    "while not(done):\n",
    "    img = env.render(mode = \"rgb_array\")\n",
    "    env.close()\n",
    "    frames.append(img)\n",
    "    reward_sum += reward\n",
    "    observation, reward, done, _ = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    plt.close()  # or else nbagg sometimes plots in the previous cell\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    return animation.FuncAnimation(fig, update_scene, fargs=(frames, patch), frames=len(frames), repeat=repeat, interval=interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(plot_animation(frames).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's hard code a simple strategy: if the pole is tilting to the left, then push the cart to the left, and _vice versa_. Let's see if that works.\n",
    "\n",
    "**Exercise** implement this policy and play 100 games with this policy. What are the means and std deviation of the reward sum over the 100 games?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise_1_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Vizualize a complete game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise_1_3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** What can you say about this strategy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a neural network to build a better policy. <br> \n",
    "Iwill take observations as inputs, and output the probability of the action to take for each observation. <br>\n",
    "In the case of the Cart-Pole environment, there are just two possible actions (left or right), so we only need one output neuron: it will output the probability `p` of the action 0 (left), and of course the probability of action 1 (right) will be `1 - p`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this problem is simple, we can define a very simple architecture for our neural network. Here it's simple MLP with 1 hidden layer and four neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the network architecture\n",
    "n_inputs = 4  # == env.observation_space.shape[0]\n",
    "n_hidden = 4  # it's a simple task, we don't need more than this\n",
    "n_outputs = 1 # only outputs the probability of accelerating left\n",
    "initializer = ki.VarianceScaling()\n",
    "\n",
    "# Build the neural network\n",
    "policy_network=km.Sequential()\n",
    "policy_network.add(kl.Dense(n_hidden, input_shape = (n_inputs,), activation = \"relu\", kernel_initializer = initializer))\n",
    "policy_network.add(kl.Dense(n_outputs, activation = \"sigmoid\", kernel_initializer = initializer))\n",
    "\n",
    "policy_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the action\n",
    "We can now easly predict the probability one of the both action given the observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "p_left = policy_network.predict(np.expand_dims(obs,axis=0))\n",
    "# Choose and action according to the previously generated probability\n",
    "action = 0 if random.uniform(0,1)< p_left else 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What can you say about the procedure to choose the action? Why are we doing this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NB*** : \n",
    "\n",
    "* In this particular environment, the past actions and observations can safely be ignored, since each observation contains the environment's full state. If there were some hidden state then you may need to consider past actions and observations in order to try to infer the hidden state of the environment. For example, if the environment only revealed the position of the cart but not its velocity, you would have to consider not only the current observation but also the previous observation in order to estimate the current velocity. Another example is if the observations are noisy: you may want to use the past few observations to estimate the most likely current state. Our problem is thus as simple as can be: the current observation is noise-free and contains the environment's full state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random neural network policy.\n",
    "Let's see how this neural network policy perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_sum = 0\n",
    "num_games = 100       \n",
    "num_game = 0\n",
    "all_reward_sum = []\n",
    "obs = env.reset()\n",
    "reward\n",
    "while num_game < num_games:\n",
    "    p_left = policy_network.predict(np.expand_dims(obs,axis=0))\n",
    "    action = 0 if random.uniform(0,1)< p_left else 1 \n",
    "    obs, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        if num_game %10 == 0:\n",
    "            print(\"Game played : %d. Reward for the last 10 episode: %s\" %(num_game,all_reward_sum[-10:]) )\n",
    "        all_reward_sum.append(reward_sum)\n",
    "        reward_sum = 0\n",
    "        num_game += 1\n",
    "        env.reset()\n",
    "print(\"Over %d episodes, mean reward: %d, std : %d\" %(num_games, np.mean(all_reward_sum), np.std(all_reward_sum)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomly initialize this policy neural network and use it to play one game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "env.reset()    \n",
    "observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "reward_sum = 0        \n",
    "while not(done):\n",
    "    img = env.render(mode = \"rgb_array\")\n",
    "    env.close()\n",
    "    frames.append(img)\n",
    "    reward_sum += reward\n",
    "    p_left = policy_network.predict(np.expand_dims(observation,axis=0))\n",
    "    action = 0 if random.uniform(0,1)< p_left else 1 \n",
    "    observation, reward, done, _ = env.step(action)\n",
    "plt.close()\n",
    "HTML(plot_animation(frames).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network is working. But it's still acting randomly. Let's try to make it learn better policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn a given policy\n",
    "\n",
    "In this part we will train the neural network in order that it learns the simple strategy we hard coded before : if the pole is tilting to the left, then push the cart to the left, and _vice versa_.\n",
    "\n",
    "We start by defining the neural network policy with the same architecture than before. <br>\n",
    "This time we compile this network in order to learn the wanted behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_network=km.Sequential()\n",
    "policy_network.add(kl.Dense(n_hidden, input_shape = (n_inputs,), activation = \"relu\", kernel_initializer = initializer))\n",
    "policy_network.add(kl.Dense(n_outputs, activation = \"sigmoid\", kernel_initializer = initializer))\n",
    "\n",
    "# apprentissage\n",
    "learning_rate = 0.01\n",
    "adam = ko.Adam(lr = learning_rate)\n",
    "policy_network.compile(loss='binary_crossentropy',optimizer=ko.Adam(lr = learning_rate),metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to learn this policy we will first define 10 environment that we will play in parallel at each iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_environments = 10\n",
    "\n",
    "envs = [gym.make(\"CartPole-v0\") for _ in range(n_environments)]\n",
    "observations = np.array([env.reset() for env in envs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each iteration we will :\n",
    "\n",
    "* Train the network over the ten observation we have, where these observation will be the input, and the target will be the expected behaviour.\n",
    "* Predict the action according to the neural network policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 2000\n",
    "for iteration in range(n_iterations):\n",
    "    target_probas = np.array([(1 if obs[2] < 0 else 0) for obs in observations]) # if angle<0 we want proba(left)=1., or else proba(left)=0.\n",
    "    policy_network.train_on_batch(observations, target_probas)  # one batch iteration\n",
    "    \n",
    "    # Generate probablity and observation on ntext step\n",
    "    p_lefts = policy_network.predict(observations)\n",
    "    actions = [0 if random.uniform(0,1)< p_left else 1 for p_left in p_lefts]\n",
    "    \n",
    "    # Compute next state\n",
    "    for env_index, env in enumerate(envs):\n",
    "        obs, reward, done, info = env.step(actions[env_index])\n",
    "        observations[env_index] = obs if not done else env.reset()\n",
    "\n",
    "for env in envs:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_sum = 0\n",
    "num_games = 100       \n",
    "num_game = 0\n",
    "all_reward_sum = []\n",
    "obs = env.reset()\n",
    "reward\n",
    "while num_game < num_games:\n",
    "    # hard-coded policy\n",
    "    p_left = policy_network.predict(np.expand_dims(obs,axis=0))\n",
    "    action = 0 if random.uniform(0,1)< p_left else 1 \n",
    "    obs, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        if num_game %10 == 0:\n",
    "            print(\"Game played : %d. Reward for the last 10 episode: %s\" %(num_game,all_reward_sum[-10:]) )\n",
    "        all_reward_sum.append(reward_sum)\n",
    "        reward_sum = 0\n",
    "        num_game += 1\n",
    "        env.reset()\n",
    "print(\"Over %d episodes, mean reward: %d, std : %d\" %(num_games, np.mean(all_reward_sum), np.std(all_reward_sum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "env.reset()    \n",
    "observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "reward_sum = 0        \n",
    "while not(done):\n",
    "    img = env.render(mode = \"rgb_array\")\n",
    "    env.close()\n",
    "    frames.append(img)\n",
    "    reward_sum += reward\n",
    "    p_left = policy_network.predict(np.expand_dims(observation,axis=0))\n",
    "    action = 0 if random.uniform(0,1)< p_left else 1 \n",
    "    observation, reward, done, _ = env.step(action)\n",
    "plt.close()\n",
    "HTML(plot_animation(frames).to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it learned the policy correctly! <br>\n",
    "\n",
    "Let's now reach our final target : The neural network has to find a better policy by its own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind *Policy Gradients* its quite simple : The _Policy Gradients_ algorithm tackles this problem by first playing multiple games, then making the actions in good games slightly more likely, while actions in bad games are made slightly less likely. First we play, then we go back and think about what we did.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "* Run an episode untill it's done and save at each iteration the observation, action and reward.\n",
    "* When an episode it's done. Compute the discounted rewards for all the episode, and save it.\n",
    "* If you have done *batch_size=50* episodes train your model on this batch.\n",
    "* Stop if you have reach *num episodes* or *goal* target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Parameters\n",
    "\n",
    "| Variable  | Value  | Description  | \n",
    "|---|---|---|\n",
    "|Gamma   | 0.99  | The discounted rate apply for the discounted reward  |\n",
    "|batch_size  | 50   | Number of episode to run before training model on a batch of episode  |\n",
    "| Num episodes | 10.000   | Maximum number of episode to run before stopping the training  | \n",
    "| goal | 190  | Number of step to achieve on one episode to stop the training.  |\n",
    "\n",
    "Those parameters are fixed for this TP, they are common value for this kind of problem based on experiences. They are not definitive nor results or any research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounted rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train this neural network we will then used the observation of the experiences as an inputs and the actions taken as an output.\n",
    "\n",
    "But how do we provide to the neural network the information the choosen actions  was good or bad?\n",
    "The problem is that most actions have delayed effects, so when you win or lose points in a game, it is not clear which actions contributed to this result: was it just the last action? Or the last 10? Or just one action 50 steps earlier? <br>\n",
    "This is called the _credit assignment problem_.\n",
    "\n",
    "\n",
    "To tackle this problem, a common strategy is to evaluate an action based on the sum of all the rewards that come after it, usually applying a discount rate r at each step. \n",
    "This rate will the be applied to the loss function of the neural network :\n",
    "* A high discounted reward will lead to higher gradient which will increase the importance of this action\n",
    "* A low  discounted reward will lead to lower gradient which will decrease the importance of this action\n",
    " \n",
    "\n",
    "**Exercise** : Implement the discount_rewards function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma=0.99):\n",
    "    \"\"\"Takes 1d float array of rewards and computes discounted reward\n",
    "    e.g. f([1, 1, 1], 0.99) -> [2.9701, 1.99, 1]\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/exercise_1_4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "As before we will define a very simple architecture to our neural network : A MLP with only one hidden layer and 8 neurons.\n",
    "\n",
    "We have to be aware here that the neural network will have two different behaviour :\n",
    "\n",
    "* For training: the model will take two information as an input : The observations (to predict the action), and the discounted rate (also call advantages) that will be applied on the loss function.\n",
    "* For prediction : the model will take only the observations as an input to predict the action.\n",
    "\n",
    "So we have to define a neural network that can either handle one or two inputs! \n",
    "\n",
    "In keras we define it that way : we define the layers, and we create two models (for training and prediction) that will share the same layers and weight.\n",
    "\n",
    "Here is how we implement it : (Make sure you understand it!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_neurons = 8\n",
    "\n",
    "dimen = env.reset().shape\n",
    "num_actions = 1\n",
    "inp = kl.Input(shape=dimen,name=\"input_x\")\n",
    "adv = kl.Input(shape=[1], name=\"advantages\")\n",
    "x = kl.Dense(hidden_layer_neurons,  activation=\"relu\", \n",
    "                 use_bias=False,\n",
    "                 kernel_initializer=initializer,\n",
    "                 name=\"dense_1\")(inp)\n",
    "out = kl.Dense(num_actions, \n",
    "                   activation=\"sigmoid\", \n",
    "                   kernel_initializer=initializer,\n",
    "                   use_bias=False,\n",
    "                   name=\"out\")(x)\n",
    "\n",
    "model_train = km.Model(inputs=[inp, adv], outputs=out)\n",
    "model_predict = km.Model(inputs=[inp], outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "We will know impement the loss function, which is weighted binary cross entropy, where the weight are the discounted rated computed from the rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "\n",
    "def my_custom_loss(y_true, y_pred):\n",
    "\n",
    "    log_lik = - (y_true*K.log(y_pred) + (1-y_true)*K.log(1-y_pred))\n",
    "    return K.mean(log_lik * adv, keepdims=True)\n",
    "model_train.compile(loss=my_custom_loss, optimizer=ko.Adam(lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Function\n",
    "\n",
    "We define a simple function that will run *num_tests* episode following our learned policy to evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See our trained bot in action\n",
    "def score_model(model, num_tests, render=False):\n",
    "    scores = []    \n",
    "    for num_test in range(num_tests):\n",
    "        observation = env.reset()\n",
    "        reward_sum = 0\n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            state = np.reshape(observation, [1, dimen])\n",
    "            predict = model.predict([state])[0]\n",
    "            action = 0 if predict>0.5 else 1 \n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            reward_sum += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores.append(reward_sum)\n",
    "    env.close()\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "gamma = .99      # -> Discounted reward\n",
    "batch_size = 50  # -> Size of episode before training on a batch\n",
    "dimen = 4        # Size of observation\n",
    "\n",
    "# Stop factor\n",
    "num_episodes = 10000 # Max number of iterations\n",
    "goal = 190           # Stop if our network achieve this goal over *n_test*\n",
    "n_test = 10   \n",
    "\n",
    "print_every = 100 #?Numbe rof episode before trying if our model perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders for our observations, outputs and rewards\n",
    "states = np.empty(0).reshape(0,dimen)\n",
    "actions = np.empty(0).reshape(0,1)\n",
    "rewards = np.empty(0).reshape(0,1)\n",
    "discounted_rewards = np.empty(0).reshape(0,1)\n",
    "\n",
    "# Setting up our environment\n",
    "observation = env.reset()\n",
    "num_episode = 0\n",
    "reward_sum = 0\n",
    "\n",
    "losses = []\n",
    "\n",
    "while num_episode < num_episodes:\n",
    "    \n",
    "    \n",
    "    # Generate state and action for the current iteratio\n",
    "    state = np.reshape(observation, [1, dimen])\n",
    "    predict = model_predict.predict([state])[0]\n",
    "    action = 0 if random.uniform(0,1)< predict else 1 \n",
    "    \n",
    "    # Append the observations and outputs for learning\n",
    "    states = np.vstack([states, state])\n",
    "    actions = np.vstack([actions, action])\n",
    "    \n",
    "    \n",
    "    # Determine the oucome of the action generated \n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    ##Append the rewards for learning\n",
    "    rewards = np.vstack([rewards, reward])\n",
    "    \n",
    "    \n",
    "    # If the episode if Over \n",
    "    if done:\n",
    "        \n",
    "        # Computed the discounted rewards for this episode\n",
    "        discounted_rewards_episode = discount_rewards(rewards, gamma)   \n",
    "        \n",
    "        # Append the discounted rewards for learning\n",
    "        discounted_rewards = np.vstack([discounted_rewards, discounted_rewards_episode])\n",
    "        rewards = np.empty(0).reshape(0,1)\n",
    "        \n",
    "        # if we have reach 50=batch_size episodes run training for the build batch\n",
    "        if (num_episode + 1) % batch_size == 0:\n",
    "            \n",
    "            # Normalize the discounted rewards\n",
    "            discounted_rewards -= discounted_rewards.mean()\n",
    "            discounted_rewards /= discounted_rewards.std()\n",
    "            discounted_rewards = discounted_rewards.squeeze()\n",
    "           \n",
    "        \n",
    "            actions_train = 1-actions\n",
    "            loss = model_train.train_on_batch([states, discounted_rewards], actions_train)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Clear out game variables\n",
    "            states = np.empty(0).reshape(0,dimen)\n",
    "            actions = np.empty(0).reshape(0,1)\n",
    "            discounted_rewards = np.empty(0).reshape(0,1)\n",
    "\n",
    "          \n",
    "        # Print results periodically\n",
    "        if (num_episode + 1) % print_every == 0:\n",
    "            # Print status\n",
    "            score = score_model(model_predict, n_test)\n",
    "            print(\"Average reward for training episode {}: {:0.2f} Test Score of {:d} episode: {:0.2f} Loss: {:0.6f} \".format(\n",
    "                (num_episode + 1), reward_sum/print_every, n_test,\n",
    "                score,\n",
    "                losses[-1]))\n",
    "            \n",
    "            if score >= goal:\n",
    "                print(\"Solved in {} episodes!\".format(num_episode))\n",
    "                break\n",
    "            reward_sum = 0\n",
    "                \n",
    "        num_episode += 1\n",
    "        observation = env.reset()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_sum = 0\n",
    "num_games = 100       \n",
    "num_game = 0\n",
    "all_reward_sum = []\n",
    "obs = env.reset()\n",
    "reward\n",
    "while num_game < num_games:\n",
    "    # hard-coded policy\n",
    "    p_left = model_predict.predict(np.expand_dims(obs,axis=0))\n",
    "    action = 0 if p_left[0][0]>0.5 else 1 \n",
    "    obs, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "    if done:\n",
    "        if num_game %10 == 0:\n",
    "            print(\"Game played : %d. Reward for the last 10 episode: %s\" %(num_game,all_reward_sum[-10:]) )\n",
    "        all_reward_sum.append(reward_sum)\n",
    "        reward_sum = 0\n",
    "        num_game += 1\n",
    "        env.reset()\n",
    "print(\"Over %d episodes, mean reward: %d, std : %d\" %(num_games, np.mean(all_reward_sum), np.std(all_reward_sum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "env.reset()    \n",
    "observation, reward, done, _ = env.step(env.action_space.sample())\n",
    "reward_sum = 0        \n",
    "while not(done):\n",
    "    img = env.render(mode = \"rgb_array\")\n",
    "    env.close()\n",
    "    frames.append(img)\n",
    "    reward_sum += reward\n",
    "    p_left = model_predict.predict(np.expand_dims(observation,axis=0))\n",
    "    action = 0 if  p_left>0.5 else 1 \n",
    "    observation, reward, done, _ = env.step(action)\n",
    "plt.close()\n",
    "HTML(plot_animation(frames).to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
