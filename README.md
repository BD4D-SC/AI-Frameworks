# Ateliers: Technologies des Données Massives avec [R](https://cran.r-project.org/), [Python](https://www.python.org/) et / ou [Spark](href="http://spark.apache.org/)

L'objectifs de ces ateliers ou tutoriels sous forme de calepins ([*jupyter notebooks*](http://jupyter.org/)) est d'introduire le **passage à l'échelle Volume**; processus qui transforme un statisticien en *Data Scientist*. 



## Introduction à [Spark](href="http://spark.apache.org/)
Un premier groupe (dossier App-PySpark) de tutoriels, propose une initiation à l'environnement de travail (*framework*) [*Spark*](http://spark.apache.org/) devenu une référence pour la gestion et l'analyse de données distribuées (*e.g* sous *Hadoop*). L'accent est mis sur la gestion des RDDs (*resilient distributed datasets*) et leur analyse à l'aide des librairies *Mllib* et *SparkML*.

## Cas d'usage
Chacun des autres dossiers concerne un jeu de données et contient un ou des calepins(*notebooks*) au format .ipynb codés en R, Python ou PySpark à télécharger et ouvrir dans *Jupyter*. L'objectifs est de comparer les performances des principaux environnements plus ou moins bien adaptés à l'analyse de données massives en fonction du but visé.

Il est question de reconnaissance de caractères (MNIST), de recommandation de films (MovieLens) et de catégorisation de produits (Cdiscount).

La synthèse des résultats obtenus est développée par [Besse et al. 2016](https://hal.archives-ouvertes.fr/hal-01350099).

