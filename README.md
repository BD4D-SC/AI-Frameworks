# Ateliers: Technologies des Données Massives avec [R](https://cran.r-project.org/), [Python](https://www.python.org/) et / ou [Spark](href="http://spark.apache.org/)

## Introduction à [Spark](href="http://spark.apache.org/)
Un premier groupe (dossier App-PySpark) de tutoriels, sous forme de calepins ([*jupyter notebooks*](http://jupyter.org/)), propose une introduction à l'environnement de travail (*framework*) [*Spark*](http://spark.apache.org/) devenu une référence pour la gestion et l'analyse de données distribuées (*e.g* sous *Hadoop*). L'accent est mis sur la gestion des RDDs (*resilient distributed datasets*) et leur analyse à l'aide des librairies *Mllib* et *SparkML*.

## Cas d'usage
Chacun des autres dossiers concerne un jeu de données et contient un ou des calepins(*notebooks*) au format .ipynb codés en R, Python ou PySpark à télécharger et ouvrir dans *Jupyter*. L'un des objectifs est de comparer les performances des principaux environnements plus ou moins bien adaptés à l'analyse de données massives en fonction du but visé.

