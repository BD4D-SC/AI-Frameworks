{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies de l'intelligence Artificielle](https://github.com/wikistat/AI-Frameworks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" width=400, style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Reinforcement Learning \n",
    " \n",
    "Source : [https://github.com/ageron/handson-ml](https://github.com/ageron/handson-ml)\n",
    "\n",
    "\n",
    "https://github.com/breeko/Simple-Reinforcement-Learning-with-Tensorflow/blob/master/Part%202%20-%20Policy-based%20Agents%20with%20Keras.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Gym Librairie\n",
    "<a href=\"https://gym.openai.com/\" ><img src=\"https://gym.openai.com/assets/dist/home/header/home-icon-54c30e2345.svg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be using [OpenAI gym](https://gym.openai.com/), a great toolkit for developing and comparing Reinforcement Learning algorithms. It provides many environments for your learning *agents* to interact with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures and animations\n",
    "%matplotlib inline\n",
    "%matplotlib nbagg\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "\n",
    "import tensorflow.keras.models as km\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.initializers as ki\n",
    "import tensorflow.keras.optimizers as ko\n",
    "import tensorflow.keras.losses as klo\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "# Gym Librairy\n",
    "import gym\n",
    "\n",
    "def display_environment(env):\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    env.close()\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: 0 1 2 1 3 \n",
      "States: 0 1 2 1 2 1 3 \n",
      "States: 0 0 0 0 0 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "States: 0 0 0 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "States: 0 0 0 0 0 0 1 3 \n",
      "States: 0 0 0 0 0 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 ...\n",
      "States: 0 0 0 0 3 \n",
      "States: 0 0 0 0 0 0 0 0 0 0 0 0 3 \n",
      "States: 0 0 0 0 0 0 0 0 0 0 3 \n",
      "States: 0 0 0 0 0 0 0 0 0 0 0 0 0 3 \n"
     ]
    }
   ],
   "source": [
    "transition_probabilities = [\n",
    "        [0.7, 0.2, 0.0, 0.1],  # from s0 to s0, s1, s2, s3\n",
    "        [0.0, 0.0, 0.9, 0.1],  # from s1 to ...\n",
    "        [0.0, 1.0, 0.0, 0.0],  # from s2 to ...\n",
    "        [0.0, 0.0, 0.0, 1.0],  # from s3 to ...\n",
    "    ]\n",
    "\n",
    "n_max_steps = 50\n",
    "\n",
    "def print_sequence(start_state=0):\n",
    "    current_state = start_state\n",
    "    print(\"States:\", end=\" \")\n",
    "    for step in range(n_max_steps):\n",
    "        print(current_state, end=\" \")\n",
    "        if current_state == 3:\n",
    "            break\n",
    "        current_state = np.random.choice(range(4), p=transition_probabilities[current_state])\n",
    "    else:\n",
    "        print(\"...\", end=\"\")\n",
    "    print()\n",
    "\n",
    "for _ in range(10):\n",
    "    print_sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_fire\n",
      "States (+rewards): 0 (10) 0 (10) 0 (10) 0 1 (-50) 2 (40) 0 (10) 0 (10) 0 (10) 0 ... Total rewards = 10\n",
      "States (+rewards): 0 (10) 0 (10) 0 (10) 0 (10) 0 1 (-50) 2 (40) 0 1 (-50) 2 (40) ... Total rewards = 50\n",
      "States (+rewards): 0 (10) 0 1 (-50) 2 2 2 (40) 0 (10) 0 (10) 0 1 (-50) ... Total rewards = 300\n",
      "States (+rewards): 0 (10) 0 1 (-50) 2 (40) 0 (10) 0 (10) 0 (10) 0 1 (-50) 2 (40) ... Total rewards = 70\n",
      "States (+rewards): 0 (10) 0 1 (-50) 2 (40) 0 (10) 0 (10) 0 (10) 0 1 (-50) 2 (40) ... Total rewards = 300\n",
      "Summary: mean=120.5, std=133.232951, min=-360, max=550\n",
      "\n",
      "policy_random\n",
      "States (+rewards): 0 0 1 (-50) 2 (40) 0 0 (10) 0 (10) 0 (10) 0 (10) 0 ... Total rewards = 60\n",
      "States (+rewards): 0 0 0 0 (10) 0 (10) 0 (10) 0 (10) 0 0 0 ... Total rewards = -30\n",
      "States (+rewards): 0 0 0 (10) 0 0 0 0 0 1 1 (-50) ... Total rewards = -90\n",
      "States (+rewards): 0 0 (10) 0 (10) 0 1 1 1 (-50) 2 (40) 0 0 ... Total rewards = -170\n",
      "States (+rewards): 0 0 (10) 0 0 0 0 0 1 1 (-50) 2 (40) ... Total rewards = 0\n",
      "Summary: mean=-29.2, std=86.235675, min=-440, max=200\n",
      "\n",
      "policy_safe\n",
      "States (+rewards): 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 1 1 1 ... Total rewards = 60\n",
      "States (+rewards): 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) ... Total rewards = 150\n",
      "States (+rewards): 0 1 1 1 1 1 1 1 1 1 ... Total rewards = 0\n",
      "States (+rewards): 0 (10) 0 1 1 1 1 1 1 1 1 ... Total rewards = 10\n",
      "States (+rewards): 0 (10) 0 (10) 0 1 1 1 1 1 1 1 ... Total rewards = 20\n",
      "Summary: mean=23.3, std=29.537703, min=0, max=240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transition_probabilities = [\n",
    "        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]], # in s0, if action a0 then proba 0.7 to state s0 and 0.3 to state s1, etc.\n",
    "        [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "        [None, [0.8, 0.1, 0.1], None],\n",
    "    ]\n",
    "\n",
    "rewards = [\n",
    "        [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "        [[0, 0, 0], [+40, 0, 0], [0, 0, 0]],\n",
    "    ]\n",
    "\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]\n",
    "\n",
    "def policy_fire(state):\n",
    "    return [0, 2, 1][state]\n",
    "\n",
    "def policy_random(state):\n",
    "    return np.random.choice(possible_actions[state])\n",
    "\n",
    "def policy_safe(state):\n",
    "    return [0, 0, 1][state]\n",
    "\n",
    "class MDPEnvironment(object):\n",
    "    def __init__(self, start_state=0):\n",
    "        self.start_state=start_state\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.total_rewards = 0\n",
    "        self.state = self.start_state\n",
    "    def step(self, action):\n",
    "        next_state = np.random.choice(range(3), p=transition_probabilities[self.state][action])\n",
    "        reward = rewards[self.state][action][next_state]\n",
    "        self.state = next_state\n",
    "        self.total_rewards += reward\n",
    "        return self.state, reward\n",
    "\n",
    "def run_episode(policy, n_steps, start_state=0, display=True):\n",
    "    env = MDPEnvironment()\n",
    "    if display:\n",
    "        print(\"States (+rewards):\", end=\" \")\n",
    "    for step in range(n_steps):\n",
    "        if display:\n",
    "            if step == 10:\n",
    "                print(\"...\", end=\" \")\n",
    "            elif step < 10:\n",
    "                print(env.state, end=\" \")\n",
    "        action = policy(env.state)\n",
    "        state, reward = env.step(action)\n",
    "        if display and step < 10:\n",
    "            if reward:\n",
    "                print(\"({})\".format(reward), end=\" \")\n",
    "    if display:\n",
    "        print(\"Total rewards =\", env.total_rewards)\n",
    "    return env.total_rewards\n",
    "\n",
    "for policy in (policy_fire, policy_random, policy_safe):\n",
    "    all_totals = []\n",
    "    print(policy.__name__)\n",
    "    for episode in range(1000):\n",
    "        all_totals.append(run_episode(policy, n_steps=100, display=(episode<5)))\n",
    "    print(\"Summary: mean={:.1f}, std={:1f}, min={}, max={}\".format(np.mean(all_totals), np.std(all_totals), np.min(all_totals), np.max(all_totals)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning works by watching an agent play (e.g., randomly) and gradually improving its estimates of the Q-Values. Once it has accurate Q-Value estimates (or close enough), then the optimal policy consists in choosing the action that has the highest Q-Value (i.e., the greedy policy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 3\n",
    "n_actions = 3\n",
    "n_steps = 20\n",
    "alpha = 0.01\n",
    "gamma = 0.99\n",
    "exploration_policy = policy_random\n",
    "q_values = np.full((n_states, n_actions), -np.inf)\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    q_values[state][actions]=0\n",
    "#display(q_values)\n",
    "env = MDPEnvironment()\n",
    "for step in range(n_steps):\n",
    "    #print(step)\n",
    "    \n",
    "    action = exploration_policy(env.state) \n",
    "    state = env.state\n",
    "    next_state, reward = env.step(action)\n",
    "    next_value = np.max(q_values[next_state]) # greedy policy\n",
    "    #print(\"Current state : %s, Action choosen : %s, lead to state : %s, reward obtain : %s next value: %s\"%(action, state, next_state, reward, next_value))\n",
    "    q_values[state, action] = (1-alpha)*q_values[state, action] + alpha*(reward + gamma * next_value)\n",
    "    #display(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.59885052,  0.0097323 ,  0.02140305],\n",
       "       [ 0.        ,        -inf, -0.5       ],\n",
       "       [       -inf,  0.4009801 ,        -inf]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(state):\n",
    "    return np.argmax(q_values[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.59885052,  0.0097323 ,  0.02140305],\n",
       "       [ 0.        ,        -inf, -0.5       ],\n",
       "       [       -inf,  0.4009801 ,        -inf]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States (+rewards): 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 1 1 ... Total rewards = 70\n",
      "States (+rewards): 0 1 1 1 1 1 1 1 1 1 ... Total rewards = 0\n",
      "States (+rewards): 0 (10) 0 (10) 0 (10) 0 (10) 0 1 1 1 1 1 ... Total rewards = 40\n",
      "States (+rewards): 0 1 1 1 1 1 1 1 1 1 ... Total rewards = 0\n",
      "States (+rewards): 0 (10) 0 (10) 0 (10) 0 (10) 0 1 1 1 1 1 ... Total rewards = 40\n",
      "Summary: mean=23.9, std=27.876469, min=0, max=180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_totals = []\n",
    "for episode in range(1000):\n",
    "    all_totals.append(run_episode(optimal_policy, n_steps=100, display=(episode<5)))\n",
    "print(\"Summary: mean={:.1f}, std={:1f}, min={}, max={}\".format(np.mean(all_totals), np.std(all_totals), np.min(all_totals), np.max(all_totals)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Play MsPacman Using the DQN Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: Unfortunately, the first version of the book contained two important errors in this section.\n",
    "\n",
    "1. The actor DQN and critic DQN should have been named _online DQN_ and _target DQN_ respectively. Actor-critic algorithms are a distinct class of algorithms.\n",
    "2. The online DQN is the one that learns and is copied to the target DQN at regular intervals. The target DQN's only role is to estimate the next state's Q-Values for each possible action. This is needed to compute the target Q-Values for training the online DQN, as shown in this equation:\n",
    "\n",
    "$y(s,a) = \\text{r} + \\gamma . \\underset{a'}{\\max} \\, Q_\\text{target}(s', a')$\n",
    "\n",
    "* $y(s,a)$ is the target Q-Value to train the online DQN for the state-action pair $(s, a)$.\n",
    "* $r$ is the reward actually collected after playing action $a$ in state $s$.\n",
    "* $\\gamma$ is the discount rate.\n",
    "* $s'$ is the state actually reached after played action $a$ in state $s$.\n",
    "* $a'$ is one of the possible actions in state $s'$.\n",
    "* $Q_\\text{target}(s', a')$ is the target DQN's estimate of the Q-Value of playing action $a'$ while in state $s'$.\n",
    "\n",
    "I hope these errors did not affect you, and if they did, I sincerely apologize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the MsPacman environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"MsPacman-v0\")\n",
    "obs = env.reset()\n",
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the images is optional but greatly speeds up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 80, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mspacman_color = 210 + 164 + 74\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "    img = obs[1:176:2, ::2] # crop and downsize\n",
    "    img = img.sum(axis=2) # to greyscale\n",
    "    img[img==mspacman_color] = 0 # Improve contrast\n",
    "    img = (img // 3 - 128).astype(np.int8) # normalize from -128 to 127\n",
    "    return img.reshape(88, 80, 1)/128\n",
    "\n",
    "img = preprocess_observation(obs)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the `preprocess_observation()` function is slightly different from the one in the book: instead of representing pixels as 64-bit floats from -1.0 to 1.0, it represents them as signed bytes (from -128 to 127). The benefit is that the replay memory will take up roughly 8 times less RAM (about 6.5 GB instead of 52 GB). The reduced precision has no visible impact on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(11, 7))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Original observation (160×210 RGB)\")\n",
    "plt.imshow(obs)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(122)\n",
    "plt.title(\"Preprocessed observation (88×80 greyscale)\")\n",
    "plt.imshow(img.reshape(88, 80), interpolation=\"nearest\", cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: instead of using `tf.contrib.layers.convolution2d()` or `tf.contrib.layers.conv2d()` (as in the first version of the book), we now use the `tf.layers.conv2d()`, which did not exist when this chapter was written. This is preferable because anything in contrib may change or be deleted without notice, while `tf.layers` is part of the official API. As you will see, the code is mostly the same, except that the parameter names have changed slightly:\n",
    "* the `num_outputs` parameter was renamed to `filters`,\n",
    "* the `stride` parameter was renamed to `strides`,\n",
    "* the `_fn` suffix was removed from parameter names that had it (e.g., `activation_fn` was renamed to `activation`),\n",
    "* the `weights_initializer` parameter was renamed to `kernel_initializer`,\n",
    "* the weights variable was renamed to `\"kernel\"` (instead of `\"weights\"`), and the biases variable was renamed from `\"biases\"` to `\"bias\"`,\n",
    "* and the default `activation` is now `None` instead of `tf.nn.relu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_height = 88\n",
    "input_width = 80\n",
    "input_channels = 1\n",
    "initializer =  ki.VarianceScaling()\n",
    "\n",
    "\n",
    "\n",
    "def q_network():\n",
    "    q_model = km.Sequential()\n",
    "    q_model.add(kl.Conv2D(32,kernel_size=(8,8),strides=(4,4),padding=\"SAME\", activation=\"relu\", kernel_initializer = initializer, \n",
    "                          input_shape=(input_height, input_width, input_channels), data_format=\"channels_last\"))\n",
    "    q_model.add(kl.Conv2D(64,kernel_size=(4,4),strides=(2,2),padding=\"SAME\", activation=\"relu\",  kernel_initializer = initializer))\n",
    "    q_model.add(kl.Conv2D(64,kernel_size=(3,4),strides=(1,1),padding=\"SAME\", activation=\"relu\",  kernel_initializer = initializer))\n",
    "    q_model.add(kl.Flatten())\n",
    "    q_model.add(kl.Dense(512, activation=\"relu\", kernel_initializer=initializer))\n",
    "    q_model.add(kl.Dense(9, activation=\"linear\", kernel_initializer=initializer))\n",
    "    \n",
    "    learning_rate = 0.001\n",
    "    momentum = 0.95\n",
    "    Opt = ko.SGD(lr=learning_rate, momentum=momentum , decay=0.0, nesterov=True)\n",
    "    q_model.compile(loss='mean_squared_error',optimizer=Opt) # later change to square loss if error >1 else linear\n",
    "\n",
    "\n",
    "\n",
    "    return q_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if False : \n",
    "    with tf.variable_scope(\"train\"):\n",
    "        X_action = tf.placeholder(tf.int32, shape=[None])\n",
    "        y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        q_value = tf.reduce_sum(online_q_values * tf.one_hot(X_action, n_outputs),\n",
    "                                axis=1, keepdims=True)\n",
    "        error = tf.abs(y - q_value)\n",
    "        clipped_error = tf.clip_by_value(error, 0.0, 1.0)\n",
    "        linear_error = 2 * (error - clipped_error)\n",
    "        loss = tf.reduce_mean(tf.square(clipped_error) + linear_error)\n",
    "\n",
    "        global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "        training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: in the first version of the book, the loss function was simply the squared error between the target Q-Values (`y`) and the estimated Q-Values (`q_value`). However, because the experiences are very noisy, it is better to use a quadratic loss only for small errors (below 1.0) and a linear loss (twice the absolute error) for larger errors, which is what the code above computes. This way large errors don't push the model parameters around as much. Note that we also tweaked some hyperparameters (using a smaller learning rate, and using Nesterov Accelerated Gradients rather than Adam optimization, since adaptive gradient algorithms may sometimes be bad, according to this [paper](https://arxiv.org/abs/1705.08292)). We also tweaked a few other hyperparameters below (a larger replay memory, longer decay for the $\\epsilon$-greedy policy, larger discount rate, less frequent copies of the online DQN to the target DQN, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this `ReplayMemory` class instead of a `deque` because it is much faster for random access (thanks to @NileshPS who contributed it). Moreover, we default to sampling with replacement, which is much faster than sampling without replacement for large replay memories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(self,buffer_size=500000):\n",
    "        \"\"\" Data structure used to hold game experiences \"\"\"\n",
    "        # Buffer will contain [state,action,reward,next_state,done]\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self, experience):\n",
    "        \"\"\" Adds list of experiences to the buffer \"\"\"\n",
    "        # Extend the stored experiences\n",
    "        self.buffer.extend(experience)\n",
    "        # Keep the last buffer_size number of experiences\n",
    "        self.buffer = self.buffer[-self.buffer_size:]\n",
    "        \n",
    "    def sample(self, size):\n",
    "        \"\"\" Returns a sample of experiences from the buffer \"\"\"\n",
    "        sample_idxs = np.random.randint(len(self.buffer),size=size)\n",
    "        sample_output = [self.buffer[idx] for idx in sample_idxs]\n",
    "        sample_output = np.reshape(sample_output,(size,-1))\n",
    "        return sample_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_memories(batch_size):\n",
    "    cols = [[], [], [], [], []] # state, action, reward, next_state, continue\n",
    "    for memory in replay_memory.sample(batch_size):\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return cols[0], cols[1], cols[2].reshape(-1, 1), cols[3], cols[4].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_min = 0.1\n",
    "eps_max = 1.0\n",
    "eps_decay_steps = 2000000\n",
    "\n",
    "def epsilon_greedy(q_values, step):\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs) # random action\n",
    "    else:\n",
    "        return np.argmax(q_values) # optimal action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_random_start = 1\n",
    "prob_random_end = 0.1\n",
    "annealing_steps = 2000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_graph(main_graph, target_graph, tau):\n",
    "    updated_weights = (np.array(main_graph.get_weights()) * tau) + \\\n",
    "        (np.array(target_graph.get_weights()) * (1 - tau))\n",
    "    target_graph.set_weights(updated_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 4000000  # total number of training steps\n",
    "pre_train_episodes = 1000  # start training after 10,000 game iterations\n",
    "update_freq = 4  # run a training step every 4 game iterations\n",
    "save_steps = 1000  # save the model every 1,000 training steps\n",
    "copy_steps = 10000  # copy online DQN to target DQN every 10,000 training steps\n",
    "y = 0.99\n",
    "skip_start = 90  # Skip the start of every game (it's just waiting time).\n",
    "batch_size = 50\n",
    "iteration = 0  # game iterations\n",
    "num_epochs=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few variables for tracking progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val = np.infty\n",
    "game_length = 0\n",
    "total_max_q = 0\n",
    "mean_max_q = 0.0\n",
    "tau=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num episode: 100 Mean reward: 214.6000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 200 Mean reward: 199.6000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 300 Mean reward: 206.4000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 400 Mean reward: 200.6000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 500 Mean reward: 210.2000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 600 Mean reward: 215.2000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 700 Mean reward: 220.4000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 800 Mean reward: 219.2000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 900 Mean reward: 209.6000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 1000 Mean reward: 230.1000 Prob random: 1.0000, Loss: 0.0000\n",
      "Num episode: 1100 Mean reward: 235.0000 Prob random: 1.0000, Loss: 3.3979\n",
      "Num episode: 1200 Mean reward: 205.2000 Prob random: 0.9999, Loss: 1.8866\n",
      "Num episode: 1300 Mean reward: 212.2000 Prob random: 0.9999, Loss: 1.5423\n",
      "Num episode: 1400 Mean reward: 208.6000 Prob random: 0.9998, Loss: 1.2705\n",
      "Num episode: 1500 Mean reward: 202.0000 Prob random: 0.9998, Loss: 0.5692\n",
      "Num episode: 1600 Mean reward: 192.6000 Prob random: 0.9997, Loss: 0.8514\n",
      "Num episode: 1700 Mean reward: 210.9000 Prob random: 0.9997, Loss: 0.9444\n",
      "Num episode: 1800 Mean reward: 199.9000 Prob random: 0.9996, Loss: 0.9968\n",
      "Num episode: 1900 Mean reward: 202.4000 Prob random: 0.9996, Loss: 1.2173\n",
      "Num episode: 2000 Mean reward: 225.6000 Prob random: 0.9996, Loss: 1.0551\n",
      "Num episode: 2100 Mean reward: 217.0000 Prob random: 0.9995, Loss: 0.8702\n",
      "Num episode: 2200 Mean reward: 208.1000 Prob random: 0.9995, Loss: 1.7112\n",
      "Num episode: 2300 Mean reward: 213.1000 Prob random: 0.9994, Loss: 1.4828\n",
      "Num episode: 2400 Mean reward: 195.2000 Prob random: 0.9994, Loss: 1.4626\n",
      "Num episode: 2500 Mean reward: 226.7000 Prob random: 0.9993, Loss: 1.4646\n",
      "Num episode: 2600 Mean reward: 209.2000 Prob random: 0.9993, Loss: 0.6281\n",
      "Num episode: 2700 Mean reward: 216.0000 Prob random: 0.9992, Loss: 0.6325\n",
      "Num episode: 2800 Mean reward: 187.2000 Prob random: 0.9992, Loss: 0.5980\n",
      "Num episode: 2900 Mean reward: 215.2000 Prob random: 0.9991, Loss: 0.8858\n",
      "Num episode: 3000 Mean reward: 230.6000 Prob random: 0.9991, Loss: 0.8395\n",
      "Num episode: 3100 Mean reward: 208.9000 Prob random: 0.9991, Loss: 0.8468\n",
      "Num episode: 3200 Mean reward: 207.8000 Prob random: 0.9990, Loss: 1.0432\n",
      "Num episode: 3300 Mean reward: 209.3000 Prob random: 0.9990, Loss: 1.5671\n",
      "Num episode: 3400 Mean reward: 219.4000 Prob random: 0.9989, Loss: 1.6743\n",
      "Num episode: 3500 Mean reward: 213.4000 Prob random: 0.9989, Loss: 1.8727\n",
      "Num episode: 3600 Mean reward: 216.0000 Prob random: 0.9988, Loss: 1.6903\n",
      "Num episode: 3700 Mean reward: 201.2000 Prob random: 0.9988, Loss: 0.9009\n",
      "Num episode: 3800 Mean reward: 211.4000 Prob random: 0.9987, Loss: 0.8547\n",
      "Num episode: 3900 Mean reward: 207.7000 Prob random: 0.9987, Loss: 0.6797\n",
      "Num episode: 4000 Mean reward: 206.0000 Prob random: 0.9987, Loss: 0.6342\n",
      "Num episode: 4100 Mean reward: 203.7000 Prob random: 0.9986, Loss: 0.6895\n",
      "Num episode: 4200 Mean reward: 212.8000 Prob random: 0.9986, Loss: 0.6541\n",
      "Num episode: 4300 Mean reward: 231.1000 Prob random: 0.9985, Loss: 1.4187\n",
      "Num episode: 4400 Mean reward: 212.5000 Prob random: 0.9985, Loss: 1.4283\n",
      "Num episode: 4500 Mean reward: 208.1000 Prob random: 0.9984, Loss: 1.3985\n",
      "Num episode: 4600 Mean reward: 207.2000 Prob random: 0.9984, Loss: 1.4165\n",
      "Num episode: 4700 Mean reward: 207.7000 Prob random: 0.9983, Loss: 0.9685\n",
      "Num episode: 4800 Mean reward: 207.6000 Prob random: 0.9983, Loss: 0.9847\n",
      "Num episode: 4900 Mean reward: 205.1000 Prob random: 0.9982, Loss: 1.0316\n",
      "Num episode: 5000 Mean reward: 220.0000 Prob random: 0.9982, Loss: 1.0806\n",
      "Num episode: 5100 Mean reward: 208.2000 Prob random: 0.9982, Loss: 0.7890\n",
      "Num episode: 5200 Mean reward: 200.6000 Prob random: 0.9981, Loss: 0.9398\n",
      "Num episode: 5300 Mean reward: 206.6000 Prob random: 0.9981, Loss: 0.9937\n",
      "Num episode: 5400 Mean reward: 216.9000 Prob random: 0.9980, Loss: 1.2348\n",
      "Num episode: 5500 Mean reward: 232.1000 Prob random: 0.9980, Loss: 1.2216\n",
      "Num episode: 5600 Mean reward: 199.7000 Prob random: 0.9979, Loss: 1.1498\n",
      "Num episode: 5700 Mean reward: 211.2000 Prob random: 0.9979, Loss: 1.1002\n",
      "Num episode: 5800 Mean reward: 215.1000 Prob random: 0.9978, Loss: 0.9318\n",
      "Num episode: 5900 Mean reward: 211.4000 Prob random: 0.9978, Loss: 1.0161\n",
      "Num episode: 6000 Mean reward: 207.2000 Prob random: 0.9978, Loss: 1.0092\n",
      "Num episode: 6100 Mean reward: 203.9000 Prob random: 0.9977, Loss: 1.0033\n",
      "Num episode: 6200 Mean reward: 227.6000 Prob random: 0.9977, Loss: 0.9482\n",
      "Num episode: 6300 Mean reward: 210.6000 Prob random: 0.9976, Loss: 0.8769\n",
      "Num episode: 6400 Mean reward: 202.2000 Prob random: 0.9976, Loss: 0.9480\n",
      "Num episode: 6500 Mean reward: 213.4000 Prob random: 0.9975, Loss: 0.9740\n",
      "Num episode: 6600 Mean reward: 210.5000 Prob random: 0.9975, Loss: 0.9024\n",
      "Num episode: 6700 Mean reward: 196.9000 Prob random: 0.9974, Loss: 0.9838\n",
      "Num episode: 6800 Mean reward: 205.7000 Prob random: 0.9974, Loss: 0.8858\n",
      "Num episode: 6900 Mean reward: 207.1000 Prob random: 0.9973, Loss: 0.7935\n",
      "Num episode: 7000 Mean reward: 235.4000 Prob random: 0.9973, Loss: 0.8402\n",
      "Num episode: 7100 Mean reward: 207.0000 Prob random: 0.9973, Loss: 0.7741\n",
      "Num episode: 7200 Mean reward: 225.1000 Prob random: 0.9972, Loss: 0.9466\n",
      "Num episode: 7300 Mean reward: 206.7000 Prob random: 0.9972, Loss: 1.0439\n",
      "Num episode: 7400 Mean reward: 217.4000 Prob random: 0.9971, Loss: 0.9939\n",
      "Num episode: 7500 Mean reward: 220.2000 Prob random: 0.9971, Loss: 1.0682\n",
      "Num episode: 7600 Mean reward: 205.4000 Prob random: 0.9970, Loss: 0.8494\n",
      "Num episode: 7700 Mean reward: 198.6000 Prob random: 0.9970, Loss: 0.8461\n",
      "Num episode: 7800 Mean reward: 213.5000 Prob random: 0.9969, Loss: 0.8919\n",
      "Num episode: 7900 Mean reward: 205.6000 Prob random: 0.9969, Loss: 0.8961\n",
      "Num episode: 8000 Mean reward: 219.1000 Prob random: 0.9969, Loss: 1.0138\n",
      "Num episode: 8100 Mean reward: 218.4000 Prob random: 0.9968, Loss: 1.1254\n",
      "Num episode: 8200 Mean reward: 206.8000 Prob random: 0.9968, Loss: 1.0899\n",
      "Num episode: 8300 Mean reward: 196.7000 Prob random: 0.9967, Loss: 0.9934\n",
      "Num episode: 8400 Mean reward: 203.0000 Prob random: 0.9967, Loss: 0.8662\n",
      "Num episode: 8500 Mean reward: 199.6000 Prob random: 0.9966, Loss: 0.6808\n",
      "Num episode: 8600 Mean reward: 198.6000 Prob random: 0.9966, Loss: 0.7522\n",
      "Num episode: 8700 Mean reward: 208.2000 Prob random: 0.9965, Loss: 0.7728\n",
      "Num episode: 8800 Mean reward: 227.8000 Prob random: 0.9965, Loss: 0.8354\n",
      "Num episode: 8900 Mean reward: 211.9000 Prob random: 0.9964, Loss: 0.9711\n",
      "Num episode: 9000 Mean reward: 193.4000 Prob random: 0.9964, Loss: 0.8929\n",
      "Num episode: 9100 Mean reward: 205.3000 Prob random: 0.9964, Loss: 0.9749\n",
      "Num episode: 9200 Mean reward: 205.4000 Prob random: 0.9963, Loss: 0.9633\n",
      "Num episode: 9300 Mean reward: 220.7000 Prob random: 0.9963, Loss: 0.8824\n",
      "Num episode: 9400 Mean reward: 212.9000 Prob random: 0.9962, Loss: 0.9074\n",
      "Num episode: 9500 Mean reward: 210.8000 Prob random: 0.9962, Loss: 0.8038\n",
      "Num episode: 9600 Mean reward: 213.5000 Prob random: 0.9961, Loss: 0.8443\n",
      "Num episode: 9700 Mean reward: 208.8000 Prob random: 0.9961, Loss: 1.0076\n",
      "Num episode: 9800 Mean reward: 230.3000 Prob random: 0.9960, Loss: 1.0166\n",
      "Num episode: 9900 Mean reward: 197.9000 Prob random: 0.9960, Loss: 1.1061\n",
      "Num episode: 10000 Mean reward: 215.0000 Prob random: 0.9960, Loss: 1.0158\n",
      "Num episode: 10100 Mean reward: 199.6000 Prob random: 0.9959, Loss: 0.8325\n",
      "Num episode: 10200 Mean reward: 220.3000 Prob random: 0.9959, Loss: 0.8563\n",
      "Num episode: 10300 Mean reward: 209.4000 Prob random: 0.9958, Loss: 1.0099\n",
      "Num episode: 10400 Mean reward: 215.9000 Prob random: 0.9958, Loss: 1.1462\n",
      "Num episode: 10500 Mean reward: 213.6000 Prob random: 0.9957, Loss: 1.1161\n",
      "Num episode: 10600 Mean reward: 205.2000 Prob random: 0.9957, Loss: 1.1535\n",
      "Num episode: 10700 Mean reward: 200.3000 Prob random: 0.9956, Loss: 0.9496\n",
      "Num episode: 10800 Mean reward: 207.5000 Prob random: 0.9956, Loss: 0.8133\n",
      "Num episode: 10900 Mean reward: 200.2000 Prob random: 0.9955, Loss: 0.8967\n",
      "Num episode: 11000 Mean reward: 206.6000 Prob random: 0.9955, Loss: 0.8812\n",
      "Num episode: 11100 Mean reward: 221.1000 Prob random: 0.9955, Loss: 1.0666\n",
      "Num episode: 11200 Mean reward: 220.4000 Prob random: 0.9954, Loss: 1.1242\n",
      "Num episode: 11300 Mean reward: 199.0000 Prob random: 0.9954, Loss: 1.1117\n",
      "Num episode: 11400 Mean reward: 213.4000 Prob random: 0.9953, Loss: 1.0667\n",
      "Num episode: 11500 Mean reward: 204.0000 Prob random: 0.9953, Loss: 0.8200\n",
      "Num episode: 11600 Mean reward: 224.8000 Prob random: 0.9952, Loss: 0.8387\n",
      "Num episode: 11700 Mean reward: 216.6000 Prob random: 0.9952, Loss: 0.7837\n",
      "Num episode: 11800 Mean reward: 220.3000 Prob random: 0.9951, Loss: 0.7741\n",
      "Num episode: 11900 Mean reward: 194.8000 Prob random: 0.9951, Loss: 1.6690\n",
      "Num episode: 12000 Mean reward: 198.6000 Prob random: 0.9951, Loss: 1.6226\n",
      "Num episode: 12100 Mean reward: 213.7000 Prob random: 0.9950, Loss: 1.8153\n",
      "Num episode: 12200 Mean reward: 206.8000 Prob random: 0.9950, Loss: 3.5694\n",
      "Num episode: 12300 Mean reward: 214.3000 Prob random: 0.9949, Loss: 2.7162\n",
      "Num episode: 12400 Mean reward: 211.0000 Prob random: 0.9949, Loss: 2.6890\n",
      "Num episode: 12500 Mean reward: 232.9000 Prob random: 0.9948, Loss: 3.2054\n",
      "Num episode: 12600 Mean reward: 231.9000 Prob random: 0.9948, Loss: 1.4375\n",
      "Num episode: 12700 Mean reward: 199.3000 Prob random: 0.9947, Loss: 1.4146\n",
      "Num episode: 12800 Mean reward: 226.2000 Prob random: 0.9947, Loss: 1.6943\n",
      "Num episode: 12900 Mean reward: 224.6000 Prob random: 0.9946, Loss: 0.9735\n",
      "Num episode: 13000 Mean reward: 205.1000 Prob random: 0.9946, Loss: 1.0239\n",
      "Num episode: 13100 Mean reward: 193.9000 Prob random: 0.9946, Loss: 1.0300\n",
      "Num episode: 13200 Mean reward: 212.2000 Prob random: 0.9945, Loss: 0.8444\n",
      "Num episode: 13300 Mean reward: 209.2000 Prob random: 0.9945, Loss: 0.8915\n",
      "Num episode: 13400 Mean reward: 201.2000 Prob random: 0.9944, Loss: 0.9095\n",
      "Num episode: 13500 Mean reward: 217.0000 Prob random: 0.9944, Loss: 0.9187\n",
      "Num episode: 13600 Mean reward: 198.5000 Prob random: 0.9943, Loss: 1.2021\n",
      "Num episode: 13700 Mean reward: 213.4000 Prob random: 0.9943, Loss: 1.1464\n",
      "Num episode: 13800 Mean reward: 217.7000 Prob random: 0.9942, Loss: 1.0546\n",
      "Num episode: 13900 Mean reward: 213.9000 Prob random: 0.9942, Loss: 1.0480\n",
      "Num episode: 14000 Mean reward: 208.9000 Prob random: 0.9942, Loss: 0.7348\n",
      "Num episode: 14100 Mean reward: 206.7000 Prob random: 0.9941, Loss: 0.7461\n",
      "Num episode: 14200 Mean reward: 219.6000 Prob random: 0.9941, Loss: 0.8962\n",
      "Num episode: 14300 Mean reward: 197.7000 Prob random: 0.9940, Loss: 0.9155\n",
      "Num episode: 14400 Mean reward: 226.6000 Prob random: 0.9940, Loss: 0.8648\n",
      "Num episode: 14500 Mean reward: 214.6000 Prob random: 0.9939, Loss: 0.8733\n",
      "Num episode: 14600 Mean reward: 210.2000 Prob random: 0.9939, Loss: 0.7996\n",
      "Num episode: 14700 Mean reward: 206.3000 Prob random: 0.9938, Loss: 0.7957\n",
      "Num episode: 14800 Mean reward: 218.2000 Prob random: 0.9938, Loss: 0.8115\n",
      "Num episode: 14900 Mean reward: 217.1000 Prob random: 0.9937, Loss: 0.8785\n",
      "Num episode: 15000 Mean reward: 211.7000 Prob random: 0.9937, Loss: 0.9354\n",
      "Num episode: 15100 Mean reward: 203.6000 Prob random: 0.9937, Loss: 0.9310\n",
      "Num episode: 15200 Mean reward: 207.9000 Prob random: 0.9936, Loss: 0.9150\n",
      "Num episode: 15300 Mean reward: 195.5000 Prob random: 0.9936, Loss: 0.8790\n",
      "Num episode: 15400 Mean reward: 234.8000 Prob random: 0.9935, Loss: 0.7831\n",
      "Num episode: 15500 Mean reward: 206.5000 Prob random: 0.9935, Loss: 0.8286\n",
      "Num episode: 15600 Mean reward: 207.4000 Prob random: 0.9934, Loss: 0.9257\n",
      "Num episode: 15700 Mean reward: 229.2000 Prob random: 0.9934, Loss: 0.9157\n",
      "Num episode: 15800 Mean reward: 219.8000 Prob random: 0.9933, Loss: 0.8949\n",
      "Num episode: 15900 Mean reward: 208.9000 Prob random: 0.9933, Loss: 0.8306\n",
      "Num episode: 16000 Mean reward: 218.5000 Prob random: 0.9933, Loss: 0.7804\n",
      "Num episode: 16100 Mean reward: 233.6000 Prob random: 0.9932, Loss: 0.8370\n",
      "Num episode: 16200 Mean reward: 206.8000 Prob random: 0.9932, Loss: 0.8823\n",
      "Num episode: 16300 Mean reward: 204.6000 Prob random: 0.9931, Loss: 1.0657\n",
      "Num episode: 16400 Mean reward: 232.4000 Prob random: 0.9931, Loss: 1.0095\n",
      "Num episode: 16500 Mean reward: 208.0000 Prob random: 0.9930, Loss: 0.9841\n",
      "Num episode: 16600 Mean reward: 212.2000 Prob random: 0.9930, Loss: 1.2316\n",
      "Num episode: 16700 Mean reward: 206.8000 Prob random: 0.9929, Loss: 1.1377\n",
      "Num episode: 16800 Mean reward: 221.9000 Prob random: 0.9929, Loss: 1.1866\n",
      "Num episode: 16900 Mean reward: 207.0000 Prob random: 0.9928, Loss: 1.1020\n",
      "Num episode: 17000 Mean reward: 200.4000 Prob random: 0.9928, Loss: 0.9653\n",
      "Num episode: 17100 Mean reward: 222.5000 Prob random: 0.9928, Loss: 0.8854\n",
      "Num episode: 17200 Mean reward: 231.6000 Prob random: 0.9927, Loss: 1.1761\n",
      "Num episode: 17300 Mean reward: 204.8000 Prob random: 0.9927, Loss: 1.3269\n",
      "Num episode: 17400 Mean reward: 196.0000 Prob random: 0.9926, Loss: 1.3282\n",
      "Num episode: 17500 Mean reward: 229.4000 Prob random: 0.9926, Loss: 1.5918\n",
      "Num episode: 17600 Mean reward: 215.0000 Prob random: 0.9925, Loss: 1.3240\n",
      "Num episode: 17700 Mean reward: 210.6000 Prob random: 0.9925, Loss: 1.3557\n",
      "Num episode: 17800 Mean reward: 224.7000 Prob random: 0.9924, Loss: 1.2315\n",
      "Num episode: 17900 Mean reward: 209.1000 Prob random: 0.9924, Loss: 1.1001\n",
      "Num episode: 18000 Mean reward: 208.3000 Prob random: 0.9924, Loss: 1.0667\n",
      "Num episode: 18100 Mean reward: 196.7000 Prob random: 0.9923, Loss: 0.9793\n",
      "Num episode: 18200 Mean reward: 208.2000 Prob random: 0.9923, Loss: 1.1297\n",
      "Num episode: 18300 Mean reward: 215.9000 Prob random: 0.9922, Loss: 0.9716\n",
      "Num episode: 18400 Mean reward: 215.9000 Prob random: 0.9922, Loss: 1.1209\n",
      "Num episode: 18500 Mean reward: 211.6000 Prob random: 0.9921, Loss: 1.2758\n",
      "Num episode: 18600 Mean reward: 213.4000 Prob random: 0.9921, Loss: 1.1255\n",
      "Num episode: 18700 Mean reward: 192.8000 Prob random: 0.9920, Loss: 1.1679\n",
      "Num episode: 18800 Mean reward: 204.7000 Prob random: 0.9920, Loss: 1.0387\n",
      "Num episode: 18900 Mean reward: 216.4000 Prob random: 0.9919, Loss: 0.8197\n",
      "Num episode: 19000 Mean reward: 223.9000 Prob random: 0.9919, Loss: 0.9230\n",
      "Num episode: 19100 Mean reward: 214.7000 Prob random: 0.9919, Loss: 1.1817\n",
      "Num episode: 19200 Mean reward: 206.3000 Prob random: 0.9918, Loss: 1.2586\n",
      "Num episode: 19300 Mean reward: 208.8000 Prob random: 0.9918, Loss: 2.5584\n",
      "Num episode: 19400 Mean reward: 210.6000 Prob random: 0.9917, Loss: 2.4329\n",
      "Num episode: 19500 Mean reward: 206.1000 Prob random: 0.9917, Loss: 2.2334\n",
      "Num episode: 19600 Mean reward: 218.8000 Prob random: 0.9916, Loss: 2.2299\n",
      "Num episode: 19700 Mean reward: 209.9000 Prob random: 0.9916, Loss: 1.1794\n",
      "Num episode: 19800 Mean reward: 211.8000 Prob random: 0.9915, Loss: 1.2182\n",
      "Num episode: 19900 Mean reward: 219.9000 Prob random: 0.9915, Loss: 1.1983\n",
      "Num episode: 20000 Mean reward: 241.3000 Prob random: 0.9915, Loss: 1.1677\n",
      "Num episode: 20100 Mean reward: 199.6000 Prob random: 0.9914, Loss: 0.9346\n",
      "Num episode: 20200 Mean reward: 201.0000 Prob random: 0.9914, Loss: 1.0253\n",
      "Num episode: 20300 Mean reward: 213.1000 Prob random: 0.9913, Loss: 1.0442\n",
      "Num episode: 20400 Mean reward: 215.4000 Prob random: 0.9913, Loss: 0.9506\n",
      "Num episode: 20500 Mean reward: 212.0000 Prob random: 0.9912, Loss: 1.0721\n",
      "Num episode: 20600 Mean reward: 219.8000 Prob random: 0.9912, Loss: 0.9925\n",
      "Num episode: 20700 Mean reward: 196.4000 Prob random: 0.9911, Loss: 1.2831\n",
      "Num episode: 20800 Mean reward: 222.8000 Prob random: 0.9911, Loss: 1.2885\n",
      "Num episode: 20900 Mean reward: 222.0000 Prob random: 0.9910, Loss: 1.1417\n",
      "Num episode: 21000 Mean reward: 193.2000 Prob random: 0.9910, Loss: 1.0832\n",
      "Num episode: 21100 Mean reward: 239.3000 Prob random: 0.9910, Loss: 0.9383\n",
      "Num episode: 21200 Mean reward: 194.9000 Prob random: 0.9909, Loss: 1.0961\n",
      "Num episode: 21300 Mean reward: 207.0000 Prob random: 0.9909, Loss: 1.1294\n",
      "Num episode: 21400 Mean reward: 209.1000 Prob random: 0.9908, Loss: 1.3160\n",
      "Num episode: 21500 Mean reward: 207.1000 Prob random: 0.9908, Loss: 1.5050\n",
      "Num episode: 21600 Mean reward: 218.4000 Prob random: 0.9907, Loss: 1.3820\n",
      "Num episode: 21700 Mean reward: 209.7000 Prob random: 0.9907, Loss: 1.3586\n",
      "Num episode: 21800 Mean reward: 217.2000 Prob random: 0.9906, Loss: 1.2158\n",
      "Num episode: 21900 Mean reward: 224.3000 Prob random: 0.9906, Loss: 0.9463\n",
      "Num episode: 22000 Mean reward: 219.0000 Prob random: 0.9906, Loss: 0.9646\n",
      "Num episode: 22100 Mean reward: 202.6000 Prob random: 0.9905, Loss: 0.9878\n",
      "Num episode: 22200 Mean reward: 201.7000 Prob random: 0.9905, Loss: 1.0646\n",
      "Num episode: 22300 Mean reward: 225.8000 Prob random: 0.9904, Loss: 0.9510\n",
      "Num episode: 22400 Mean reward: 214.4000 Prob random: 0.9904, Loss: 1.1125\n",
      "Num episode: 22500 Mean reward: 209.8000 Prob random: 0.9903, Loss: 1.1875\n",
      "Num episode: 22600 Mean reward: 239.8000 Prob random: 0.9903, Loss: 1.3173\n",
      "Num episode: 22700 Mean reward: 217.1000 Prob random: 0.9902, Loss: 1.3101\n",
      "Num episode: 22800 Mean reward: 203.0000 Prob random: 0.9902, Loss: 1.2370\n",
      "Num episode: 22900 Mean reward: 196.1000 Prob random: 0.9901, Loss: 1.2705\n",
      "Num episode: 23000 Mean reward: 212.2000 Prob random: 0.9901, Loss: 1.0573\n",
      "Num episode: 23100 Mean reward: 212.0000 Prob random: 0.9901, Loss: 1.0386\n",
      "Num episode: 23200 Mean reward: 216.5000 Prob random: 0.9900, Loss: 0.9024\n",
      "Num episode: 23300 Mean reward: 209.2000 Prob random: 0.9900, Loss: 0.8054\n",
      "Num episode: 23400 Mean reward: 208.0000 Prob random: 0.9899, Loss: 0.8234\n",
      "Num episode: 23500 Mean reward: 226.3000 Prob random: 0.9899, Loss: 0.9859\n",
      "Num episode: 23600 Mean reward: 218.9000 Prob random: 0.9898, Loss: 1.0443\n",
      "Num episode: 23700 Mean reward: 203.1000 Prob random: 0.9898, Loss: 1.0531\n",
      "Num episode: 23800 Mean reward: 215.1000 Prob random: 0.9897, Loss: 1.0572\n",
      "Num episode: 23900 Mean reward: 207.2000 Prob random: 0.9897, Loss: 0.9084\n",
      "Num episode: 24000 Mean reward: 228.1000 Prob random: 0.9897, Loss: 0.8766\n",
      "Num episode: 24100 Mean reward: 215.9000 Prob random: 0.9896, Loss: 1.1038\n",
      "Num episode: 24200 Mean reward: 201.7000 Prob random: 0.9896, Loss: 1.0664\n",
      "Num episode: 24300 Mean reward: 217.2000 Prob random: 0.9895, Loss: 1.0706\n",
      "Num episode: 24400 Mean reward: 237.9000 Prob random: 0.9895, Loss: 1.0513\n",
      "Num episode: 24500 Mean reward: 210.7000 Prob random: 0.9894, Loss: 0.8280\n",
      "Num episode: 24600 Mean reward: 213.9000 Prob random: 0.9894, Loss: 0.8756\n",
      "Num episode: 24700 Mean reward: 206.1000 Prob random: 0.9893, Loss: 1.0354\n",
      "Num episode: 24800 Mean reward: 231.8000 Prob random: 0.9893, Loss: 1.0950\n",
      "Num episode: 24900 Mean reward: 197.9000 Prob random: 0.9892, Loss: 1.0773\n",
      "Num episode: 25000 Mean reward: 201.2000 Prob random: 0.9892, Loss: 1.0907\n",
      "Num episode: 25100 Mean reward: 216.1000 Prob random: 0.9892, Loss: 1.2646\n",
      "Num episode: 25200 Mean reward: 204.4000 Prob random: 0.9891, Loss: 1.1938\n",
      "Num episode: 25300 Mean reward: 212.4000 Prob random: 0.9891, Loss: 1.2154\n",
      "Num episode: 25400 Mean reward: 210.8000 Prob random: 0.9890, Loss: 1.1777\n",
      "Num episode: 25500 Mean reward: 222.3000 Prob random: 0.9890, Loss: 0.8613\n",
      "Num episode: 25600 Mean reward: 223.3000 Prob random: 0.9889, Loss: 0.8645\n",
      "Num episode: 25700 Mean reward: 211.2000 Prob random: 0.9889, Loss: 0.9021\n",
      "Num episode: 25800 Mean reward: 220.0000 Prob random: 0.9888, Loss: 0.9705\n",
      "Num episode: 25900 Mean reward: 209.4000 Prob random: 0.9888, Loss: 1.0310\n",
      "Num episode: 26000 Mean reward: 213.1000 Prob random: 0.9888, Loss: 1.4902\n",
      "Num episode: 26100 Mean reward: 215.0000 Prob random: 0.9887, Loss: 1.3893\n",
      "Num episode: 26200 Mean reward: 204.7000 Prob random: 0.9887, Loss: 1.3281\n",
      "Num episode: 26300 Mean reward: 215.9000 Prob random: 0.9886, Loss: 1.2179\n",
      "Num episode: 26400 Mean reward: 207.6000 Prob random: 0.9886, Loss: 0.9986\n",
      "Num episode: 26500 Mean reward: 212.2000 Prob random: 0.9885, Loss: nan\n",
      "Num episode: 26600 Mean reward: 212.0000 Prob random: 0.9885, Loss: nan\n",
      "Num episode: 26700 Mean reward: 199.2000 Prob random: 0.9884, Loss: nan\n",
      "Num episode: 26800 Mean reward: 225.3000 Prob random: 0.9884, Loss: nan\n",
      "Num episode: 26900 Mean reward: 206.4000 Prob random: 0.9883, Loss: nan\n",
      "Num episode: 27000 Mean reward: 203.2000 Prob random: 0.9883, Loss: nan\n",
      "Num episode: 27100 Mean reward: 211.1000 Prob random: 0.9883, Loss: nan\n",
      "Num episode: 27200 Mean reward: 203.8000 Prob random: 0.9882, Loss: nan\n",
      "Num episode: 27300 Mean reward: 202.9000 Prob random: 0.9882, Loss: nan\n",
      "Num episode: 27400 Mean reward: 211.1000 Prob random: 0.9881, Loss: nan\n",
      "Num episode: 27500 Mean reward: 202.8000 Prob random: 0.9881, Loss: nan\n",
      "Num episode: 27600 Mean reward: 230.4000 Prob random: 0.9880, Loss: nan\n",
      "Num episode: 27700 Mean reward: 227.7000 Prob random: 0.9880, Loss: nan\n",
      "Num episode: 27800 Mean reward: 205.3000 Prob random: 0.9879, Loss: nan\n",
      "Num episode: 27900 Mean reward: 205.1000 Prob random: 0.9879, Loss: nan\n",
      "Num episode: 28000 Mean reward: 206.0000 Prob random: 0.9879, Loss: nan\n",
      "Num episode: 28100 Mean reward: 207.9000 Prob random: 0.9878, Loss: nan\n",
      "Num episode: 28200 Mean reward: 206.0000 Prob random: 0.9878, Loss: nan\n",
      "Num episode: 28300 Mean reward: 221.0000 Prob random: 0.9877, Loss: nan\n",
      "Num episode: 28400 Mean reward: 229.3000 Prob random: 0.9877, Loss: nan\n",
      "Num episode: 28500 Mean reward: 207.2000 Prob random: 0.9876, Loss: nan\n",
      "Num episode: 28600 Mean reward: 203.2000 Prob random: 0.9876, Loss: nan\n",
      "Num episode: 28700 Mean reward: 202.2000 Prob random: 0.9875, Loss: nan\n",
      "Num episode: 28800 Mean reward: 214.2000 Prob random: 0.9875, Loss: nan\n",
      "Num episode: 28900 Mean reward: 211.7000 Prob random: 0.9874, Loss: nan\n",
      "Num episode: 29000 Mean reward: 200.9000 Prob random: 0.9874, Loss: nan\n",
      "Num episode: 29100 Mean reward: 233.0000 Prob random: 0.9874, Loss: nan\n",
      "Num episode: 29200 Mean reward: 220.4000 Prob random: 0.9873, Loss: nan\n",
      "Num episode: 29300 Mean reward: 203.6000 Prob random: 0.9873, Loss: nan\n",
      "Num episode: 29400 Mean reward: 213.1000 Prob random: 0.9872, Loss: nan\n",
      "Num episode: 29500 Mean reward: 202.1000 Prob random: 0.9872, Loss: nan\n",
      "Num episode: 29600 Mean reward: 211.7000 Prob random: 0.9871, Loss: nan\n",
      "Num episode: 29700 Mean reward: 210.1000 Prob random: 0.9871, Loss: nan\n",
      "Num episode: 29800 Mean reward: 223.0000 Prob random: 0.9870, Loss: nan\n",
      "Num episode: 29900 Mean reward: 217.9000 Prob random: 0.9870, Loss: nan\n",
      "Num episode: 30000 Mean reward: 199.3000 Prob random: 0.9870, Loss: nan\n",
      "Num episode: 30100 Mean reward: 208.7000 Prob random: 0.9869, Loss: nan\n",
      "Num episode: 30200 Mean reward: 211.4000 Prob random: 0.9869, Loss: nan\n",
      "Num episode: 30300 Mean reward: 216.1000 Prob random: 0.9868, Loss: nan\n",
      "Num episode: 30400 Mean reward: 204.2000 Prob random: 0.9868, Loss: nan\n",
      "Num episode: 30500 Mean reward: 195.0000 Prob random: 0.9867, Loss: nan\n",
      "Num episode: 30600 Mean reward: 208.5000 Prob random: 0.9867, Loss: nan\n",
      "Num episode: 30700 Mean reward: 205.1000 Prob random: 0.9866, Loss: nan\n",
      "Num episode: 30800 Mean reward: 207.9000 Prob random: 0.9866, Loss: nan\n",
      "Num episode: 30900 Mean reward: 198.1000 Prob random: 0.9865, Loss: nan\n",
      "Num episode: 31000 Mean reward: 202.5000 Prob random: 0.9865, Loss: nan\n",
      "Num episode: 31100 Mean reward: 207.1000 Prob random: 0.9865, Loss: nan\n",
      "Num episode: 31200 Mean reward: 214.6000 Prob random: 0.9864, Loss: nan\n",
      "Num episode: 31300 Mean reward: 210.8000 Prob random: 0.9864, Loss: nan\n",
      "Num episode: 31400 Mean reward: 209.0000 Prob random: 0.9863, Loss: nan\n",
      "Num episode: 31500 Mean reward: 208.4000 Prob random: 0.9863, Loss: nan\n",
      "Num episode: 31600 Mean reward: 211.8000 Prob random: 0.9862, Loss: nan\n",
      "Num episode: 31700 Mean reward: 230.2000 Prob random: 0.9862, Loss: nan\n",
      "Num episode: 31800 Mean reward: 214.1000 Prob random: 0.9861, Loss: nan\n",
      "Num episode: 31900 Mean reward: 201.9000 Prob random: 0.9861, Loss: nan\n",
      "Num episode: 32000 Mean reward: 223.5000 Prob random: 0.9861, Loss: nan\n",
      "Num episode: 32100 Mean reward: 212.9000 Prob random: 0.9860, Loss: nan\n",
      "Num episode: 32200 Mean reward: 205.3000 Prob random: 0.9860, Loss: nan\n",
      "Num episode: 32300 Mean reward: 220.3000 Prob random: 0.9859, Loss: nan\n",
      "Num episode: 32400 Mean reward: 209.4000 Prob random: 0.9859, Loss: nan\n",
      "Num episode: 32500 Mean reward: 211.1000 Prob random: 0.9858, Loss: nan\n",
      "Num episode: 32600 Mean reward: 196.0000 Prob random: 0.9858, Loss: nan\n",
      "Num episode: 32700 Mean reward: 202.8000 Prob random: 0.9857, Loss: nan\n",
      "Num episode: 32800 Mean reward: 206.5000 Prob random: 0.9857, Loss: nan\n",
      "Num episode: 32900 Mean reward: 202.0000 Prob random: 0.9856, Loss: nan\n",
      "Num episode: 33000 Mean reward: 225.5000 Prob random: 0.9856, Loss: nan\n",
      "Num episode: 33100 Mean reward: 202.1000 Prob random: 0.9856, Loss: nan\n",
      "Num episode: 33200 Mean reward: 205.4000 Prob random: 0.9855, Loss: nan\n",
      "Num episode: 33300 Mean reward: 217.7000 Prob random: 0.9855, Loss: nan\n",
      "Num episode: 33400 Mean reward: 225.5000 Prob random: 0.9854, Loss: nan\n",
      "Num episode: 33500 Mean reward: 208.7000 Prob random: 0.9854, Loss: nan\n",
      "Num episode: 33600 Mean reward: 209.8000 Prob random: 0.9853, Loss: nan\n",
      "Num episode: 33700 Mean reward: 239.8000 Prob random: 0.9853, Loss: nan\n",
      "Num episode: 33800 Mean reward: 223.9000 Prob random: 0.9852, Loss: nan\n",
      "Num episode: 33900 Mean reward: 211.0000 Prob random: 0.9852, Loss: nan\n",
      "Num episode: 34000 Mean reward: 219.1000 Prob random: 0.9852, Loss: nan\n",
      "Num episode: 34100 Mean reward: 216.8000 Prob random: 0.9851, Loss: nan\n",
      "Num episode: 34200 Mean reward: 199.7000 Prob random: 0.9851, Loss: nan\n",
      "Num episode: 34300 Mean reward: 205.5000 Prob random: 0.9850, Loss: nan\n",
      "Num episode: 34400 Mean reward: 223.5000 Prob random: 0.9850, Loss: nan\n",
      "Num episode: 34500 Mean reward: 221.8000 Prob random: 0.9849, Loss: nan\n",
      "Num episode: 34600 Mean reward: 204.2000 Prob random: 0.9849, Loss: nan\n",
      "Num episode: 34700 Mean reward: 209.2000 Prob random: 0.9848, Loss: nan\n",
      "Num episode: 34800 Mean reward: 209.6000 Prob random: 0.9848, Loss: nan\n",
      "Num episode: 34900 Mean reward: 228.7000 Prob random: 0.9847, Loss: nan\n",
      "Num episode: 35000 Mean reward: 215.7000 Prob random: 0.9847, Loss: nan\n",
      "Num episode: 35100 Mean reward: 204.6000 Prob random: 0.9847, Loss: nan\n",
      "Num episode: 35200 Mean reward: 201.4000 Prob random: 0.9846, Loss: nan\n",
      "Num episode: 35300 Mean reward: 213.3000 Prob random: 0.9846, Loss: nan\n",
      "Num episode: 35400 Mean reward: 217.3000 Prob random: 0.9845, Loss: nan\n",
      "Num episode: 35500 Mean reward: 208.9000 Prob random: 0.9845, Loss: nan\n",
      "Num episode: 35600 Mean reward: 225.4000 Prob random: 0.9844, Loss: nan\n",
      "Num episode: 35700 Mean reward: 232.4000 Prob random: 0.9844, Loss: nan\n",
      "Num episode: 35800 Mean reward: 213.2000 Prob random: 0.9843, Loss: nan\n",
      "Num episode: 35900 Mean reward: 214.5000 Prob random: 0.9843, Loss: nan\n",
      "Num episode: 36000 Mean reward: 217.1000 Prob random: 0.9843, Loss: nan\n",
      "Num episode: 36100 Mean reward: 192.0000 Prob random: 0.9842, Loss: nan\n",
      "Num episode: 36200 Mean reward: 212.1000 Prob random: 0.9842, Loss: nan\n",
      "Num episode: 36300 Mean reward: 218.9000 Prob random: 0.9841, Loss: nan\n",
      "Num episode: 36400 Mean reward: 199.7000 Prob random: 0.9841, Loss: nan\n",
      "Num episode: 36500 Mean reward: 210.1000 Prob random: 0.9840, Loss: nan\n",
      "Num episode: 36600 Mean reward: 226.3000 Prob random: 0.9840, Loss: nan\n",
      "Num episode: 36700 Mean reward: 208.4000 Prob random: 0.9839, Loss: nan\n",
      "Num episode: 36800 Mean reward: 202.7000 Prob random: 0.9839, Loss: nan\n",
      "Num episode: 36900 Mean reward: 206.0000 Prob random: 0.9838, Loss: nan\n",
      "Num episode: 37000 Mean reward: 210.6000 Prob random: 0.9838, Loss: nan\n",
      "Num episode: 37100 Mean reward: 218.5000 Prob random: 0.9838, Loss: nan\n",
      "Num episode: 37200 Mean reward: 203.4000 Prob random: 0.9837, Loss: nan\n",
      "Num episode: 37300 Mean reward: 204.0000 Prob random: 0.9837, Loss: nan\n",
      "Num episode: 37400 Mean reward: 237.4000 Prob random: 0.9836, Loss: nan\n",
      "Num episode: 37500 Mean reward: 212.9000 Prob random: 0.9836, Loss: nan\n",
      "Num episode: 37600 Mean reward: 201.7000 Prob random: 0.9835, Loss: nan\n",
      "Num episode: 37700 Mean reward: 213.9000 Prob random: 0.9835, Loss: nan\n",
      "Num episode: 37800 Mean reward: 222.3000 Prob random: 0.9834, Loss: nan\n",
      "Num episode: 37900 Mean reward: 207.6000 Prob random: 0.9834, Loss: nan\n",
      "Num episode: 38000 Mean reward: 208.0000 Prob random: 0.9834, Loss: nan\n",
      "Num episode: 38100 Mean reward: 196.3000 Prob random: 0.9833, Loss: nan\n",
      "Num episode: 38200 Mean reward: 225.7000 Prob random: 0.9833, Loss: nan\n",
      "Num episode: 38300 Mean reward: 227.8000 Prob random: 0.9832, Loss: nan\n",
      "Num episode: 38400 Mean reward: 201.7000 Prob random: 0.9832, Loss: nan\n",
      "Num episode: 38500 Mean reward: 230.7000 Prob random: 0.9831, Loss: nan\n",
      "Num episode: 38600 Mean reward: 206.1000 Prob random: 0.9831, Loss: nan\n",
      "Num episode: 38700 Mean reward: 203.3000 Prob random: 0.9830, Loss: nan\n",
      "Num episode: 38800 Mean reward: 204.9000 Prob random: 0.9830, Loss: nan\n",
      "Num episode: 38900 Mean reward: 218.4000 Prob random: 0.9829, Loss: nan\n",
      "Num episode: 39000 Mean reward: 205.8000 Prob random: 0.9829, Loss: nan\n",
      "Num episode: 39100 Mean reward: 214.8000 Prob random: 0.9829, Loss: nan\n",
      "Num episode: 39200 Mean reward: 215.8000 Prob random: 0.9828, Loss: nan\n",
      "Num episode: 39300 Mean reward: 196.0000 Prob random: 0.9828, Loss: nan\n",
      "Num episode: 39400 Mean reward: 218.5000 Prob random: 0.9827, Loss: nan\n",
      "Num episode: 39500 Mean reward: 213.0000 Prob random: 0.9827, Loss: nan\n",
      "Num episode: 39600 Mean reward: 191.3000 Prob random: 0.9826, Loss: nan\n",
      "Num episode: 39700 Mean reward: 216.5000 Prob random: 0.9826, Loss: nan\n",
      "Num episode: 39800 Mean reward: 192.2000 Prob random: 0.9825, Loss: nan\n",
      "Num episode: 39900 Mean reward: 217.2000 Prob random: 0.9825, Loss: nan\n",
      "Num episode: 40000 Mean reward: 207.2000 Prob random: 0.9825, Loss: nan\n",
      "Num episode: 40100 Mean reward: 201.6000 Prob random: 0.9824, Loss: nan\n",
      "Num episode: 40200 Mean reward: 198.9000 Prob random: 0.9824, Loss: nan\n",
      "Num episode: 40300 Mean reward: 206.8000 Prob random: 0.9823, Loss: nan\n",
      "Num episode: 40400 Mean reward: 227.5000 Prob random: 0.9823, Loss: nan\n",
      "Num episode: 40500 Mean reward: 205.5000 Prob random: 0.9822, Loss: nan\n",
      "Num episode: 40600 Mean reward: 237.8000 Prob random: 0.9822, Loss: nan\n",
      "Num episode: 40700 Mean reward: 213.9000 Prob random: 0.9821, Loss: nan\n",
      "Num episode: 40800 Mean reward: 207.8000 Prob random: 0.9821, Loss: nan\n",
      "Num episode: 40900 Mean reward: 207.2000 Prob random: 0.9820, Loss: nan\n",
      "Num episode: 41000 Mean reward: 211.9000 Prob random: 0.9820, Loss: nan\n",
      "Num episode: 41100 Mean reward: 201.7000 Prob random: 0.9820, Loss: nan\n",
      "Num episode: 41200 Mean reward: 216.1000 Prob random: 0.9819, Loss: nan\n",
      "Num episode: 41300 Mean reward: 215.0000 Prob random: 0.9819, Loss: nan\n",
      "Num episode: 41400 Mean reward: 205.6000 Prob random: 0.9818, Loss: nan\n",
      "Num episode: 41500 Mean reward: 227.4000 Prob random: 0.9818, Loss: nan\n",
      "Num episode: 41600 Mean reward: 210.9000 Prob random: 0.9817, Loss: nan\n",
      "Num episode: 41700 Mean reward: 206.2000 Prob random: 0.9817, Loss: nan\n",
      "Num episode: 41800 Mean reward: 209.0000 Prob random: 0.9816, Loss: nan\n",
      "Num episode: 41900 Mean reward: 215.0000 Prob random: 0.9816, Loss: nan\n",
      "Num episode: 42000 Mean reward: 203.3000 Prob random: 0.9816, Loss: nan\n",
      "Num episode: 42100 Mean reward: 223.2000 Prob random: 0.9815, Loss: nan\n",
      "Num episode: 42200 Mean reward: 219.4000 Prob random: 0.9815, Loss: nan\n",
      "Num episode: 42300 Mean reward: 216.7000 Prob random: 0.9814, Loss: nan\n",
      "Num episode: 42400 Mean reward: 208.5000 Prob random: 0.9814, Loss: nan\n",
      "Num episode: 42500 Mean reward: 198.4000 Prob random: 0.9813, Loss: nan\n",
      "Num episode: 42600 Mean reward: 228.0000 Prob random: 0.9813, Loss: nan\n",
      "Num episode: 42700 Mean reward: 215.6000 Prob random: 0.9812, Loss: nan\n",
      "Num episode: 42800 Mean reward: 205.0000 Prob random: 0.9812, Loss: nan\n",
      "Num episode: 42900 Mean reward: 218.5000 Prob random: 0.9811, Loss: nan\n",
      "Num episode: 43000 Mean reward: 222.4000 Prob random: 0.9811, Loss: nan\n",
      "Num episode: 43100 Mean reward: 213.3000 Prob random: 0.9811, Loss: nan\n",
      "Num episode: 43200 Mean reward: 215.1000 Prob random: 0.9810, Loss: nan\n",
      "Num episode: 43300 Mean reward: 212.3000 Prob random: 0.9810, Loss: nan\n",
      "Num episode: 43400 Mean reward: 216.9000 Prob random: 0.9809, Loss: nan\n",
      "Num episode: 43500 Mean reward: 213.7000 Prob random: 0.9809, Loss: nan\n",
      "Num episode: 43600 Mean reward: 219.0000 Prob random: 0.9808, Loss: nan\n",
      "Num episode: 43700 Mean reward: 207.8000 Prob random: 0.9808, Loss: nan\n",
      "Num episode: 43800 Mean reward: 207.9000 Prob random: 0.9807, Loss: nan\n",
      "Num episode: 43900 Mean reward: 224.3000 Prob random: 0.9807, Loss: nan\n",
      "Num episode: 44000 Mean reward: 223.3000 Prob random: 0.9807, Loss: nan\n",
      "Num episode: 44100 Mean reward: 220.5000 Prob random: 0.9806, Loss: nan\n",
      "Num episode: 44200 Mean reward: 218.6000 Prob random: 0.9806, Loss: nan\n",
      "Num episode: 44300 Mean reward: 213.8000 Prob random: 0.9805, Loss: nan\n",
      "Num episode: 44400 Mean reward: 232.1000 Prob random: 0.9805, Loss: nan\n",
      "Num episode: 44500 Mean reward: 218.2000 Prob random: 0.9804, Loss: nan\n",
      "Num episode: 44600 Mean reward: 210.4000 Prob random: 0.9804, Loss: nan\n",
      "Num episode: 44700 Mean reward: 192.6000 Prob random: 0.9803, Loss: nan\n",
      "Num episode: 44800 Mean reward: 236.8000 Prob random: 0.9803, Loss: nan\n",
      "Num episode: 44900 Mean reward: 215.4000 Prob random: 0.9802, Loss: nan\n",
      "Num episode: 45000 Mean reward: 232.1000 Prob random: 0.9802, Loss: nan\n",
      "Num episode: 45100 Mean reward: 208.2000 Prob random: 0.9802, Loss: nan\n",
      "Num episode: 45200 Mean reward: 215.2000 Prob random: 0.9801, Loss: nan\n",
      "Num episode: 45300 Mean reward: 197.8000 Prob random: 0.9801, Loss: nan\n",
      "Num episode: 45400 Mean reward: 217.0000 Prob random: 0.9800, Loss: nan\n",
      "Num episode: 45500 Mean reward: 199.8000 Prob random: 0.9800, Loss: nan\n",
      "Num episode: 45600 Mean reward: 217.0000 Prob random: 0.9799, Loss: nan\n",
      "Num episode: 45700 Mean reward: 213.5000 Prob random: 0.9799, Loss: nan\n",
      "Num episode: 45800 Mean reward: 212.1000 Prob random: 0.9798, Loss: nan\n",
      "Num episode: 45900 Mean reward: 219.6000 Prob random: 0.9798, Loss: nan\n",
      "Num episode: 46000 Mean reward: 224.5000 Prob random: 0.9798, Loss: nan\n",
      "Num episode: 46100 Mean reward: 210.4000 Prob random: 0.9797, Loss: nan\n",
      "Num episode: 46200 Mean reward: 210.0000 Prob random: 0.9797, Loss: nan\n",
      "Num episode: 46300 Mean reward: 206.2000 Prob random: 0.9796, Loss: nan\n",
      "Num episode: 46400 Mean reward: 208.8000 Prob random: 0.9796, Loss: nan\n",
      "Num episode: 46500 Mean reward: 221.7000 Prob random: 0.9795, Loss: nan\n",
      "Num episode: 46600 Mean reward: 206.6000 Prob random: 0.9795, Loss: nan\n",
      "Num episode: 46700 Mean reward: 215.4000 Prob random: 0.9794, Loss: nan\n",
      "Num episode: 46800 Mean reward: 237.3000 Prob random: 0.9794, Loss: nan\n",
      "Num episode: 46900 Mean reward: 213.4000 Prob random: 0.9793, Loss: nan\n",
      "Num episode: 47000 Mean reward: 207.5000 Prob random: 0.9793, Loss: nan\n",
      "Num episode: 47100 Mean reward: 232.4000 Prob random: 0.9793, Loss: nan\n",
      "Num episode: 47200 Mean reward: 218.4000 Prob random: 0.9792, Loss: nan\n",
      "Num episode: 47300 Mean reward: 207.7000 Prob random: 0.9792, Loss: nan\n",
      "Num episode: 47400 Mean reward: 218.4000 Prob random: 0.9791, Loss: nan\n",
      "Num episode: 47500 Mean reward: 212.8000 Prob random: 0.9791, Loss: nan\n",
      "Num episode: 47600 Mean reward: 222.7000 Prob random: 0.9790, Loss: nan\n",
      "Num episode: 47700 Mean reward: 206.9000 Prob random: 0.9790, Loss: nan\n",
      "Num episode: 47800 Mean reward: 213.3000 Prob random: 0.9789, Loss: nan\n",
      "Num episode: 47900 Mean reward: 216.9000 Prob random: 0.9789, Loss: nan\n",
      "Num episode: 48000 Mean reward: 231.5000 Prob random: 0.9789, Loss: nan\n",
      "Num episode: 48100 Mean reward: 217.4000 Prob random: 0.9788, Loss: nan\n",
      "Num episode: 48200 Mean reward: 205.6000 Prob random: 0.9788, Loss: nan\n",
      "Num episode: 48300 Mean reward: 208.0000 Prob random: 0.9787, Loss: nan\n",
      "Num episode: 48400 Mean reward: 197.2000 Prob random: 0.9787, Loss: nan\n",
      "Num episode: 48500 Mean reward: 217.1000 Prob random: 0.9786, Loss: nan\n",
      "Num episode: 48600 Mean reward: 224.2000 Prob random: 0.9786, Loss: nan\n",
      "Num episode: 48700 Mean reward: 220.4000 Prob random: 0.9785, Loss: nan\n",
      "Num episode: 48800 Mean reward: 214.4000 Prob random: 0.9785, Loss: nan\n",
      "Num episode: 48900 Mean reward: 221.8000 Prob random: 0.9784, Loss: nan\n",
      "Num episode: 49000 Mean reward: 216.4000 Prob random: 0.9784, Loss: nan\n",
      "Num episode: 49100 Mean reward: 216.6000 Prob random: 0.9784, Loss: nan\n",
      "Num episode: 49200 Mean reward: 209.1000 Prob random: 0.9783, Loss: nan\n",
      "Num episode: 49300 Mean reward: 222.4000 Prob random: 0.9783, Loss: nan\n",
      "Num episode: 49400 Mean reward: 218.7000 Prob random: 0.9782, Loss: nan\n",
      "Num episode: 49500 Mean reward: 206.2000 Prob random: 0.9782, Loss: nan\n",
      "Num episode: 49600 Mean reward: 210.9000 Prob random: 0.9781, Loss: nan\n",
      "Num episode: 49700 Mean reward: 187.2000 Prob random: 0.9781, Loss: nan\n",
      "Num episode: 49800 Mean reward: 217.0000 Prob random: 0.9780, Loss: nan\n",
      "Num episode: 49900 Mean reward: 213.2000 Prob random: 0.9780, Loss: nan\n",
      "Num episode: 50000 Mean reward: 207.7000 Prob random: 0.9780, Loss: nan\n",
      "Num episode: 50100 Mean reward: 226.4000 Prob random: 0.9779, Loss: nan\n",
      "Num episode: 50200 Mean reward: 204.1000 Prob random: 0.9779, Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-08e18ac0fe9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Take the action and retrieve the next state, reward and done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/INSA/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/INSA/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"ale.lives\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/INSA/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_get_obs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/INSA/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_get_image\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/INSA/lib/python3.6/site-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mgetScreenRGB2\u001b[0;34m(self, screen_data)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mscreen_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m480\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ctypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Reset everything\n",
    "K.clear_session()\n",
    "\n",
    "# Setup our Q-networks\n",
    "online_q_values = q_network()\n",
    "target_q_values = q_network()\n",
    "\n",
    "\n",
    "# Make the networks equal\n",
    "update_target_graph(online_q_values, target_q_values, 1)\n",
    "\n",
    "# Setup our experience replay\n",
    "experience_replay = ExperienceReplay()\n",
    "\n",
    "# We'll begin by acting complete randomly. As we gain experience and improve,\n",
    "# we will begin reducing the probability of acting randomly, and instead\n",
    "# take the actions that our Q network suggests\n",
    "prob_random = prob_random_start\n",
    "prob_random_drop = (prob_random_start - prob_random_end) / annealing_steps\n",
    "\n",
    "num_steps = [] # Tracks number of steps per episode\n",
    "rewards = [] # Tracks rewards per episode\n",
    "total_steps = 0 # Tracks cumulative steps taken in training\n",
    "\n",
    "print_every = 100 # How often to print status\n",
    "save_every = 5 # How often to save\n",
    "\n",
    "losses = [0] # Tracking training losses\n",
    "\n",
    "num_episode = 0\n",
    "\n",
    "while num_episode < num_episodes:\n",
    "    # Create an experience replay for the current episode\n",
    "    episode_buffer = ExperienceReplay()\n",
    "    \n",
    "     # Get the game state from the environment\n",
    "        \n",
    "        \n",
    "    state = env.reset()\n",
    "    for skip in range(skip_start): # skip the start of each game\n",
    "        state, reward, done, info = env.step(0)\n",
    "    state = preprocess_observation(state)\n",
    "    \n",
    "    done = False # Game is complete\n",
    "    sum_rewards = 0 # Running sum of rewards in episode\n",
    "    cur_step = 0 # Running sum of number of steps taken in episode\n",
    "    \n",
    "    \n",
    "    while not(done):\n",
    "        cur_step += 1\n",
    "        total_steps += 1\n",
    "\n",
    "        if np.random.rand() < prob_random or \\\n",
    "            num_episode < pre_train_episodes:\n",
    "                # Act randomly based on prob_random or if we\n",
    "                # have not accumulated enough pre_train episodes\n",
    "                action = np.random.randint(9)\n",
    "        else:\n",
    "            # Decide what action to take from the Q network\n",
    "            action = np.argmax(online_q_values.predict(np.array([state])))\n",
    "\n",
    "        # Take the action and retrieve the next state, reward and done\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = preprocess_observation(next_state)\n",
    "\n",
    "        # Setup the episode to be stored in the episode buffer\n",
    "        episode = np.array([[state],action,reward,[next_state],done])\n",
    "        episode = episode.reshape(1,-1)\n",
    "\n",
    "        # Store the experience in the episode buffer\n",
    "        episode_buffer.add(episode)\n",
    "\n",
    "        # Update the running rewards\n",
    "        sum_rewards += reward\n",
    "\n",
    "        # Update the state\n",
    "        state = next_state\n",
    "    if num_episode > pre_train_episodes:\n",
    "        # Training the network\n",
    "        if prob_random > prob_random_end:\n",
    "                # Drop the probability of a random action\n",
    "                prob_random -= prob_random_drop\n",
    "                \n",
    "        if num_episode % update_freq == 0:\n",
    "            for num_epoch in range(num_epochs):\n",
    "                # Train batch is [[state,action,reward,next_state,done],...]\n",
    "                train_batch = experience_replay.sample(batch_size)\n",
    "\n",
    "                # Separate the batch into its components\n",
    "                train_state, train_action, train_reward, \\\n",
    "                    train_next_state, train_done = train_batch.T\n",
    "\n",
    "                # Convert the action array into an array of ints so they can be used for indexing\n",
    "                train_action = train_action.astype(np.int)\n",
    "\n",
    "                # Stack the train_state and train_next_state for learning\n",
    "                train_state = np.vstack(train_state)\n",
    "                train_next_state = np.vstack(train_next_state)\n",
    "\n",
    "                # Our predictions (actions to take) from the main Q network\n",
    "                target_q = target_q_values.predict(train_state)\n",
    "\n",
    "                # The Q values from our target network from the next state\n",
    "                target_q_next_state = online_q_values.predict(train_next_state)\n",
    "                train_next_state_action = np.argmax(target_q_next_state,axis=1)\n",
    "                train_next_state_action = train_next_state_action.astype(np.int)\n",
    "\n",
    "                # Tells us whether game over or not\n",
    "                # We will multiply our rewards by this value\n",
    "                # to ensure we don't train on the last move\n",
    "                train_gameover = train_done == 0\n",
    "\n",
    "                # Q value of the next state based on action\n",
    "                train_next_state_values = target_q_next_state[range(batch_size), train_next_state_action]\n",
    "\n",
    "                # Reward from the action chosen in the train batch\n",
    "                actual_reward = train_reward + (y * train_next_state_values * train_gameover)\n",
    "                target_q[range(batch_size), train_action] = actual_reward\n",
    "\n",
    "                # Train the main model\n",
    "                loss = online_q_values.train_on_batch(train_state, target_q)\n",
    "                losses.append(loss)\n",
    "\n",
    "            # Update the target model with values from the main model\n",
    "            update_target_graph(online_q_values, target_q_values, tau)\n",
    "\n",
    "            #if (num_episode + 1) % save_every == 0:\n",
    "            #    # Save the model\n",
    "            #    online_q_values.model.save_weights(main_weights_file)\n",
    "            #    target_q_values.model.save_weights(target_weights_file)\n",
    "\n",
    "    \n",
    "    # Increment the episode\n",
    "    num_episode += 1\n",
    "    \n",
    "    experience_replay.add(episode_buffer.buffer)\n",
    "    num_steps.append(cur_step)\n",
    "    rewards.append(sum_rewards)\n",
    "    \n",
    "    if num_episode % print_every == 0:\n",
    "        # Print progress\n",
    "        mean_loss = np.mean(losses[-(print_every * num_epochs):])\n",
    "\n",
    "        print(\"Num episode: {} Mean reward: {:0.4f} Prob random: {:0.4f}, Loss: {:0.04f}\".format(\n",
    "            num_episode, np.mean(rewards[-print_every:]), prob_random, mean_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    if num_episode > pre_train_episodes:\n",
    "        # Training the network\n",
    "\n",
    "        if prob_random > prob_random_end:\n",
    "            # Drop the probability of a random action\n",
    "            prob_random -= prob_random_drop\n",
    "\n",
    "        if num_episode % update_freq == 0:\n",
    "            for num_epoch in range(num_epochs):\n",
    "                # Train batch is [[state,action,reward,next_state,done],...]\n",
    "                train_batch = experience_replay.sample(batch_size)\n",
    "\n",
    "                # Separate the batch into its components\n",
    "                train_state, train_action, train_reward, \\\n",
    "                    train_next_state, train_done = train_batch.T\n",
    "                    \n",
    "                # Convert the action array into an array of ints so they can be used for indexing\n",
    "                train_action = train_action.astype(np.int)\n",
    "\n",
    "                # Stack the train_state and train_next_state for learning\n",
    "                train_state = np.vstack(train_state)\n",
    "                train_next_state = np.vstack(train_next_state)\n",
    "\n",
    "                # Our predictions (actions to take) from the main Q network\n",
    "                target_q = target_qn.model.predict(train_state)\n",
    "                \n",
    "                # The Q values from our target network from the next state\n",
    "                target_q_next_state = main_qn.model.predict(train_next_state)\n",
    "                train_next_state_action = np.argmax(target_q_next_state,axis=1)\n",
    "                train_next_state_action = train_next_state_action.astype(np.int)\n",
    "                \n",
    "                # Tells us whether game over or not\n",
    "                # We will multiply our rewards by this value\n",
    "                # to ensure we don't train on the last move\n",
    "                train_gameover = train_done == 0\n",
    "\n",
    "                # Q value of the next state based on action\n",
    "                train_next_state_values = target_q_next_state[range(batch_size), train_next_state_action]\n",
    "\n",
    "                # Reward from the action chosen in the train batch\n",
    "                actual_reward = train_reward + (y * train_next_state_values * train_gameover)\n",
    "                target_q[range(batch_size), train_action] = actual_reward\n",
    "                \n",
    "                # Train the main model\n",
    "                loss = main_qn.model.train_on_batch(train_state, target_q)\n",
    "                losses.append(loss)\n",
    "                \n",
    "            # Update the target model with values from the main model\n",
    "            update_target_graph(main_qn.model, target_qn.model, tau)\n",
    "\n",
    "            if (num_episode + 1) % save_every == 0:\n",
    "                # Save the model\n",
    "                main_qn.model.save_weights(main_weights_file)\n",
    "                target_qn.model.save_weights(target_weights_file)\n",
    "    \n",
    "\n",
    "    # Increment the episode\n",
    "    num_episode += 1\n",
    "\n",
    "    experience_replay.add(episode_buffer.buffer)\n",
    "    num_steps.append(cur_step)\n",
    "    rewards.append(sum_rewards)\n",
    "        \n",
    "    if num_episode % print_every == 0:\n",
    "        # Print progress\n",
    "        mean_loss = np.mean(losses[-(print_every * num_epochs):])\n",
    "\n",
    "        print(\"Num episode: {} Mean reward: {:0.4f} Prob random: {:0.4f}, Loss: {:0.04f}\".format(\n",
    "            num_episode, np.mean(rewards[-print_every:]), prob_random, mean_loss))\n",
    "        if np.mean(rewards[-print_every:]) >= goal:\n",
    "            print(\"Training complete!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the main training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    step = global_step.eval()\n",
    "    if step >= n_steps:\n",
    "        break\n",
    "    iteration += 1\n",
    "    print(\"\\rIteration {}\\tTraining step {}/{} ({:.1f})%\\tLoss {:5f}\\tMean Max-Q {:5f}   \".format(\n",
    "        iteration, step, n_steps, step * 100 / n_steps, loss_val, mean_max_q), end=\"\")\n",
    "    if done: # game over, start again\n",
    "        obs = env.reset()\n",
    "        for skip in range(skip_start): # skip the start of each game\n",
    "            obs, reward, done, info = env.step(0)\n",
    "        state = preprocess_observation(obs)\n",
    "\n",
    "    # Online DQN evaluates what to do\n",
    "    q_values = online_q_values.eval(feed_dict={X_state: [state]})\n",
    "    action = epsilon_greedy(q_values, step)\n",
    "\n",
    "    # Online DQN plays\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    next_state = preprocess_observation(obs)\n",
    "\n",
    "    # Let's memorize what happened\n",
    "    replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
    "    state = next_state\n",
    "\n",
    "    # Compute statistics for tracking progress (not shown in the book)\n",
    "    total_max_q += q_values.max()\n",
    "    game_length += 1\n",
    "    if done:\n",
    "        mean_max_q = total_max_q / game_length\n",
    "        total_max_q = 0.0\n",
    "        game_length = 0\n",
    "\n",
    "    if iteration < training_start or iteration % training_interval != 0:\n",
    "        continue # only train after warmup period and at regular intervals\n",
    "\n",
    "    # Sample memories and use the target DQN to produce the target Q-Value\n",
    "    X_state_val, X_action_val, rewards, X_next_state_val, continues = (\n",
    "        sample_memories(batch_size))\n",
    "    next_q_values = target_q_values.eval(\n",
    "        feed_dict={X_state: X_next_state_val})\n",
    "    max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "    y_val = rewards + continues * discount_rate * max_next_q_values\n",
    "\n",
    "    # Train the online DQN\n",
    "    _, loss_val = sess.run([training_op, loss], feed_dict={\n",
    "        X_state: X_state_val, X_action: X_action_val, y: y_val})\n",
    "\n",
    "    # Regularly copy the online DQN to the target DQN\n",
    "    if step % copy_steps == 0:\n",
    "        copy_online_to_target.run()\n",
    "\n",
    "    # And save regularly\n",
    "    if step % save_steps == 0:\n",
    "        saver.save(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can interrupt the cell above at any time to test your agent using the cell below. You can then run the cell above once again, it will load the last parameters saved and resume training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "n_max_steps = 10000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "\n",
    "    obs = env.reset()\n",
    "    for step in range(n_max_steps):\n",
    "        state = preprocess_observation(obs)\n",
    "\n",
    "        # Online DQN evaluates what to do\n",
    "        q_values = online_q_values.eval(feed_dict={X_state: [state]})\n",
    "        action = np.argmax(q_values)\n",
    "\n",
    "        # Online DQN plays\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        img = env.render(mode=\"rgb_array\")\n",
    "        frames.append(img)\n",
    "\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
