{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "\n",
    "<a href=\"http://www.hupi.fr/\" ><img src=\"http://www.hupi.fr/wp-content/uploads/2016/03/hupi_logo_vectoris_menu.png\" style=\"float:right; max-width: 300px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies des grosses données](https://github.com/wikistat/Ateliers-Big-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommandation de Films par Filtrage Collaboratif: [NMF](http://wikistat.fr/pdf/st-m-explo-nmf.pdf) de la librairie [MLlib](http://spark.apache.org/mllib/) de <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"Spark\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Ce calepin traite d'un problème classique de recommandation par filtrage collaboratif en utilisant les ressources de la librairie [MLlib de Spark]([http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.ALS) avec l'API pyspark. Le problème général est décrit en [introduction](https://github.com/wikistat/Ateliers-Big-Data/tree/master/3-MovieLens) et dans une [vignette](http://wikistat.fr/pdf/st-m-datSc3-colFil.pdf) de [Wikistat](http://wikistat.fr/). Il est appliqué aux données publiques du site [GroupLens](http://grouplens.org/datasets/movielens/). L'objectif est de tester les méthodes et la procédure d'optimisation sur le plus petit jeu de données composé de 100k notes  de 943 clients sur 1682 films où chaque client a au moins noté 20 films. Les jeux de données plus gros (1M, 10M, 20M notes) peuvent être utilisés pour \"passer à l'échelle volume\". \n",
    "\n",
    "Ce calepin s'inspire des exemples de la [documentation](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.ALS) et d'un [tutoriel](https://github.com/jadianes/spark-movie-lens/blob/master/notebooks/building-recommender.ipynb) de [Jose A. Dianes](https://www.codementor.io/jadianes). Le sujet a été traité lors d'un [Spark Summit](https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html).\n",
    "\n",
    "L'objectif est d'utiliser ces seules données pour proposer des recommandations.  Les données initiales sont sous la forme d'une matrice **très creuse** (*sparse*) contenant des notes ou évaluations. **Attention**, les \"0\" de la matrice ne sont pas des notes mais des *données manquantes*, le film n'a pas encore été vu ou évalué. \n",
    "\n",
    "Un algorithme satisfaisant à l'objectif de *complétion de grande matrice creuse*, et implémenté dans un logiciel libre d'accès est disponible dans la librairie [softImpute de R](https://cran.r-project.org/web/packages/softImpute/index.html). SOn utilisaiton est décrite dans un autre [calepin](https://github.com/wikistat/Ateliers-Big-Data/blob/master/3-MovieLens/Atelier-MovieLens-softImpute.ipynb). La version de [NMF](http://wikistat.fr/pdf/st-m-explo-nmf.pdf) de [MLlib de Spark](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.ALS) autorise permet également la complétion.\n",
    "\n",
    "En revanche,la version  de NMF incluse dans la librairie [Scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html) traite également des [matrices creuses](http://docs.scipy.org/doc/scipy/reference/sparse.html) mais le critère (moindres carrés) optimisé considère les \"0\" comme des notes nulles, pas comme des données manquantes. *Elle n'est pas adaptée au problème de complétion*, contrairement à celle de MLliB. Il faudrait sans doute utiliser la librairie [nonnegfac](https://github.com/kimjingu/nonnegfac-python) en Python  de [Kim et al. (2014)](http://link.springer.com/content/pdf/10.1007%2Fs10898-013-0035-4.pdf); **à tester**!\n",
    "\n",
    "Dans la première partie, le plus petit fichier est partagé en trois échantillons: apprentissage, validation et test; l'optimisation du rang de la factorisation (nombre de facteurs latents) est réalisée par minimisation de l'erreur estimée sur l'échantillon de validation.\n",
    "\n",
    "Ensuite le plus gros fichier est utilisé pour évaluer l'impact de la taille de la base d'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Importation des données en HDFS\n",
    "Les données doivent être stockées à un emplacement accessibles de tous les noeuds du cluster pour permettre la construction de la base de données réparties (RDD). Dans une utilisation monoposte (*standalone*) de *Spark*, elles sont simplement chargées dans le répertoire courant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Chargement des fichiers si ce n'est déjà fait\n",
    "import urllib\n",
    "# fichier réduit\n",
    "f = urllib.urlretrieve(\"http://www.math.univ-toulouse.fr/~besse/Wikistat/data/ml-ratings100k.csv\",\"ml-ratings100k.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données sont lues comme une seule ligne de texte avant d'être restructurées au bon format d'une *matrice creuse* à savoir une liste de triplets contenant les  indices de ligne, de colonne et la note pour les seules valeurs renseignées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importer les données au format texte dans un RDD\n",
    "\n",
    "# le chemin dépend de l'environnement\n",
    "path=\"\"\n",
    "small_ratings_raw_data = sc.textFile(path+\"ml-ratings100k.csv\")\n",
    "# Identifier et afficher la première ligne\n",
    "small_ratings_raw_data_header = small_ratings_raw_data.take(1)[0]\n",
    "small_ratings_raw_data_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Séparer les champs (user, item, note) dans un nouveau RDD\n",
    "# La première ligne est éliminée\n",
    "# .cache() : le RDD est conservé en mémoire une fois traité\n",
    "small_ratings_data = small_ratings_raw_data.filter(lambda line: line!=small_ratings_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1],tokens[2])).cache()\n",
    "# Trois premières lignes pour voir\n",
    "small_ratings_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimisation du rang sur l'échantillon 10k\n",
    "Le fichier comporte 10 000 évaluations croisant les avis de mille utilisateurs sur les films qu'ils ont vus parmi 1700."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Constitution des échantillons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Séparation aléatoire en trois échantillons apprentissage, validation et test. Le paramètre de rang est optimisé en minimisant l'estimaiton de l'erreur sur l'échantillon test. Cette stratégie, plutôt qu'ue validation croisée est plus adaptée à des données massives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tauxApp=0.6\n",
    "tauxVal=0.2\n",
    "tauxTes=0.2\n",
    "# Si le total est inférieur à 1, les données sont sous-échantillonnées.\n",
    "trainRDD, validRDD, testRDD = small_ratings_data.randomSplit([6, 2, 2], seed=0L)\n",
    "# validation et test à prédire, sans les notes\n",
    "validRDD_P = validRDD.map(lambda x: (x[0], x[1]))\n",
    "testRDD_P = testRDD.map(lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Optimisation du rang de la NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'erreur d'imputation des données, donc de recommandation, est estimée sur l'échantillon de validation pour différentes valeurs (grille) du rang de la factorisation matricielle. \n",
    "\n",
    "Il faudrait en principe aussi optimiser la valeur du paramètre de pénalisation pris à 0.1 par défaut.\n",
    "\n",
    "*Point important:* l'erreur d'ajustement de la factorisation ne prend en compte que les valeurs listées dans la matrice creuses, pas les \"0\" qui sont des données manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "import math\n",
    "# Initialisation du générateur\n",
    "seed = 5L\n",
    "# Nombre max d'itérations (ALS)\n",
    "iterations = 10\n",
    "# Régularisation L1; à optimiser également\n",
    "regularization_parameter = 0.1\n",
    "# Choix d'une grille pour les valeurs du rang à optimiser\n",
    "ranks = [4, 8, 12]\n",
    "# Une erreur par rang\n",
    "errors = [0, 0, 0]\n",
    "# Initialisations\n",
    "err = 0\n",
    "tolerance = 0.02\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "best_iteration = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Estimation pour chaque valeur de rang\n",
    "for rank in ranks:\n",
    "    model = ALS.train(trainRDD, rank, seed=seed, iterations=iterations,\n",
    "                      lambda_=regularization_parameter)\n",
    "    # Prévision de l'échantillon de validation\n",
    "    predRDD = model.predictAll(validRDD_P).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    # Jointure avec la vraie solution (échantillon de validation)\n",
    "    notETpredRDD = validRDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predRDD)\n",
    "    # Calcul du RMSE\n",
    "    error = math.sqrt(notETpredRDD.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    errors[err] = error\n",
    "    err += 1\n",
    "    print 'Pour le rang %s le RMSE est: %s' % (rank, error)\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        best_rank = rank\n",
    "# Meilleure solution\n",
    "print 'Rang optimal: %s' % best_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Résultats et test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Quelques prévisions\n",
    "predRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \"vraie\" note et sa prévision\n",
    "notETpredRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prévision finale de l'échantillon test.\n",
    "\n",
    "**Remarque :** il aurait été judicieux de fusionner les échantillons d'apprentissage et de validation avant de réestimer le modèle avec le rang optimal précédemment trouvé avant de prévoir l'échantillon test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Le seul échantillon d'apprentissage est utilisé avec le rang optimal\n",
    "model = ALS.train(trainRDD, best_rank, seed=seed, iterations=iterations,\n",
    "                      lambda_=regularization_parameter)\n",
    "# Prévision de l'échantillon test\n",
    "predRDD = model.predictAll(testRDD_P).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "# Jointure\n",
    "notETpredRDD = testRDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predRDD)\n",
    "# Calcul de l'erreur.\n",
    "error = math.sqrt(notETpredRDD.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    \n",
    "print 'RMSE pour le test: %s' % (error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3 Analyse du fichier complet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MovieLens propose un plus gros fichier avec 20M de notes (138000 utilisateurs, 27000 films). Ce fichier est utilisé pour extraire un fichier test de deux millions de notes à reconstruire. Les paramètres précédemment optimisés, ils pourraient sans doute l'être mieux, sont appliqués pour une succesion d'estimation / prévision avec une taille croissante de l'échantillon d'apprentissage. Il aurait été plus élégant d'automatiser le travail dans une boucle mais lorsque les données sont les plus volumineuses des comportement mal contrôlés de Spark peuvent provoquer des plantages par défaut de mémoire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.1 Lecture des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le fichier est prétraité de manière analogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Chargement des fichiers si ce n'est déjà fait\n",
    "import urllib\n",
    "# fichier complet mais compressé\n",
    "f = urllib.urlretrieve(\"http://www.math.univ-toulouse.fr/~besse/Wikistat/data/ml-ratings20M.zip\",\"ml-ratings20M.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importer les données au format texte dans un RDD\n",
    "path=\"\"\n",
    "ratings_raw_data = sc.textFile(path+\"ml-ratings20M.zip\")\n",
    "# Identifier et afficher la première ligne\n",
    "ratings_raw_data_header = ratings_raw_data.take(1)[0]\n",
    "ratings_raw_data_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Séparer les champs (user, item, note) dans un nouveau RDD\n",
    "# La première ligne est éliminée\n",
    "# les trois premiers champs sont sélectionnés\n",
    "ratings_data = ratings_raw_data.filter(lambda line: line!=ratings_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1],tokens[2]))\n",
    "# Trois premières lignes pour voir\n",
    "ratings_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Echantillonnage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction de l'échantillon test et éventuellement sous-échantillonnage de l'échantillon d'apprentissage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tauxTest=0.10\n",
    "(trainDataTot, testData) = ratings_data.randomSplit([1-tauxTest, tauxTest], seed=0L)\n",
    "testData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# départ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sous-échantillonnage de l'apprentissage permettant de \n",
    "# tester pour des tailles croissantes de cet échantillon\n",
    "tauxEch=1\n",
    "(trainData, DropData) = trainDataTot.randomSplit([tauxEch, 1-tauxEch])\n",
    "trainData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Estimation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle est estimé en utilisant les valeurs des paramètres obtenues dans l'étape précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "import math\n",
    "import time\n",
    "time_start=time.time()\n",
    "# Initialisation du générateur\n",
    "seed = 5L\n",
    "# Nombre max d'itérations (ALS)\n",
    "iterations = 10\n",
    "# Régularisation L1 (valeur par défaut)\n",
    "regularization_parameter = 0.1\n",
    "tolerance = 0.02\n",
    "min_error = float('inf')\n",
    "best_rank = 8\n",
    "# Estimation pour chaque valeur de rang\n",
    "model = ALS.train(trainData, rank=best_rank, seed=seed, \n",
    "                iterations=iterations,lambda_=regularization_parameter)\n",
    "time_end=time.time()\n",
    "time_als=(time_end - time_start)\n",
    "print(\"ALS prend %d s\" %(time_als)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Prévision de l'échantillon test et erreur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Listes (i,j) des notes à prédire\n",
    "testData_r = testData.map(lambda x: (x[0], x[1]))\n",
    "# Prévision de l'échantillon test\n",
    "predTest = model.predictAll(testData_r).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "# Jointure\n",
    "notETpred = testData.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predTest)\n",
    "\n",
    "# Calcul de l'erreur\n",
    "erreur = math.sqrt(notETpred.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "print 'RMSE pour le test: %s' % (erreur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques résultats montrant l'évolution du temps de calcul et de l'erreur de prévision en fonction de la taille de l'échantillon d'apprentissage. Le nombre d'utilisateurs de la plateforme étant assez aléatoire, les temps calculés sont peu fiables. D'autre part il est probable que la valeur des paramètres optimaux dépendent de la taille de l'échantillon d'apprenitssage.\n",
    "\n",
    "Taille | Temps(s) | RMSE\n",
    "-------|-------|------\n",
    "217439 | 70    | 1.65\n",
    "1029416| 73    | 1.06\n",
    "2059855| 72    | 1.05\n",
    "4119486| 89    | 0.88\n",
    "6176085| 99    | 0.85\n",
    "10301909| 117  | 0.83\n",
    "12361034| 125  | 0.83\n",
    "14414907| 137  | 0.82\n",
    "16474087| 148  | 0.818\n",
    "18538142| 190  | 0.816\n",
    "20596263| 166  | 0.82\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
