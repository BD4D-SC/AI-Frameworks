{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies de l'intelligence Artificielle](https://github.com/wikistat/AI-Frameworks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" width=400, style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement Naturel du Langage (NLP) : Catégorisation de Produits Cdiscount\n",
    "\n",
    "Il s'agit d'une version simplifiée du concours proposé par Cdiscount et paru sur le site [datascience.net](https://www.datascience.net/fr/challenge). Les données d'apprentissage sont accessibles sur demande auprès de Cdiscount mais les solutions de l'échantillon test du concours ne sont pas et ne seront pas rendues publiques. Un échantillon test est donc construit pour l'usage de ce tutoriel.  L'objectif est de prévoir la catégorie d'un produit à partir de son descriptif (*text mining*). Seule la catégorie principale (1er niveau, 47 classes) est prédite au lieu des trois niveaux demandés dans le concours. L'objectif est plutôt de comparer les performances des méthodes et technologies en fonction de la taille de la base d'apprentissage ainsi que d'illustrer sur un exemple complexe le prétraitement de données textuelles. \n",
    "\n",
    "Le jeux de données complet (15M produits) permet un test en vrai grandeur du **passage à l'échelle volume** des phases de préparation (*munging*), vectorisation (hashage, TF-IDF) et d'apprentissage en fonction de la technologie utilisée.\n",
    "\n",
    "La synthèse des résultats obtenus est développée par [Besse et al. 2016](https://hal.archives-ouvertes.fr/hal-01350099) (section 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 : Construction des caractéristiques ou *features*, Vectorisation et Word embedding.\n",
    "\n",
    "Les données textuelles ne peuvent pas être utilisés directement dans les différents algorithmes d'apprentissage statistiques. Nous allons voir dans ce tutoriel plusieurs techniques permettant de traduire les données textuelles sous formes de vecteur numérique : \n",
    "\n",
    "Fonction de vectorisation présente dans la librairie `scikit-learn` :\n",
    "\n",
    "* `One-Hot-Encoder`\n",
    "* `Tf-Idf`\n",
    "* `Hashing`\n",
    "\n",
    "Word Embedding  présente dans la librairie `gensim` :\n",
    "\n",
    "* `Word2Vec`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Téléchargement des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bguillouet/anaconda/envs/TPInsa/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Importation des librairies utilisées\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import itertools\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Téléchargement des données\n",
    "\n",
    "On télécharge les données de train et de validation néttoyé et racinisé dans le notebook précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_valid_clean_stem = pd.read_csv(\"data/cdiscount_valid_clean_stem.csv\").fillna(\"\")\n",
    "data_train_clean_stem = pd.read_csv(\"data/cdiscount_train_clean_stem.csv\").fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On créé un dossier dans lequel nous allons sauvegarder les DataFrame constitués des features que l'on va construire dans ce notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_OUTPUT_DIR = \"data/features\"\n",
    "if not(os.path.isdir(\"data/features\")):\n",
    "    os.mkdir(\"data/features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, en guise d'exemple et pour réduire le temps de calcul, on ne considère que la colonne *Description* de nos `DataFrame` générés dans le calepin précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_array = data_train_clean_stem[\"Description\"].values\n",
    "valid_array = data_valid_clean_stem[\"Description\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorisaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'batter original samsung galaxy ace ion mah batter qualit superieur ideal lautonom portabl reduit voir present'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_array[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encoding\n",
    "\n",
    "* **One-Hot-Encoding**\n",
    "\n",
    "L'encodage  *One-Hot-Encoding* des données peut être effectué grâce à la classe `CountVectorizer`de **scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "extr_cv = CountVectorizer(binary=False)\n",
    "data_train_OHE = extr_cv.fit_transform(train_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** A quoi sert l'argument binary de la classe?\n",
    "\n",
    "**Q** Sous quel format sont stockées les vecteurs OHE? Pourquoi ce format est-il choisi?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `get_feature_names` permet d'avoir accès à la liste des mots présents dans l'ensemble des lignes de l'array converti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de mots : 56668\n"
     ]
    }
   ],
   "source": [
    "vocabulary = extr_cv.get_feature_names()\n",
    "N_vocabulary = len(vocabulary)\n",
    "print(\"Nombre de mots : %d\" %N_vocabulary )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** Pour la première ligne de votre dataset train. retrouvez l'ensemble des mots constituant cette ligne à partir de l'objet `data_train_OHE`et de `vocabulary` ainsi que le nombre d'occurence de chacun de ces mots dans la ligne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/2_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La même transformation est appliqué sur l'échantillon de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5000x56668 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 56985 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_valid_OHE = extr_cv.transform(valid_array)\n",
    "data_valid_OHE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que se passe-til pour les mots présents dans le dataset de validation mais qui ne sont pas présent dans le dataset d'apprentissage?\n",
    "\n",
    "**Q** Pourquoi on ne 're-fit' pas la Classe `CountVectorizer`l'échantillon de validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF¶\n",
    "\n",
    "* **TF-IDF**. Le TF-IDF permet de faire ressortir l'importance relative de chaque mot $m$ (ou couples de mots consécutifs) dans un texte-produit ou un descriptif $d$, par rapport à la liste entière des produits. La fonction $TF(m,d)$ compte le nombre d'occurences du mot $m$ dans le descriptif $d$. La fonction $IDF(m)$ mesure l'importance du terme dans l'ensemble des documents ou descriptifs en donnant plus de poids aux termes les moins fréquents car considérés comme les plus discriminants (motivation analogue à celle de la métrique du chi2 en anamlyse des correspondance). $IDF(m,l)=\\log\\frac{D}{f(m)}$ où $D$ est le nombre de documents, la taille de l'échantillon d'apprentissage, et $f(m)$ le nombre de documents ou descriptifs contenant le mot $m$. La nouvelle variable ou *features* est $V_m(l)=TF(m,l)\\times IDF(m,l)$.\n",
    "\n",
    "* Comme pour les transformations des variables quantitatives (centrage, réduction), la même transformation c'est-à-dire les mêmes pondérations, est calculée sur l'achantillon d'apprentissage et appliquée à celui de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utiliser la fonction `TfidfVectorizer` qui permet de parser également le texte\n",
    "\n",
    "Dans un premier temps, on fixe le paramètre `norm` = False afin de rendre les résultats plus explicite et analyser les sorties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer(ngram_range=(1,1), norm = False)\n",
    "data_train_TFIDF = vec.fit_transform(train_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** A quoi sert l'argument ngram_range?\n",
    "\n",
    "Constatez que `data_train_TFIDF`est stocké sous le même format que `data_train_OHE` et que la taile du vocabulaire est la même."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56668"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = vec.get_feature_names()\n",
    "N_vocabulary = len(vocabulary)\n",
    "N_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** Pour la première ligne de votre dataset train. retrouvez l'ensemble des mots constituant cette ligne à partir de l'objet `data_train_TFIDF`et de `vocabulary` ainsi que la valeur de l'idf, du tf et du poids tfidf de chacun de ces mots dans la ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/2_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Commentez les valeurs de l'idf pour chacun des mots.\n",
    "\n",
    "**Q** Comment evolue les poids en changeant les paramètre *smooth idf* et *sublinear_tf* de la méthode `TfidfVectorizer`?\n",
    "\n",
    "**Exercice** Changez l'argument *ngram_range* de la méthode `TfidfVectorizer` et re-affichez les résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique maintenant le `vectorizer` sur le jeu de données de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5000x56668 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 56985 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_valid_TFIDF = vec.transform(valid_array)\n",
    "data_valid_TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ATENTION** Si le tf est recalculé pour chaque ligne, le même idf est utilisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solution/2_2bis.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le **Hashage**. Il permet de réduire l'espace des variables (taille du dictionnaire) en un nombre limité et fixé a priori `n_hash` de caractéristiques. Il repose sur la définition d'une fonction de hashage, $h$ qui à un indice $j$ défini dans l'espace des entiers naturels, renvoie un indice $i=h(j)$ dans dans l'espace réduit (1 à n_hash) des caractéristiques. Ainsi le poids de l'indice $i$, du nouvel espace, est l'association de tous les poids d'indice $j$ tels que $i=h(j)$ de l'espace originale. Ici, les poids sont associés d'après la méthode décrite par Weinberger et al. (2009).\n",
    "\n",
    "N.B. $h$ n'est pas généré aléatoirement. Ainsi pour un même fichier d'apprentissage (ou de test) et pour un même entier n_hash, le résultat de la fonction de hashage est identique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au contraire des classe `CountVectorizer`et `TfidfVectorizer`, la classe `FeatureHasher` prend en entré un dictionnaire d'occurence des mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ace': 1,\n",
       "         'batter': 2,\n",
       "         'galaxy': 1,\n",
       "         'ideal': 1,\n",
       "         'ion': 1,\n",
       "         'lautonom': 1,\n",
       "         'mah': 1,\n",
       "         'original': 1,\n",
       "         'portabl': 1,\n",
       "         'present': 1,\n",
       "         'qualit': 1,\n",
       "         'reduit': 1,\n",
       "         'samsung': 1,\n",
       "         'superieur': 1,\n",
       "         'voir': 1})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict_array  = list(map(lambda x : collections.Counter(x.split(\" \")), train_array))\n",
    "train_dict_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "nb_hash = 300\n",
    "feathash = FeatureHasher(nb_hash)\n",
    "data_train_hash = feathash.fit_transform(train_dict_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constatez que `data_train_hash` est stocké sous le même format que `data_train_OHE` ou `data_train_TFIDF`.  \n",
    "\n",
    "**Q** Que dire cependant de sa dimension?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cellule suivante permet d'afficher le poids de chacun des indices dans le nouvel espace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des tokens racinisé de la première ligne : batter original samsung galaxy ace ion mah batter qualit superieur ideal lautonom portabl reduit voir present\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indices</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>74</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>113</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>131</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>173</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>175</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>204</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>220</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>272</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>274</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>288</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    indices  weight\n",
       "0         3     1.0\n",
       "1         4     1.0\n",
       "2        14     1.0\n",
       "3        21    -1.0\n",
       "4        30     1.0\n",
       "5        74     1.0\n",
       "6       113    -1.0\n",
       "7       131     1.0\n",
       "8       173     1.0\n",
       "9       175     1.0\n",
       "10      204     2.0\n",
       "11      220    -1.0\n",
       "12      272    -1.0\n",
       "13      274     1.0\n",
       "14      288    -1.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir = 0\n",
    "rw = data_train_hash.getrow(ir)\n",
    "print(\"Liste des tokens racinisé de la première ligne : \" + train_array[0])\n",
    "pd.DataFrame([(v, k)  for k,v in zip(rw.data,rw.indices)], columns=[\"indices\",\"weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que pouvez-vous dire des poids?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La taille de la matrice a donc été très réduit par rapport au vectorizer, TFIDF ou One-Hot-Enconding. Cependant, il n'y a pas de fonction inverse transform ce qui peut rendre le résultat moin compréhensible.\n",
    "\n",
    "\n",
    "Il est possible de combiner le `FeatureHasher`avec un autre vectorizer comme le TFIDF. \n",
    "\n",
    "C'est cette fois la classe `TFIDFTransformer` qui est utilisé. Celle ci ne ne considère pas des string en entré mais l'array en sortie de l'étape de Hashage. Les mots sont les `nb_hash`indices sélectionnés et le tf pour chaque individu sont les poids calculé par la fonction de hasage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<95000x300 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1082962 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vec =  TfidfTransformer(norm = False)\n",
    "data_train_HTfidf = vec.fit_transform(data_train_hash)\n",
    "data_train_HTfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batter original samsung galaxy ace ion mah batter qualit superieur ideal lautonom portabl reduit voir present\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indices</th>\n",
       "      <th>idf</th>\n",
       "      <th>tf</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>288</td>\n",
       "      <td>4.510436</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-4.510436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>274</td>\n",
       "      <td>1.310477</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.310477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>272</td>\n",
       "      <td>4.510788</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-4.510788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>220</td>\n",
       "      <td>1.326674</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.326674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>204</td>\n",
       "      <td>3.179447</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.358894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>175</td>\n",
       "      <td>3.764130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.764130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>173</td>\n",
       "      <td>3.332079</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.332079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>131</td>\n",
       "      <td>3.910628</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.910628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>113</td>\n",
       "      <td>5.015058</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-5.015058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>74</td>\n",
       "      <td>3.637848</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.637848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>30</td>\n",
       "      <td>3.233364</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.233364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>21</td>\n",
       "      <td>3.230030</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-3.230030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>3.285583</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.285583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>3.657618</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.657618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>3.965469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.965469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    indices       idf   tf    weight\n",
       "0       288  4.510436 -1.0 -4.510436\n",
       "1       274  1.310477  1.0  1.310477\n",
       "2       272  4.510788 -1.0 -4.510788\n",
       "3       220  1.326674 -1.0 -1.326674\n",
       "4       204  3.179447  2.0  6.358894\n",
       "5       175  3.764130  1.0  3.764130\n",
       "6       173  3.332079  1.0  3.332079\n",
       "7       131  3.910628  1.0  3.910628\n",
       "8       113  5.015058 -1.0 -5.015058\n",
       "9        74  3.637848  1.0  3.637848\n",
       "10       30  3.233364  1.0  3.233364\n",
       "11       21  3.230030 -1.0 -3.230030\n",
       "12       14  3.285583  1.0  3.285583\n",
       "13        4  3.657618  1.0  3.657618\n",
       "14        3  3.965469  1.0  3.965469"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir = 0\n",
    "\n",
    "rw = data_train_HTfidf.getrow(ir)\n",
    "print(train_array[ir])\n",
    "pd.DataFrame([(ind, vec.idf_[ind], w/vec.idf_[ind], w)  for w,ind in zip(rw.data, rw.indices)], columns=[\"indices\",\"idf\",\"tf\",\"weight\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Build and Save Vectorize Vector\n",
    "\n",
    "Afin de comparer l'effet de ces différentes vectorisation sur l'apprentissage, nous allons sauvegarder ces dernières sur les machines.\n",
    "\n",
    "De nombreux paramètres sont à régler ce qui entraine donc un un très grand nombre de combinaison.\n",
    "\n",
    "ici nous allons créer 4 jeu de données avec `CountVectorizer` et `TFIDF` chacun avec et sans hashage de taille 300.\n",
    "\n",
    "Il est evidemment possible de tester d'autre combinaison. Libre à vous de les tester ;) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction de Vectorisation\n",
    "\n",
    "on créé deux fonctions `vectorizer_train` and `apply_vectorizer` afin de générer automatiquement différent dataframe d'apprentissage et de validation vectorisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizer_train(df, columns=['Description'], nb_hash=None, nb_gram = 1, vectorizer = \"tfidf\" , binary = False):\n",
    "    \n",
    "    data_array = [\" \".join(line) for line in df[columns].values]\n",
    "    \n",
    "    # Hashage\n",
    "    if nb_hash is None:\n",
    "        feathash = None\n",
    "        if vectorizer == \"tfidf\":\n",
    "            vec = TfidfVectorizer(ngram_range=(1,nb_gram))\n",
    "            data_vec = vec.fit_transform(data_array)\n",
    "        else:\n",
    "            vec = CountVectorizer(binary=binary)\n",
    "            data_vec = vec.fit_transform(data_array)\n",
    "    else:\n",
    "        data_dic_array = [collections.Counter(line.split(\" \")) for line in data_array]\n",
    "        feathash = FeatureHasher(nb_hash)\n",
    "        data_hash = feathash.fit_transform(data_dic_array)\n",
    "        \n",
    "        if vectorizer==\"tfidf\":\n",
    "            vec =  TfidfTransformer()\n",
    "            data_vec =  vec.fit_transform(data_hash)\n",
    "        else:\n",
    "            vec = None\n",
    "            data_vec = data_hash\n",
    "\n",
    "    return vec, feathash, data_vec\n",
    "\n",
    "\n",
    "\n",
    "def apply_vectorizer(df, vec, feathash, columns =['Description', 'Libelle', 'Marque']):\n",
    "    \n",
    "    data_array = [\" \".join(line) for line in df[columns].values]\n",
    "    \n",
    "    #Hashage\n",
    "    if feathash is None:\n",
    "        data_hash = data_array\n",
    "    else:\n",
    "        data_dic_array = [collections.Counter(line.split(\" \")) for line in data_array]\n",
    "        data_hash = feathash.transform(data_dic_array)\n",
    "    \n",
    "    if vec is None:\n",
    "        data_vec = data_hash\n",
    "    else:\n",
    "        data_vec = vec.transform(data_hash)\n",
    "    return data_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_hash : None, vectorizer : count\n",
      "Runing time for vectorization : 2.2 seconds\n",
      "Train shape : (95000, 56668)\n",
      "Valid shape : (5000, 56668)\n",
      "nb_hash : 300, vectorizer : count\n",
      "Runing time for vectorization : 1.8 seconds\n",
      "Train shape : (95000, 300)\n",
      "Valid shape : (5000, 300)\n",
      "nb_hash : None, vectorizer : tfidf\n",
      "Runing time for vectorization : 2.0 seconds\n",
      "Train shape : (95000, 56668)\n",
      "Valid shape : (5000, 56668)\n",
      "nb_hash : 300, vectorizer : tfidf\n",
      "Runing time for vectorization : 1.7 seconds\n",
      "Train shape : (95000, 300)\n",
      "Valid shape : (5000, 300)\n"
     ]
    }
   ],
   "source": [
    "parameters = [[None, \"count\"],\n",
    "              [300, \"count\"],\n",
    "              [None, \"tfidf\"],\n",
    "              [300, \"tfidf\"]]\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "for nb_hash, vectorizer in parameters:\n",
    "    ts = time.time()\n",
    "    vec, feathash, data_train_vec = vectorizer_train(data_train_clean_stem, nb_hash=nb_hash, vectorizer = vectorizer)\n",
    "    data_valid_vec = apply_vectorizer(data_valid_clean_stem, vec, feathash)\n",
    "    te = time.time()\n",
    "    \n",
    "    print(\"nb_hash : \" + str(nb_hash) + \", vectorizer : \" + str(vectorizer))\n",
    "    print(\"Runing time for vectorization : %.1f seconds\" %(te-ts))\n",
    "    print(\"Train shape : \" + str(data_train_vec.shape))\n",
    "    print(\"Valid shape : \" + str(data_valid_vec.shape))\n",
    "\n",
    "    \n",
    "    sparse.save_npz(DATA_OUTPUT_DIR +\"/vec_train_nb_hash_\" + str(nb_hash) + \"_vectorizer_\" + str(vectorizer), data_train_vec)\n",
    "    sparse.save_npz(DATA_OUTPUT_DIR +\"/vec_valid_nb_hash_\" + str(nb_hash) + \"_vectorizer_\" + str(vectorizer), data_valid_vec)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Dans cette partie, des modèles `Word2Vec`Seront créés à l'aide de la librairie [**gensim**](https://radimrehurek.com/gensim/index.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Word2Vec model\n",
    "\n",
    "La fonction `gensim.models.Word2Vec`qui permet de construire des modèle Word2Vec prend en entrée une liste de tokens. On tranformer donc nos données dans un premier temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['batter',\n",
       " 'original',\n",
       " 'samsung',\n",
       " 'galaxy',\n",
       " 'ace',\n",
       " 'ion',\n",
       " 'mah',\n",
       " 'batter',\n",
       " 'qualit',\n",
       " 'superieur',\n",
       " 'ideal',\n",
       " 'lautonom',\n",
       " 'portabl',\n",
       " 'reduit',\n",
       " 'voir',\n",
       " 'present']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_array_token = [line.split(\" \") for line in train_array]\n",
    "valid_array_token = [line.split(\" \") for line in valid_array]\n",
    "train_array_token[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction contient un grand nombre d' [arguments](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec). Le but de ce TP n'est pas d'optimiser les paramètres de ce modèle mais de le comprendre. Nous allons donc fixer quelques argument par défault : \n",
    "\n",
    "* Features_dimension = 300 : Dimension de l'espace des features (d'*embedding*) qui sera crée.\n",
    "* hs = 0\n",
    "* negative = 10\n",
    "\n",
    "**Q** A quoi servent les arguments *hs* et *negative*? Quels influences ces arguments ont sur le modèle avec les valeurs défini précédemment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Features_dimension = 300\n",
    "hs = 0\n",
    "negative = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons créer deux modèles :\n",
    "\n",
    "* Un modèle **skip-sgram**, sg = 1\n",
    "* Un modèle **CBOW**, sg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start learning skip-gram Word2Vec\n",
      "Learning time : 36.95 Word2Vec\n",
      "Start learning CBOW Word2Vec\n",
      "Learning time : 11.05 Word2Vec\n"
     ]
    }
   ],
   "source": [
    "sg = 1\n",
    "print(\"Start learning skip-gram Word2Vec\")\n",
    "ts = time.time()\n",
    "model_sg = gensim.models.Word2Vec(X, sg=sg, hs=hs, negative=negative, min_count=1, size=Features_dimension)\n",
    "te = time.time()\n",
    "t_learning = te-ts\n",
    "print(\"Learning time : %.2f Word2Vec\" %t_learning)\n",
    "\n",
    "\n",
    "sg = 0\n",
    "print(\"Start learning CBOW Word2Vec\")\n",
    "ts = time.time()\n",
    "model_cbow = gensim.models.Word2Vec(X, sg=sg, hs=hs, negative=negative, min_count=1, size=Features_dimension)\n",
    "te = time.time()\n",
    "t_learning = te-ts\n",
    "print(\"Learning time : %.2f Word2Vec\" %t_learning)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que dire du temps d'apprentissage de ces deux modèles? D'ou vient cette différence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Trained Model\n",
    "\n",
    "Comme pour les réseaux de convolution, des modèles pré-entrainés de Word2Vec éxistent également. \n",
    "Le plus célèbre et le plus utilisé étant [`GoogleNewsVectors`](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit) appris sur plus de 100 milliard de mots à partir des articles de GoogleNews. Cependant ce modèle est en anglais, et ne nous saura pas utile ici.\n",
    "\n",
    "On utilisera des modèle appris dans le projet suivant [https://github.com/Kyubyong/wordvectors](https://github.com/Kyubyong/wordvectors) appris sur 1Giga d'articles de wikipedia. \n",
    "\n",
    "Vous pouvez télécharger ce modèle en suivant ce [lien](https://we.tl/t-QMWo949gSf). Dezipez-le puis téléchargez le modèle en indiquant la direction du fichier \"fr/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_online_dir = \"data/fr/fr.bin\"\n",
    "#model_online_dir = \"ACOMPLETER/fr.bin\"\n",
    "model_online = gensim.models.Word2Vec.load(model_online_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most similar world\n",
    "\n",
    "Nous allons maintenant comparérer quelques propriétés de chacun de ces modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "stemmer=nltk.stem.SnowballStemmer('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.predict_output_word([\"homm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words for word : homme\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CBOW</th>\n",
       "      <th>skip-gram</th>\n",
       "      <th>online</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>femm</td>\n",
       "      <td>unisex</td>\n",
       "      <td>aventurier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unisex</td>\n",
       "      <td>chronograph</td>\n",
       "      <td>aristocrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arman</td>\n",
       "      <td>arman</td>\n",
       "      <td>garçon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bermud</td>\n",
       "      <td>umbro</td>\n",
       "      <td>vieillard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>diesel</td>\n",
       "      <td>army</td>\n",
       "      <td>délinquant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>emporio</td>\n",
       "      <td>emporio</td>\n",
       "      <td>écrivain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>norway</td>\n",
       "      <td>ralph</td>\n",
       "      <td>politicien</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>womag</td>\n",
       "      <td>chrono</td>\n",
       "      <td>escroc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>navy</td>\n",
       "      <td>lauren</td>\n",
       "      <td>orateur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>geographical</td>\n",
       "      <td>calvin</td>\n",
       "      <td>érudit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           CBOW    skip-gram       online\n",
       "0          femm       unisex   aventurier\n",
       "1        unisex  chronograph  aristocrate\n",
       "2         arman        arman       garçon\n",
       "3        bermud        umbro    vieillard\n",
       "4        diesel         army   délinquant\n",
       "5       emporio      emporio     écrivain\n",
       "6        norway        ralph   politicien\n",
       "7         womag       chrono       escroc\n",
       "8          navy       lauren      orateur\n",
       "9  geographical       calvin       érudit"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term=\"homme\"\n",
    "\n",
    "df_ = []\n",
    "columns = []\n",
    "for model_name, model in model_dic.items():\n",
    "    token = stemmer.stem(term) if \"online\"!=model_name else term\n",
    "    mpow = model.wv.most_similar([token])\n",
    "    if mpow is None:\n",
    "        df_.append([\"\" for k in range(10)])\n",
    "    else:\n",
    "        df_.append([k[0] for k in mpow])\n",
    "    columns.append(model_name)\n",
    "print(\"Most similar words for word : \"+term)\n",
    "pd.DataFrame(np.array(df_).T, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice** Affichez les résultats pour d'autres mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CBOW</th>\n",
       "      <th>skip-gram</th>\n",
       "      <th>online</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>romant</td>\n",
       "      <td>jungl</td>\n",
       "      <td>reine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rev</td>\n",
       "      <td>savan</td>\n",
       "      <td>belle-mère</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lapin</td>\n",
       "      <td>marionnet</td>\n",
       "      <td>brunehilde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jungl</td>\n",
       "      <td>joyeux</td>\n",
       "      <td>régente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eleph</td>\n",
       "      <td>felin</td>\n",
       "      <td>reine-mère</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>runeun</td>\n",
       "      <td>paon</td>\n",
       "      <td>belle-sœur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>coquelicot</td>\n",
       "      <td>lilipinso</td>\n",
       "      <td>veuve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>amour</td>\n",
       "      <td>ruin</td>\n",
       "      <td>mère</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>halloween</td>\n",
       "      <td>tendress</td>\n",
       "      <td>demi-sœur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>calin</td>\n",
       "      <td>capelin</td>\n",
       "      <td>nièce</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CBOW  skip-gram      online\n",
       "0      romant      jungl       reine\n",
       "1         rev      savan  belle-mère\n",
       "2       lapin  marionnet  brunehilde\n",
       "3       jungl     joyeux     régente\n",
       "4       eleph      felin  reine-mère\n",
       "5      runeun       paon  belle-sœur\n",
       "6  coquelicot  lilipinso       veuve\n",
       "7       amour       ruin        mère\n",
       "8   halloween   tendress   demi-sœur\n",
       "9       calin    capelin       nièce"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_positif = [\"femme\",\"roi\"]\n",
    "terms_negatif = [\"homme\"]\n",
    "\n",
    "df_ = []\n",
    "columns = []\n",
    "for model_name, model in model_dic.items():\n",
    "    token_positif = [stemmer.stem(term) if \"online\"!=model_name else term for term in terms_positif]\n",
    "    token_negativ = [stemmer.stem(term) if \"online\"!=model_name else term for term in terms_negatif]\n",
    "    \n",
    "    mpow = model.wv.most_similar(positive=token_positif, negative=token_negativ)\n",
    "    if mpow is None:\n",
    "        df_.append([\"\" for k in range(10)])\n",
    "    else:\n",
    "        df_.append([k[0] for k in mpow])\n",
    "    columns.append(model_name)\n",
    "pd.DataFrame(np.array(df_).T, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CBOW</th>\n",
       "      <th>skip-gram</th>\n",
       "      <th>online</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sweetness</td>\n",
       "      <td>dublin</td>\n",
       "      <td>ségovie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flex</td>\n",
       "      <td>arizon</td>\n",
       "      <td>madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flocon</td>\n",
       "      <td>zurich</td>\n",
       "      <td>alicante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>etire</td>\n",
       "      <td>autrich</td>\n",
       "      <td>aix-en-provence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>opta</td>\n",
       "      <td>copenhagen</td>\n",
       "      <td>ancône</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>capitonnag</td>\n",
       "      <td>buffalo</td>\n",
       "      <td>séville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>canc</td>\n",
       "      <td>janeiro</td>\n",
       "      <td>malaga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>liberty</td>\n",
       "      <td>foxy</td>\n",
       "      <td>andalousie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cercl</td>\n",
       "      <td>atlant</td>\n",
       "      <td>alger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sophistique</td>\n",
       "      <td>toronto</td>\n",
       "      <td>avignon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          CBOW   skip-gram           online\n",
       "0    sweetness      dublin          ségovie\n",
       "1         flex      arizon           madrid\n",
       "2       flocon      zurich         alicante\n",
       "3        etire     autrich  aix-en-provence\n",
       "4         opta  copenhagen           ancône\n",
       "5   capitonnag     buffalo          séville\n",
       "6         canc     janeiro           malaga\n",
       "7      liberty        foxy       andalousie\n",
       "8        cercl      atlant            alger\n",
       "9  sophistique     toronto          avignon"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_positif = [\"espagne\",\"paris\"]\n",
    "terms_negatif = [\"france\"]\n",
    "\n",
    "df_ = []\n",
    "columns = []\n",
    "for model_name, model in model_dic.items():\n",
    "    token_positif = [stemmer.stem(term) if \"online\"!=model_name else term for term in terms_positif]\n",
    "    token_negativ = [stemmer.stem(term) if \"online\"!=model_name else term for term in terms_negatif]\n",
    "    \n",
    "    mpow = model.wv.most_similar(positive=token_positif, negative=token_negativ)\n",
    "    if mpow is None:\n",
    "        df_.append([\"\" for k in range(10)])\n",
    "    else:\n",
    "        df_.append([k[0] for k in mpow])\n",
    "    columns.append(model_name)\n",
    "pd.DataFrame(np.array(df_).T, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict output word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [\"voir\",\"la\"]\n",
    "\n",
    "\n",
    "df_ = []\n",
    "columns = []\n",
    "for model_name, model in model_dic.items():\n",
    "    tokens = [stemmer.stem(term) if \"online\"!=model_name else term for term in terms]\n",
    "    \n",
    "    mpow = model.predict_output_word(tokens)\n",
    "    if mpow is None:\n",
    "        df_.append([\"\" for k in range(10)])\n",
    "    else:\n",
    "        df_.append([k[0] for k in mpow])\n",
    "    columns.append(model_name)\n",
    "pd.DataFrame(np.array(df_).T, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [\"coque\",'pour',\"samsung\"]\n",
    "\n",
    "\n",
    "df_ = []\n",
    "columns = []\n",
    "for model_name, model in model_dic.items():\n",
    "    tokens = [stemmer.stem(term) if \"online\"!=model_name else term for term in terms]\n",
    "    \n",
    "    mpow = model.predict_output_word(tokens)\n",
    "    if mpow is None:\n",
    "        df_.append([\"\" for k in range(10)])\n",
    "    else:\n",
    "        df_.append([k[0] for k in mpow])\n",
    "    columns.append(model_name)\n",
    "pd.DataFrame(np.array(df_).T, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [\"homme\"]\n",
    "\n",
    "\n",
    "df_ = []\n",
    "columns = []\n",
    "for model_name, model in model_dic.items():\n",
    "    tokens = [stemmer.stem(term) if \"online\"!=model_name else term for term in terms]\n",
    "    \n",
    "    mpow = model.predict_output_word(tokens)\n",
    "    if mpow is None:\n",
    "        df_.append([\"\" for k in range(10)])\n",
    "    else:\n",
    "        df_.append([k[0] for k in mpow])\n",
    "    columns.append(model_name)\n",
    "pd.DataFrame(np.array(df_).T, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [\"femme\"]\n",
    "\n",
    "\n",
    "df_ = []\n",
    "columns = []\n",
    "for model_name, model in model_dic.items():\n",
    "    tokens = [stemmer.stem(term) if \"online\"!=model_name else term for term in terms]\n",
    "    \n",
    "    mpow = model.predict_output_word(tokens)\n",
    "    if mpow is None:\n",
    "        df_.append([\"\" for k in range(10)])\n",
    "    else:\n",
    "        df_.append([k[0] for k in mpow])\n",
    "    columns.append(model_name)\n",
    "pd.DataFrame(np.array(df_).T, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_valid_clean = pd.read_csv(\"data/cdiscount_valid_clean.csv\").fillna(\"\")\n",
    "data_train_clean = pd.read_csv(\"data/cdiscount_train_clean.csv\").fillna(\"\")\n",
    "\n",
    "train_array_token_wstem = [line.split(\" \") for line in data_train_clean[\"Description\"].values]\n",
    "valid_array_token_wstem = [line.split(\" \") for line in data_valid_clean[\"Description\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features_mean(lines):\n",
    "    features = [MODEL[x] for x  in lines if x in MODEL]\n",
    "    if features == []:   \n",
    "        fm =np.ones(F_SIZE)\n",
    "    else :\n",
    "        fm = np.mean(features,axis=0)\n",
    "    return fm\n",
    "\n",
    "def get_matrix_features_means(X):\n",
    "    X_embedded_ = list(map(get_features_mean, X))\n",
    "    X_embedded = np.vstack(X_embedded_)\n",
    "    return X_embedded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in [\"CBOW\",\"skip-gram\", \"online\"]:\n",
    "    \n",
    "    if \"online\" == model_name:\n",
    "        X_train = train_array_token_wstem\n",
    "        X_valid = valid_array_token_wstem\n",
    "    else:\n",
    "        X_train = train_array_token\n",
    "        X_valid = valid_array_token\n",
    "    \n",
    "    model = model_dic[model_name]\n",
    "    MODEL = model\n",
    "    F_SIZE = Features_dimension\n",
    "\n",
    "    ts = time.time()\n",
    "    X_embedded_train = get_matrix_features_means(X_train)\n",
    "    te = time.time()\n",
    "    t_train = te-ts\n",
    "    #np.save(embedded_train_dir, X_embedded_train)\n",
    "    print(\"Time conversion : %d seconds\"%t_train)\n",
    "    print(\"Shape Matrix : (%d,%d)\"%X_embedded_train.shape)\n",
    "    np.save(DATA_OUTPUT_DIR +\"/embedded_train_nb_hash_\"+model_name, X_embedded_train)\n",
    "\n",
    "    \n",
    "    ts = time.time()\n",
    "    X_embedded_valid = get_matrix_features_means(X_valid)\n",
    "    te = time.time()\n",
    "    t_valid = te-ts\n",
    "    #np.save(embedded_valid_dir, X_embedded_valid)\n",
    "    print(\"Time conversion : %d seconds\"%t_valid)\n",
    "    print(\"Shape Matrix : (%d,%d)\"%X_embedded_valid.shape)\n",
    "    np.save(DATA_OUTPUT_DIR +\"/embedded_valid_nb_hash_\"+model_name, X_embedded_valid)\n",
    "    \n",
    "    metadata = {\"t_train\" : t_train, \"t_valid\" : t_valid, \"sg\":sg}\n",
    "    print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[\"homme\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.syn0[model.wv.vocab[\"homme\"].index] == model[\"homme\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.syn1neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vh = model.wv.vocab[\"homme\"]\n",
    "vh.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:TPInsa]",
   "language": "python",
   "name": "conda-env-TPInsa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "nav_menu": {
    "height": "279px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
