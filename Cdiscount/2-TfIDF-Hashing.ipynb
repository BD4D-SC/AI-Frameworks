{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 250px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bguillouet/anaconda/envs/TPInsa/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Importation des librairies utilisées\n",
    "import unicodedata \n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "import collections\n",
    "import itertools\n",
    "import csv\n",
    "import warnings\n",
    "import scipy\n",
    "\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Répertoire de travail\n",
    "DATA_DIR = \"/Users/bguillouet/Insa/TP_Insa/data/cdiscount_NLP/\"\n",
    "DATA_TFIDF_DIR = \"/Users/bguillouet/Insa/TP_Insa/data/cdiscount_NLP/tfidf/\"\n",
    "\n",
    "# Nom des fichiers\n",
    "training_reduit_path = DATA_DIR + \"Categorie_reduit.csv\"\n",
    "# Variable Globale\n",
    "HEADER_TEST = ['Description','Libelle','Marque']\n",
    "HEADER_TRAIN =['Categorie1','Categorie2','Categorie3','Description','Libelle','Marque']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Construction des caractéristiques ou *features* (TF-IDF)¶\n",
    "### Introduction\n",
    "La vectorisation, c'est-à-dire la construction des caractéristiques à partir de la liste des mots se fait en 2 étapes:\n",
    "* **Hashage**. Il permet de réduire l'espace des variables (taille du dictionnaire) en un nombre limité et fixé a priori `n_hash` de caractéristiques. Il repose sur la définition d'une fonction de hashage, $h$ qui à un indice $j$ défini dans l'espace des entiers naturels, renvoie un indice $i=h(j)$ dans dans l'espace réduit (1 à n_hash) des caractéristiques. Ainsi le poids de l'indice $i$, du nouvel espace, est l'association de tous les poids d'indice $j$ tels que $i=h(j)$ de l'espace originale. Ici, les poids sont associés d'après la méthode décrite par Weinberger et al. (2009).\n",
    "\n",
    "N.B. $h$ n'est pas généré aléatoirement. Ainsi pour un même fichier d'apprentissage (ou de test) et pour un même entier n_hash, le résultat de la fonction de hashage est identique\n",
    "\n",
    "* **TF-IDF**. Le TF-IDF permet de faire ressortir l'importance relative de chaque mot $m$ (ou couples de mots consécutifs) dans un texte-produit ou un descriptif $d$, par rapport à la liste entière des produits. La fonction $TF(m,d)$ compte le nombre d'occurences du mot $m$ dans le descriptif $d$. La fonction $IDF(m)$ mesure l'importance du terme dans l'ensemble des documents ou descriptifs en donnant plus de poids aux termes les moins fréquents car considérés comme les plus discriminants (motivation analogue à celle de la métrique du chi2 en anamlyse des correspondance). $IDF(m,l)=\\log\\frac{D}{f(m)}$ où $D$ est le nombre de documents, la taille de l'échantillon d'apprentissage, et $f(m)$ le nombre de documents ou descriptifs contenant le mot $m$. La nouvelle variable ou *features* est $V_m(l)=TF(m,l)\\times IDF(m,l)$.\n",
    "\n",
    "* Comme pour les transformations des variables quantitatives (centrage, réduction), la même transformation c'est-à-dire les mêmes pondérations, est calculée sur l'achantillon d'apprentissage et appliquée à celui de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction de Vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Création d’une matrice indiquant\n",
    "## les fréquences des mots contenus dans chaque description\n",
    "## de nombreux paramètres seraient à tester\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "\n",
    "def vectorizer_train(df, columns=['Description', 'Libelle', 'Marque'], nb_hash=None, stop_words=None):\n",
    "    \n",
    "    # Hashage\n",
    "    if nb_hash is None:\n",
    "        data_hash = map(lambda x : \" \".join(x), df[columns].values)\n",
    "        feathash = None\n",
    "            # TFIDF\n",
    "        vec = TfidfVectorizer(\n",
    "            min_df = 1,\n",
    "            stop_words = stop_words,\n",
    "            smooth_idf=True,\n",
    "            norm='l2',\n",
    "            sublinear_tf=True,\n",
    "            use_idf=True,\n",
    "            ngram_range=(1,1)) #bi-grams\n",
    "        tfidf = vec.fit_transform(data_hash)\n",
    "    else:\n",
    "        df_text = map(lambda x : collections.Counter(\" \".join(x).split(\" \")), df[columns].values)\n",
    "        feathash = FeatureHasher(nb_hash)\n",
    "        data_hash = feathash.fit_transform(map(collections.Counter,df_text))\n",
    "        \n",
    "        vec =  TfidfTransformer(use_idf=True,\n",
    "                            smooth_idf=True, sublinear_tf=False)\n",
    "        tfidf =  vec.fit_transform(data_hash)\n",
    "\n",
    "    return vec, feathash, tfidf\n",
    "\n",
    "\n",
    "\n",
    "def apply_vectorizer(df, vec, columns =['Description', 'Libelle', 'Marque'], feathash = None ):\n",
    "    \n",
    "    #Hashage\n",
    "    if feathash is None:\n",
    "        data_hash = map(lambda x : \" \".join(x), df[columns].values)\n",
    "    else:\n",
    "        df_text = map(lambda x : collections.Counter(\" \".join(x).split(\" \")), df[columns].values)\n",
    "        data_hash = feathash.transform(df_text)\n",
    "    \n",
    "    # TFIDF\n",
    "    tfidf=vec.transform(data_hash)\n",
    "    return tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "300\n",
      "1000\n",
      "10000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "data_valid_clean = pd.read_csv(DATA_DIR + \"Categorie_reduit_valid_clean.csv\").fillna(\"\")\n",
    "data_train_clean = pd.read_csv(DATA_DIR + \"Categorie_reduit_train_clean.csv\").fillna(\"\")\n",
    "\n",
    "#for nb_hash in [30]:\n",
    "for nb_hash in [None, 300, 1000, 10000, 100000]:\n",
    "    print(nb_hash)\n",
    "    vec, feathash, X = vectorizer_train(data_train_clean, columns=[\"Description\"], nb_hash = nb_hash)\n",
    "    Xv = apply_vectorizer(data_valid_clean, vec, feathash=feathash)\n",
    "    scipy.sparse.save_npz(DATA_TFIDF_DIR + \"Embedded_idf_Categorie_reduit_train_clean_N_hash_\"+str(nb_hash), X)\n",
    "    scipy.sparse.save_npz(DATA_TFIDF_DIR + \"Embedded_idf_Categorie_reduit_valid_clean_N_hash_\"+str(nb_hash), Xv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "300\n",
      "1000\n",
      "10000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "data_valid_clean = pd.read_csv(DATA_DIR + \"Categorie_reduit_valid_clean_stem.csv\").fillna(\"\")\n",
    "data_train_clean = pd.read_csv(DATA_DIR + \"Categorie_reduit_train_clean_stem.csv\").fillna(\"\")\n",
    "\n",
    "#for nb_hash in [30]:\n",
    "for nb_hash in [None, 300, 1000, 10000, 100000]:\n",
    "    print(nb_hash)\n",
    "    vec, feathash, X = vectorizer_train(data_train_clean, columns=[\"Description\"], nb_hash = nb_hash)\n",
    "    Xv = apply_vectorizer(data_valid_clean, vec, feathash=feathash)\n",
    "    scipy.sparse.save_npz(DATA_TFIDF_DIR + \"Embedded_idf_Categorie_reduit_train_clean_N_hash_stem_\"+str(nb_hash), X)\n",
    "    scipy.sparse.save_npz(DATA_TFIDF_DIR + \"Embedded_idf_Categorie_reduit_valid_clean_N_hash_stem_\"+str(nb_hash), Xv)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [TPInsa]",
   "language": "python",
   "name": "Python [TPInsa]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "nav_menu": {
    "height": "279px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
