{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies de l'intelligence Artificielle](https://github.com/wikistat/AI-Frameworks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" width=400, style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement Naturel du Langage (NLP) : Catégorisation de Produits Cdiscount\n",
    "\n",
    "Il s'agit d'une version simplifiée du concours proposé par Cdiscount et paru sur le site [datascience.net](https://www.datascience.net/fr/challenge). Les données d'apprentissage sont accessibles sur demande auprès de Cdiscount mais les solutions de l'échantillon test du concours ne sont pas et ne seront pas rendues publiques. Un échantillon test est donc construit pour l'usage de ce tutoriel.  L'objectif est de prévoir la catégorie d'un produit à partir de son descriptif (*text mining*). Seule la catégorie principale (1er niveau, 47 classes) est prédite au lieu des trois niveaux demandés dans le concours. L'objectif est plutôt de comparer les performances des méthodes et technologies en fonction de la taille de la base d'apprentissage ainsi que d'illustrer sur un exemple complexe le prétraitement de données textuelles. \n",
    "\n",
    "Le jeux de données complet (15M produits) permet un test en vrai grandeur du **passage à l'échelle volume** des phases de préparation (*munging*), vectorisation (hashage, TF-IDF) et d'apprentissage en fonction de la technologie utilisée.\n",
    "\n",
    "La synthèse des résultats obtenus est développée par [Besse et al. 2016](https://hal.archives-ouvertes.fr/hal-01350099) (section 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1-3 : Modèle d'apprentissage statistiques.\n",
    "\n",
    "Dans le calepin numéro 2, nous avons créés 2x7 matrices de features correspondant au mêmes échantillons d'apprentissage et de validation des données textuelles de description d'objet de Cdiscount.  Ces matrices ont été crées avec les méthodes suivantes. \n",
    "\n",
    "1. `Count_Vectorizer`. `No hashing`.\n",
    "2. `Count_Vectorizer`. `Hashing = 300`.\n",
    "3. `TFIDF_vectorizer`. `No hashing`. \n",
    "4. `TFIDF_vectorizer`. `Hashing = 300`.\n",
    "5. `Word2Vec`. `CBOW`\n",
    "6. `Word2Vec`. `Skip-Gram`\n",
    "7. `Word2Vec`. `Pre-trained`\n",
    "\n",
    "Nous allons maintenant étudiés les performances d'algorithmes de *machine learning* (`Regression logistique`, `Forêts aléatoire`, `Perceptron multicouche`) sur ces différents features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importation des librairies utilisées\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "DATA_DIR = \"data/features\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Téléchargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Téléchargement des variables réponses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = pd.read_csv(\"data/cdiscount_train_subset.csv\").fillna(\"\")[\"Categorie1\"]\n",
    "Y_valid = pd.read_csv(\"data/cdiscount_valid.csv\").fillna(\"\")[\"Categorie1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un dictionnaire contenant les chemins ou des différents objets où sont stockés les matrices de features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path_dic = {}\n",
    "\n",
    "parameters = [[\"count_no_hashing\", None, \"count\"],\n",
    "              [\"count_300\", 300, \"count\"],\n",
    "              [\"tfidf_no_hashing\", None, \"tfidf\"],\n",
    "              [\"tfidf_300\",300, \"tfidf\"]]\n",
    "for name, nb_hash, vectorizer in parameters:\n",
    "        x_train_path = DATA_DIR +\"/vec_train_nb_hash_\" + str(nb_hash) + \"_vectorizer_\" + str(vectorizer)+\".npz\"\n",
    "        x_valid_path = DATA_DIR +\"/vec_valid_nb_hash_\" + str(nb_hash) + \"_vectorizer_\" + str(vectorizer)+\".npz\"\n",
    "        dic = {\"x_train_path\" : x_train_path, \"x_valid_path\" : x_valid_path, \"load\" : \"npz\"}\n",
    "        features_path_dic.update({name : dic})\n",
    " \n",
    "parametersw2v = [[\"word2vec_cbow\",\"cbow\"],\n",
    "                 [\"word2vec_sg\",\"sg\"],\n",
    "                 [\"word2vec_online\",\"online\"]]\n",
    "for name, mtype in parametersw2v:\n",
    "        x_train_path = DATA_DIR +\"/embedded_train_\" + mtype+\".npy\"\n",
    "        x_valid_path = DATA_DIR +\"/embedded_valid_\" + mtype+\".npy\"\n",
    "        dic = {\"x_train_path\" : x_train_path, \"x_valid_path\" : x_valid_path, \"load\" : \"npy\"}\n",
    "        features_path_dic.update({name : dic})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load features : word2vec_sg\n",
      "start Learning :word2vec_sg\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ................... C=10, score=0.8477807942420608, total= 7.2min\n",
      "[CV] C=10 ............................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  7.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=10, score=0.8506473002841806, total= 7.0min\n",
      "[CV] C=10 ............................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 14.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=10, score=0.851478579552635, total= 7.3min\n",
      "[CV] C=1 .............................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 21.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=1, score=0.8380579582044321, total= 3.8min\n",
      "[CV] C=1 .............................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 25.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=1, score=0.8404799494790022, total= 3.8min\n",
      "[CV] C=1 .............................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 29.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=1, score=0.8414634146341463, total= 4.0min\n",
      "[CV] C=0.1 ...........................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 33.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=0.1, score=0.8146978975945451, total= 2.6min\n",
      "[CV] C=0.1 ...........................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 35.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. C=0.1, score=0.8148089674771076, total= 2.5min\n",
      "[CV] C=0.1 ...........................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 38.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=0.1, score=0.818431694679641, total= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 40.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 40.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start prediction :word2vec_sg\n",
      "Load features : word2vec_cbow\n",
      "start Learning :word2vec_cbow\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] C=10 ............................................................\n",
      "[CV] ..................... C=10, score=0.81049940021466, total=20.1min\n",
      "[CV] C=10 ............................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 20.1min remaining:    0.0s\n"
     ]
    }
   ],
   "source": [
    "metadata_list_lr = []\n",
    "\n",
    "param_grid = {\"C\" : [10,1,0.1]}\n",
    "#param_grid = {\"C\" : [1]}\n",
    "\n",
    "for name, dic in features_path_dic.items():\n",
    "    \n",
    "    x_train_path = dic[\"x_train_path\"]\n",
    "    x_valid_path = dic[\"x_valid_path\"]\n",
    "    load = dic[\"load\"]\n",
    "    \n",
    "    print(\"Load features : \" + name)\n",
    "    if load == \"npz\":\n",
    "        X_train = sc.sparse.load_npz(x_train_path)\n",
    "        X_valid = sc.sparse.load_npz(x_valid_path)\n",
    "    else : \n",
    "        X_train = np.load(x_train_path)\n",
    "        X_valid = np.load(x_valid_path)\n",
    "    \n",
    "    print(\"start Learning :\" + name)\n",
    "    ts = time.time()\n",
    "    gs = GridSearchCV(LogisticRegression(), param_grid=param_grid, verbose=15)\n",
    "    gs.fit(X_train,Y_train.values)\n",
    "    te=time.time()\n",
    "    t_learning = te-ts\n",
    "    \n",
    "    print(\"start prediction :\" + name)\n",
    "    ts = time.time()\n",
    "    score_train=gs.score(X_train,Y_train)\n",
    "    score_valid=gs.score(X_valid,Y_valid)\n",
    "    te=time.time()\n",
    "    t_predict = te-ts\n",
    "    \n",
    "    metadata = {\"name\":name, \"learning_time\" : t_learning, \"predict_time\":t_predict, \"score_train\": score_train, \"score_valid\": score_valid}\n",
    "    metadata_list_lr.append(metadata)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "\n",
    "for model_name in [\"CBOW\",\"skip-gram\", \"online\"]:\n",
    "    print(\"Word2Vec :\" + model_name)\n",
    "\n",
    "    X_train = np.load(DATA_DIR +\"/embedded_train_nb_hash_\" + model_name+\".npy\")\n",
    "    X_valid = np.load(DATA_DIR +\"/embedded_valid_nb_hash_\" + model_name+\".npy\")\n",
    "    \n",
    "    ts = time.time()\n",
    "    cla = LogisticRegression()\n",
    "    cla.fit(X_train,Y_train.values)\n",
    "    te=time.time()\n",
    "    t_learning = te-ts\n",
    "    ts = time.time()\n",
    "    score_train=cla.score(X_train,Y_train)\n",
    "    score_valid=cla.score(X_valid,Y_valid)\n",
    "    te=time.time()\n",
    "    t_predict = te-ts\n",
    "    metadata = {\"typeW2V\": model_name ,\"nb_hash\": None, \"vectorizer\":\"word2vec\" ,\"learning_time\" : t_learning, \"predict_time\":t_predict, \"score_train\": score_train, \"score_valid\": score_valid}\n",
    "    print(metadata)\n",
    "    metadata_list_lr.append(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(metadata_list_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "metadata_list_rf = []\n",
    "\n",
    "parameters = [[None, \"count\"],\n",
    "              [300, \"count\"],\n",
    "              [10000, \"count\"],\n",
    "              [None, \"tfidf\"],\n",
    "              [300, \"tfidf\"],\n",
    "              [10000, \"tfidf\"],]\n",
    "\n",
    "for nb_hash, vectorizer in parameters:\n",
    "    print(\"nb_hash : \" + str(nb_hash) + \", vectorizer : \" + str(vectorizer))\n",
    "    X_train = sparse.load_npz(DATA_DIR +\"/vec_train_nb_hash_\" + str(nb_hash) + \"_vectorizer_\" + str(vectorizer)+\".npz\")\n",
    "    X_valid = sparse.load_npz(DATA_DIR +\"/vec_valid_nb_hash_\" + str(nb_hash) + \"_vectorizer_\" + str(vectorizer)+\".npz\")\n",
    "    ts = time.time()\n",
    "    cla = RandomForestClassifier(n_estimators=100)\n",
    "    cla.fit(X_train,Y_train.values)\n",
    "    te=time.time()\n",
    "    t_learning = te-ts\n",
    "    ts = time.time()\n",
    "    score_train=cla.score(X_train,Y_train)\n",
    "    score_valid=cla.score(X_valid,Y_valid)\n",
    "    te=time.time()\n",
    "    t_predict = te-ts\n",
    "    metadata = {\"typeW2V\": None, \"nb_hash\": nb_hash, \"vectorizer\":vectorizer , \"learning_time\" : t_learning, \"predict_time\":t_predict, \"score_train\": score_train, \"score_valid\": score_valid}\n",
    "    print(metadata)\n",
    "    metadata_list_rf.append(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "\n",
    "for model_name in [\"CBOW\",\"skip-gram\", \"online\"]:\n",
    "    print(\"Word2Vec :\" + model_name)\n",
    "\n",
    "    X_train = np.load(DATA_DIR +\"/embedded_train_nb_hash_\" + model_name+\".npy\")\n",
    "    X_valid = np.load(DATA_DIR +\"/embedded_valid_nb_hash_\" + model_name+\".npy\")\n",
    "    \n",
    "    ts = time.time()\n",
    "    cla = RandomForestClassifier(n_estimators=100)\n",
    "    cla.fit(X_train,Y_train.values)\n",
    "    te=time.time()\n",
    "    t_learning = te-ts\n",
    "    ts = time.time()\n",
    "    score_train=cla.score(X_train,Y_train)\n",
    "    score_valid=cla.score(X_valid,Y_valid)\n",
    "    te=time.time()\n",
    "    t_predict = te-ts\n",
    "    metadata = {\"typeW2V\": model_name ,\"nb_hash\": None, \"vectorizer\":\"word2vec\" ,\"learning_time\" : t_learning, \"predict_time\":t_predict, \"score_train\": score_train, \"score_valid\": score_valid}\n",
    "    print(metadata)\n",
    "    metadata_list_rf.append(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(metadata_list_lr)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:TPInsa]",
   "language": "python",
   "name": "conda-env-TPInsa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
