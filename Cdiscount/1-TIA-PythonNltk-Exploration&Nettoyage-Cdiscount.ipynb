{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 250px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration et Nettoyage des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importation des librairies utilisées\n",
    "import unicodedata \n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "import collections\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous utilisez la librairie `nltk` pour la première fois, il est nécessaire d'utiliser la commande suivante. Cette commande permet de télécharger de nombreux corpus de texte, mais également des informations grammaticales sur différentes langues. Information notamment nécessaire à l'étape de racinisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des données\n",
    "\n",
    "Le jeu de données utilisé ici est un sous ensemble du jeu de données fourni par par CDiscount et paru sur le site [datascience.net](https://www.datascience.net/fr/challenge). \n",
    "Les données d'apprentissage sont accessibles sur demande auprès de CDiscount. Les solutions de l'échantillon test du concours ne sont pas et ne seront pas rendues publiques. Un échantillon test est donc construit pour l'usage de ce tutoriel.  L'objectif est de prévoir la catégorie d'un produit à partir de son descriptif. Seule la catégorie principale (1er niveau) est prédite au lieu des trois niveaux demandés dans le concours. L'objectif est plutôt de comparer les performances des méthodes et technologies en fonction de la taille de la base d'apprentissage ainsi que d'illustrer sur un exemple complexe le prétraitement de données textuelles. La stratégie de sous ou sur échantillonnage des catégories qui permet d'améliorer la prévision n'a pas été mise en oeuvre.\n",
    "* L'exemple est présenté sur un échantillon réduit d'un million de produits au lieu des 15M initiaux\n",
    "* L'échantillon réduit peut encore l'être puis séparé en 2 parties: apprentissage et validation. \n",
    "* Les données textuelles sont  nettoyées, racinisées, vectorisées avant modélisation.\n",
    "* Trois modélisations sont estimées: logistique, arbre, forêt aléatoire.\n",
    "* Optimiser l'erreur en faisant varier différents paramètres: types et paramètres de vectorisation (TF-IDF), paramètres de la régression logistique (pénalisation l1) et de la forêt aléatoire (nombre d'arbres et nombre de variables aléatoire).\n",
    "\n",
    "Exécuter finalement le code pour différentes tailles (paramètre  `tauxTot` ci-dessous) de l'échantillon d'apprentissage et comparer les qualités de prévision obtenues. \n",
    "\n",
    "Deux échantillons de test ont été mis de côté et seront utilisés dans un prochain calepin (avec pyspark) pour comparer les stratégies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nom des fichiers\n",
    "input_path = \"data/cdiscount.csv\"\n",
    "\n",
    "data = pd.read_csv(input_path,sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Read & Split Dataset\n",
    "   Fonction permettant de lire le fichier d'apprentissage et de créer deux DataFrame Pandas, un pour l'apprentissage, l'autre pour la validation.\n",
    "   La première méthode créée un DataFrame en lisant entièrement le fichier. Puis elle scinde le DataFrame en deux  grâce à la fonction dédiée de sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Takes 6 s\n",
      "Train set : 900000 elements, Test set : 100000 elements\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categorie1</th>\n",
       "      <th>Categorie2</th>\n",
       "      <th>Categorie3</th>\n",
       "      <th>Description</th>\n",
       "      <th>Libelle</th>\n",
       "      <th>Marque</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>146828</th>\n",
       "      <td>ELECTROMENAGER</td>\n",
       "      <td>GROS APPAREILS LAVAGE-SECHAGE</td>\n",
       "      <td>ACCESSOIRES ET PIECES - LAVAGE-SECHAGE</td>\n",
       "      <td>Ressort fixation manchette - Ressort fixation ...</td>\n",
       "      <td>Ressort fixation manchette</td>\n",
       "      <td>BRANDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681349</th>\n",
       "      <td>DECO - LINGE - LUMINAIRE</td>\n",
       "      <td>DECORATION MURALE - TABLEAU - CADRE PHOTO - ST...</td>\n",
       "      <td>TABLEAU - TOILE</td>\n",
       "      <td>Tableau deco moderne   BROOKLYN and MANHATTAN ...</td>\n",
       "      <td>Tableau deco moderne   BROOKLYN and MANHATTAN ...</td>\n",
       "      <td>ELITE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317614</th>\n",
       "      <td>LIBRAIRIE</td>\n",
       "      <td>AUTRES LIVRES</td>\n",
       "      <td>AUTRES LIVRES</td>\n",
       "      <td>De Germinal aux éditions SOCIETE DES ECRIVAINS</td>\n",
       "      <td>RETRAITES ET SANTE : POIL A GRATTER</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209372</th>\n",
       "      <td>TELEPHONIE - GPS</td>\n",
       "      <td>ACCESSOIRE TELEPHONE</td>\n",
       "      <td>COQUE - BUMPER - FACADE TELEPHONE</td>\n",
       "      <td>Coque souple Noire pour LG G3 D851 motif Drape...</td>\n",
       "      <td>Coque souple Noire pour LG G3 D851 motif Drapea…</td>\n",
       "      <td>MUZZANO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15259</th>\n",
       "      <td>TELEPHONIE - GPS</td>\n",
       "      <td>ACCESSOIRE TELEPHONE</td>\n",
       "      <td>COQUE - BUMPER - FACADE TELEPHONE</td>\n",
       "      <td>Coque de protection rigide. Design original d'...</td>\n",
       "      <td>Coque Galaxy S4 Design Skull and bones raiddog</td>\n",
       "      <td>SKINKIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Categorie1  \\\n",
       "146828            ELECTROMENAGER   \n",
       "681349  DECO - LINGE - LUMINAIRE   \n",
       "317614                 LIBRAIRIE   \n",
       "209372          TELEPHONIE - GPS   \n",
       "15259           TELEPHONIE - GPS   \n",
       "\n",
       "                                               Categorie2  \\\n",
       "146828                     GROS APPAREILS LAVAGE-SECHAGE    \n",
       "681349  DECORATION MURALE - TABLEAU - CADRE PHOTO - ST...   \n",
       "317614                                      AUTRES LIVRES   \n",
       "209372                               ACCESSOIRE TELEPHONE   \n",
       "15259                                ACCESSOIRE TELEPHONE   \n",
       "\n",
       "                                    Categorie3  \\\n",
       "146828  ACCESSOIRES ET PIECES - LAVAGE-SECHAGE   \n",
       "681349                         TABLEAU - TOILE   \n",
       "317614                           AUTRES LIVRES   \n",
       "209372       COQUE - BUMPER - FACADE TELEPHONE   \n",
       "15259        COQUE - BUMPER - FACADE TELEPHONE   \n",
       "\n",
       "                                              Description  \\\n",
       "146828  Ressort fixation manchette - Ressort fixation ...   \n",
       "681349  Tableau deco moderne   BROOKLYN and MANHATTAN ...   \n",
       "317614     De Germinal aux éditions SOCIETE DES ECRIVAINS   \n",
       "209372  Coque souple Noire pour LG G3 D851 motif Drape...   \n",
       "15259   Coque de protection rigide. Design original d'...   \n",
       "\n",
       "                                                  Libelle   Marque  \n",
       "146828                         Ressort fixation manchette   BRANDT  \n",
       "681349  Tableau deco moderne   BROOKLYN and MANHATTAN ...    ELITE  \n",
       "317614                RETRAITES ET SANTE : POIL A GRATTER           \n",
       "209372   Coque souple Noire pour LG G3 D851 motif Drapea…  MUZZANO  \n",
       "15259      Coque Galaxy S4 Design Skull and bones raiddog  SKINKIN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_dataset(input_path, nb_line, tauxValid,columns):\n",
    "    time_start = time.time()\n",
    "    data_all = pd.read_csv(input_path,sep=\",\")\n",
    "    data_all = data_all.fillna(\"\")\n",
    "    data_train, data_valid = train_test_split(data_all, test_size = tauxValid)\n",
    "    time_end = time.time()\n",
    "    print(\"Split Takes %d s\" %(time_end-time_start))\n",
    "    return data_train, data_valid\n",
    "\n",
    "nb_line=10000000  # part totale extraite du fichier initial ici déjà réduit\n",
    "data_train, data_valid = split_dataset(training_reduit_path, nb_line, tauxValid, HEADER_TRAIN)\n",
    "# Cette ligne permet de visualiser les 5 premières lignes de la DataFrame \n",
    "N_train = data_train.shape[0]\n",
    "N_valid = data_valid.shape[0]\n",
    "print(\"Train set : %d elements, Test set : %d elements\" %(N_train, N_valid))\n",
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_valid.to_csv(DATA_DIR + \"Categorie_reduit_valid.csv\", index=False)\n",
    "data_train.to_csv(DATA_DIR + \"Categorie_reduit_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nettoyage des données\n",
    "Afin de limiter la dimension de l'espace des variables ou *features*, tout en conservant les informations essentielles, il est nécessaire de nettoyer les données en appliquant plusieurs étapes:\n",
    "* Chaque mot est écrit en minuscule.\n",
    "* Les termes numériques, de ponctuation et autres symboles sont supprimés.\n",
    "* 155 mots-courants, et donc non informatifs, de la langue française sont supprimés (STOPWORDS). Ex: le, la, du, alors, etc...\n",
    "* Chaque mot est \"racinisé\", via la fonction `STEMMER.stem` de la librairie nltk. La racinisation transforme un mot en son radical ou sa racine. Par exemple, les mots: cheval, chevaux, chevalier, chevalerie, chevaucher sont tous remplacés par \"cheva\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des librairies et fichier pour le nettoyage des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Librairies \n",
    "from bs4 import BeautifulSoup #Nettoyage d'HTML\n",
    "import re # Regex\n",
    "import nltk # Nettoyage des données\n",
    "\n",
    "## listes de mots à supprimer dans la description des produits\n",
    "## Depuis NLTK\n",
    "nltk_stopwords = nltk.corpus.stopwords.words('french') \n",
    "## Depuis Un fichier externe.\n",
    "lucene_stopwords =open(DATA_DIR+\"lucene_stopwords.txt\",\"r\").read().split(\",\") #En local\n",
    "## Union des deux fichiers de stopwords \n",
    "stopwords = list(set(nltk_stopwords).union(set(lucene_stopwords)))\n",
    "\n",
    "## Fonction de setmming de stemming permettant la racinisation\n",
    "stemmer=nltk.stem.SnowballStemmer('french')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction de nettoyage de texte\n",
    "Fonction qui prend en intrée un texte et retourne le texte nettoyé en appliquant successivement les étapes suivantes: Nettoyage des données HTML, conversion en texte minuscule, encodage uniforme, suppression des caractéres non alpha numérique (ponctuations), suppression des stopwords, racinisation de chaque mot individuellement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fonction clean générale\n",
    "def clean_txt(txt):\n",
    "    ### remove html stuff\n",
    "    txt = BeautifulSoup(txt,\"html.parser\",from_encoding='utf-8').get_text()\n",
    "    ### lower case\n",
    "    txt = txt.lower()\n",
    "    ### special escaping character '...'\n",
    "    txt = txt.replace(u'\\u2026','.')\n",
    "    txt = txt.replace(u'\\u00a0',' ')\n",
    "    ### remove accent btw\n",
    "    txt = unicodedata.normalize('NFD', txt).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    ###txt = unidecode(txt)\n",
    "    ### remove non alphanumeric char\n",
    "    txt = re.sub('[^a-z_]', ' ', txt)\n",
    "    ### remove french stop words\n",
    "    tokens = [w for w in txt.split() if (len(w)>2) and (w not in stopwords)]\n",
    "    ### french stemming\n",
    "    tokens_stem = [stemmer.stem(token) for token in tokens]\n",
    "    ### tokens = stemmer.stemWords(tokens)\n",
    "    return ' '.join(tokens), \" \".join(tokens_stem)\n",
    "\n",
    "def clean_marque(txt):\n",
    "    txt = re.sub('[^a-zA-Z0-9]', '_', txt).lower()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage des DataFrames\n",
    "Applique le nettoyage sur toutes les lignes de la DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fonction de nettoyage du fichier(stemming et liste de mots à supprimer)\n",
    "def clean_df(input_data, column_names= ['Description', 'Libelle', 'Marque']):\n",
    "    #Test if columns entry match columns names of input data\n",
    "    column_names_diff= set(column_names).difference(set(input_data.columns))\n",
    "    if column_names_diff:\n",
    "        warnings.warn(\"Column(s) '\"+\", \".join(list(column_names_diff)) +\"' do(es) not match columns of input data\", Warning)\n",
    "    \n",
    "    nb_line = input_data.shape[0]\n",
    "    print(\"Start Clean %d lines\" %nb_line)\n",
    "    \n",
    "    # Cleaning start for each columns\n",
    "    time_start = time.time()\n",
    "    clean_list=[]\n",
    "    clean_stem_list=[]\n",
    "    for column_name in column_names:\n",
    "        column = input_data[column_name].values\n",
    "        if column_name == \"Marque\":\n",
    "            array_clean = np.array(list(map(clean_marque,column)))\n",
    "        else:\n",
    "            A = np.array(list(map(clean_txt,column)))\n",
    "            array_clean = A[:,0]\n",
    "            array_clean_stem = A[:,1]\n",
    "        clean_list.append(array_clean)\n",
    "        clean_stem_list.append(array_clean_stem)\n",
    "    time_end = time.time()\n",
    "    print(\"Cleaning time: %d secondes\"%(time_end-time_start))\n",
    "    \n",
    "    #Convert list to DataFrame\n",
    "    array_clean = np.array(clean_list).T\n",
    "    data_clean = pd.DataFrame(array_clean, columns = column_names)\n",
    "    \n",
    "    array_clean_stem = np.array(clean_stem_list).T\n",
    "    data_clean_stem = pd.DataFrame(array_clean_stem, columns = column_names)\n",
    "    return data_clean, data_clean_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Clean 100000 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bguillouet/anaconda/envs/TPInsa/lib/python3.5/site-packages/bs4/__init__.py:146: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
      "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n",
      "/Users/bguillouet/anaconda/envs/TPInsa/lib/python3.5/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning time: 91 secondes\n"
     ]
    }
   ],
   "source": [
    "# Take approximately 2 minutes fors 100.000 rows\n",
    "data_valid_clean, data_valid_clean_stem = clean_df(data_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 'batterie pavilion mah ion batterie pavilion mah ioncaracteri voir presentation',\n",
       "        'batterie pavilion mah', 'hp'], dtype=object),\n",
       " array(['batter pavilion mah ion batter pavilion mah ioncaracter voir present',\n",
       "        'batter pavilion mah', 'batter pavilion mah'], dtype=object))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_valid_clean.values[0], data_valid_clean_stem.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Clean 900000 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bguillouet/anaconda/envs/TPInsa/lib/python3.5/site-packages/bs4/__init__.py:146: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
      "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n",
      "/Users/bguillouet/anaconda/envs/TPInsa/lib/python3.5/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/Users/bguillouet/anaconda/envs/TPInsa/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.sportsdepot.fr/SPORTS_DEPOT/pics/part/7\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/bguillouet/anaconda/envs/TPInsa/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.ecopresto.com/images/produits-ecologiqu\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/bguillouet/anaconda/envs/TPInsa/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.bijouterie-blanc-et-or.com/images/store\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning time: 73220 secondes\n"
     ]
    }
   ],
   "source": [
    "data_train_clean, data_train_clean_stem = clean_df(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affiche les 5 premières lignes de la DataFrame d'apprentissage après nettoyage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Libelle</th>\n",
       "      <th>Marque</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fermeture boutonnee maille fine toucher doux b...</td>\n",
       "      <td>gilet col cachemire</td>\n",
       "      <td>aucune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coque souple rose htc one mini motif love bang...</td>\n",
       "      <td>coque souple rose htc one mini motif lov</td>\n",
       "      <td>muzzano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coque souple grise wiko stairway motif drapeau...</td>\n",
       "      <td>coque souple grise wiko stairway motif dra</td>\n",
       "      <td>muzzano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>samsung galaxy batterie haute densite origine ...</td>\n",
       "      <td>samsung galaxy batterie haute den</td>\n",
       "      <td>samsung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>batterie sony vaio vgn ion mah noir compatible...</td>\n",
       "      <td>batterie sony vaio vgn</td>\n",
       "      <td>aucune</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Description  \\\n",
       "0  fermeture boutonnee maille fine toucher doux b...   \n",
       "1  coque souple rose htc one mini motif love bang...   \n",
       "2  coque souple grise wiko stairway motif drapeau...   \n",
       "3  samsung galaxy batterie haute densite origine ...   \n",
       "4  batterie sony vaio vgn ion mah noir compatible...   \n",
       "\n",
       "                                      Libelle   Marque  \n",
       "0                         gilet col cachemire   aucune  \n",
       "1    coque souple rose htc one mini motif lov  muzzano  \n",
       "2  coque souple grise wiko stairway motif dra  muzzano  \n",
       "3           samsung galaxy batterie haute den  samsung  \n",
       "4                      batterie sony vaio vgn   aucune  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_clean.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Libelle</th>\n",
       "      <th>Marque</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fermetur boutonne maill fin touch doux bord co...</td>\n",
       "      <td>gilet col cachemir</td>\n",
       "      <td>gilet col cachemir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coqu soupl ros htc one min motif lov bangkok f...</td>\n",
       "      <td>coqu soupl ros htc one min motif lov</td>\n",
       "      <td>coqu soupl ros htc one min motif lov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coqu soupl gris wiko stairway motif drapeau is...</td>\n",
       "      <td>coqu soupl gris wiko stairway motif dra</td>\n",
       "      <td>coqu soupl gris wiko stairway motif dra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>samsung galaxy batter haut densit origin samsu...</td>\n",
       "      <td>samsung galaxy batter haut den</td>\n",
       "      <td>samsung galaxy batter haut den</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>batter sony vaio vgn ion mah noir compatibl ba...</td>\n",
       "      <td>batter sony vaio vgn</td>\n",
       "      <td>batter sony vaio vgn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Description  \\\n",
       "0  fermetur boutonne maill fin touch doux bord co...   \n",
       "1  coqu soupl ros htc one min motif lov bangkok f...   \n",
       "2  coqu soupl gris wiko stairway motif drapeau is...   \n",
       "3  samsung galaxy batter haut densit origin samsu...   \n",
       "4  batter sony vaio vgn ion mah noir compatibl ba...   \n",
       "\n",
       "                                   Libelle  \\\n",
       "0                       gilet col cachemir   \n",
       "1     coqu soupl ros htc one min motif lov   \n",
       "2  coqu soupl gris wiko stairway motif dra   \n",
       "3           samsung galaxy batter haut den   \n",
       "4                     batter sony vaio vgn   \n",
       "\n",
       "                                    Marque  \n",
       "0                       gilet col cachemir  \n",
       "1     coqu soupl ros htc one min motif lov  \n",
       "2  coqu soupl gris wiko stairway motif dra  \n",
       "3           samsung galaxy batter haut den  \n",
       "4                     batter sony vaio vgn  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_clean_stem.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252126\n",
      "['fermeture', 'boutonnee', 'maille', 'fine', 'toucher', 'doux', 'bords', 'coteles', 'cachemire', 'soie']\n"
     ]
    }
   ],
   "source": [
    "concatenate_text = \" \".join(data_train_clean[\"Description\"].values)\n",
    "list_of_word = concatenate_text.split(\" \")\n",
    "N = len(set(list_of_word))\n",
    "print(N)\n",
    "print(list_of_word[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202289\n",
      "['fermetur', 'boutonne', 'maill', 'fin', 'touch', 'doux', 'bord', 'cotel', 'cachemir', 'soi']\n"
     ]
    }
   ],
   "source": [
    "concatenate_text = \" \".join(data_train_clean_stem[\"Description\"].values)\n",
    "list_of_word_stem = concatenate_text.split(\" \")\n",
    "N = len(set(list_of_word_stem))\n",
    "print(N)\n",
    "print(list_of_word_stem[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save valide and train clean DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_valid_clean.to_csv(DATA_DIR + \"Categorie_reduit_valid_clean.csv\", index=False)\n",
    "data_train_clean.to_csv(DATA_DIR + \"Categorie_reduit_train_clean.csv\", index=False)\n",
    "\n",
    "data_valid_clean_stem.to_csv(DATA_DIR + \"Categorie_reduit_valid_clean_stem.csv\", index=False)\n",
    "data_train_clean_stem.to_csv(DATA_DIR + \"Categorie_reduit_train_clean_stem.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CC = collections.Counter(list_of_word)\n",
    "CC_stem = collections.Counter(list_of_word_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(247, 246712)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CC[\"coqu\"],CC[\"coque\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248137, 11)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CC_stem[\"coqu\"], CC_stem[\"coque\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'homm'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [TPInsa]",
   "language": "python",
   "name": "Python [TPInsa]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "nav_menu": {
    "height": "279px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
