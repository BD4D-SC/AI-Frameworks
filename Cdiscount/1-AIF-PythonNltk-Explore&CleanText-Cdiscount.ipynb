{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies de l'intelligence Artificielles](https://github.com/wikistat/AI-Frameworks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 150px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" width=400,  style=\"float:right;  display: inline\" alt=\"IMT\"/> </a>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement Naturel du Langage (NLP) - Partie 1 : Exploration et Nettoyage de données textuelles\n",
    "\n",
    "Dans ce premier notebook nous verrons différent traitements généralement opérés sur des données textuelles :\n",
    "\n",
    "* **Nettoyage** : Suppression des caractères mal codés et de ponctuation, transformation des majuscules en minuscules, en remarquant que ces transformations ne seraient pas pertinentes pour un objectif de détection de pourriels.\n",
    "* **StopWord** : Suppression des mots inutiles ou mots de liaison, articles qui n'ont a priori pas de pouvoir discriminant.\n",
    "* **Stemming** (ou Racinisation): Les mots sont réduits à leur seule racine afin de réduire la taille du dictionnaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brendan.guillouet/anaconda3/envs/DEEL/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Importation des librairies utilisées\n",
    "import unicodedata \n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "import collections\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set_style(\"whitegrid\")\n",
    "\n",
    "import sklearn.cross_validation as scv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nltk**\n",
    "\n",
    "Si vous utilisez la librairie `nltk` pour la première fois, il est nécessaire d'utiliser la commande suivante. Cette commande permet de télécharger de nombreux corpus de texte, mais également des informations grammaticales sur différentes langues. Information notamment nécessaire à l'étape de racinisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les données\n",
    "\n",
    "* `cdiscount_train`: 1.000.000 de lignes\n",
    "* `cdisount_test`: 50.000 lignes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Read & Split Dataset\n",
    "   On définit une fonction permettant de lire le fichier d'apprentissage et de créer deux DataFrame Pandas, un pour l'apprentissage, l'autre pour la validation.\n",
    "   \n",
    "   La fonction créée un DataFrame en lisant entièrement le fichier. Puis elle scinde ce DataFrame en deux grâce à la fonction dédiée de sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_dataset(input_path, nb_line, tauxValid):\n",
    "    data_all = pd.read_csv(input_path,sep=\",\", nrows=nb_line)\n",
    "    data_all = data_all.fillna(\"\")\n",
    "    data_train, data_valid = scv.train_test_split(data_all, test_size = tauxValid)\n",
    "    time_end = time.time()\n",
    "    return data_train, data_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"data/cdiscount_train.csv.zip\"\n",
    "nb_line=100000  # part totale extraite du fichier initial ici déjà réduit\n",
    "tauxValid = 0.05\n",
    "data_train, data_valid = split_dataset(input_path, nb_line, tauxValid)\n",
    "# Cette ligne permet de visualiser les 5 premières lignes de la DataFrame \n",
    "N_train = data_train.shape[0]\n",
    "N_valid = data_valid.shape[0]\n",
    "print(\"Train set : %d elements, Test set : %d elements\" %(N_train, N_valid))\n",
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.groupby(\"Categorie1\").first()[[\"Description\",\"Marque\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count occurence of each Categorie\n",
    "data_count = data_train[\"Categorie1\"].value_counts()\n",
    "#Rename index to add percentage\n",
    "new_index = [k+ \": %.2f%%\" %(v*100/N_train) for k,v in data_count.iteritems()]\n",
    "data_count.index= new_index\n",
    "\n",
    "fig=plt.figure(figsize= (10,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "data_count.plot.barh(logx = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que peut-on dire sur la distribution des ce classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarde des données\n",
    "\n",
    "On sauvegarde dans des csv les fichiers `train` et `validation` afin que ces mêmes fichiers soit ré-utilisés plus tard dans d'autre calepin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_valid.to_csv(\"data/cdiscount_valid.csv\", index=False)\n",
    "data_train.to_csv(\"data/cdiscount_train_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Nettoyage des données\n",
    "Afin de limiter la dimension de l'espace des variables ou *features*, tout en conservant les informations essentielles, il est nécessaire de nettoyer les données en appliquant plusieurs étapes:\n",
    "* Chaque mot est écrit en minuscule.\n",
    "* Les termes numériques, de ponctuation et autres symboles sont supprimés.\n",
    "* 155 mots-courants, et donc non informatifs, de la langue française sont supprimés (STOPWORDS). Ex: le, la, du, alors, etc...\n",
    "* Chaque mot est \"racinisé\", via la fonction `STEMMER.stem` de la librairie nltk. La racinisation transforme un mot en son radical ou sa racine. Par exemple, les mots: cheval, chevaux, chevalier, chevalerie, chevaucher sont tous remplacés par \"cheva\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des librairies et fichier pour le nettoyage des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Librairies \n",
    "from bs4 import BeautifulSoup #Nettoyage d'HTML\n",
    "import re # Regex\n",
    "import nltk # Nettoyage des données\n",
    "\n",
    "## listes de mots à supprimer dans la description des produits\n",
    "## Depuis NLTK\n",
    "nltk_stopwords = nltk.corpus.stopwords.words('french') \n",
    "## Depuis Un fichier externe.\n",
    "lucene_stopwords =open(\"data/lucene_stopwords.txt\",\"r\").read().split(\",\") #En local\n",
    "## Union des deux fichiers de stopwords \n",
    "stopwords = list(set(nltk_stopwords).union(set(lucene_stopwords)))\n",
    "\n",
    "## Fonction de setmming de stemming permettant la racinisation\n",
    "stemmer=nltk.stem.SnowballStemmer('french')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple \n",
    "\n",
    "**Ligne Originale**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "description = data_train.Description.values[i]\n",
    "print(\"Original Description : \" + description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Supprime les posibles balises HTML dans la description **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = BeautifulSoup(description,\"html.parser\",from_encoding='utf-8').get_text()\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Convertit le texte en minuscule **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = txt.lower()\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Remplace quelques caractères spéciaux **\n",
    "\n",
    "* `\\u2026`: `…`\n",
    "* `\\u00a0`: `NO-BREAK SPACE`\n",
    "\n",
    "Cette liste peut être etayer en fonction du jeu de donées étudiés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = txt.replace(u'\\u2026','.')    \n",
    "txt = txt.replace(u'\\u00a0',' ')\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Supprime les accents **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = unicodedata.normalize('NFD', txt).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Supprime les caractères qui ne sont ne sont pas des lettres minuscules **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = re.sub('[^a-z_]', ' ', txt)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Remplace la description par une liste de mots (tokens), supprime les mots de moins de 2 lettres ainsi que les stopwords **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [w for w in txt.split() if (len(w)>2) and (w not in stopwords)]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Racinise (Stem) chaque tokens **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_stem = [stemmer.stem(token) for token in tokens]\n",
    "print(tokens_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction de nettoyage de texte\n",
    "Fonction qui prend en entrée un texte et retourne le texte nettoyé en appliquant successivement les étapes présentés précedemment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fonction clean générale\n",
    "def clean_txt(txt):\n",
    "    ### remove html stuff\n",
    "    txt = BeautifulSoup(txt,\"html.parser\",from_encoding='utf-8').get_text()\n",
    "    ### lower case\n",
    "    txt = txt.lower()\n",
    "    ### special escaping character '...'\n",
    "    txt = txt.replace(u'\\u2026','.')\n",
    "    txt = txt.replace(u'\\u00a0',' ')\n",
    "    ### remove accent btw\n",
    "    txt = unicodedata.normalize('NFD', txt).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    ###txt = unidecode(txt)\n",
    "    ### remove non alphanumeric char\n",
    "    txt = re.sub('[^a-z_]', ' ', txt)\n",
    "    ### remove french stop words\n",
    "    tokens = [w for w in txt.split() if (len(w)>2) and (w not in stopwords)]\n",
    "    ### french stemming\n",
    "    tokens_stem = [stemmer.stem(token) for token in tokens]\n",
    "    ### tokens = stemmer.stemWords(tokens)\n",
    "    return ' '.join(tokens), \" \".join(tokens_stem)\n",
    "\n",
    "def clean_marque(txt):\n",
    "    txt = re.sub('[^a-zA-Z0-9]', '_', txt).lower()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applique le nettoyage sur toutes les lignes de la DataFrame et créé deux nouvelles Dataframe (avant et sans l'étape de racinisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "p = Pool(4)\n",
    "    \n",
    "# fonction de nettoyage du fichier(stemming et liste de mots à supprimer)\n",
    "def clean_df(input_data, column_names= ['Description', 'Libelle', 'Marque']):\n",
    "\n",
    "    nb_line = input_data.shape[0]\n",
    "    print(\"Start Clean %d lines\" %nb_line)\n",
    "    \n",
    "    # Cleaning start for each columns\n",
    "    time_start = time.time()\n",
    "    clean_list=[]\n",
    "    clean_stem_list=[]\n",
    "    for column_name in column_names:\n",
    "        column = input_data[column_name].values\n",
    "        if column_name == \"Marque\":\n",
    "            array_clean = np.array(list(p.map(clean_marque,column)))\n",
    "            clean_list.append(array_clean)\n",
    "            clean_stem_list.append(array_clean)\n",
    "        else:\n",
    "            A = np.array(list(p.map(clean_txt,column)))\n",
    "            array_clean = A[:,0]\n",
    "            array_clean_stem = A[:,1]\n",
    "            clean_list.append(array_clean)\n",
    "            clean_stem_list.append(array_clean_stem)\n",
    "    time_end = time.time()\n",
    "    print(\"Cleaning time: %d secondes\"%(time_end-time_start))\n",
    "    \n",
    "    #Convert list to DataFrame\n",
    "    array_clean = np.array(clean_list).T\n",
    "    data_clean = pd.DataFrame(array_clean, columns = column_names)\n",
    "    \n",
    "    array_clean_stem = np.array(clean_stem_list).T\n",
    "    data_clean_stem = pd.DataFrame(array_clean_stem, columns = column_names)\n",
    "    return data_clean, data_clean_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage des DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take approximately 2 minutes fors 100.000 rows\n",
    "data_valid_clean, data_valid_clean_stem = clean_df(data_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_clean, data_train_clean_stem = clean_df(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affiche les 5 premières lignes de la DataFrame d'apprentissage après nettoyage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_clean.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_clean_stem.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taille du dictionnaire de mots pour le dataset avant et après la racinisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_text = \" \".join(data_train_clean[\"Description\"].values)\n",
    "list_of_word = concatenate_text.split(\" \")\n",
    "N = len(set(list_of_word))\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_text = \" \".join(data_train_clean_stem[\"Description\"].values)\n",
    "list_of_word_stem = concatenate_text.split(\" \")\n",
    "N = len(set(list_of_word_stem))\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_descr = \" \".join(data_valid.Description.values)\n",
    "wordcloud_word = WordCloud(background_color=\"black\").generate_from_text(all_descr)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud_word,cmap=plt.cm.Paired)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_descr_clean_stem = \" \".join(data_valid_clean_stem.Description.values)\n",
    "wordcloud_word = WordCloud(background_color=\"black\").generate_from_text(all_descr_clean_stem)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud_word,cmap=plt.cm.Paired)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde des jeux de données nettoyés dans des fichiers csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_valid_clean.to_csv(\"data/cdiscount_valid_clean.csv\", index=False)\n",
    "data_train_clean.to_csv(\"data/cdiscount_train_clean.csv\", index=False)\n",
    "\n",
    "data_valid_clean_stem.to_csv(\"data/cdiscount_valid_clean_stem.csv\", index=False)\n",
    "data_train_clean_stem.to_csv(\"data/cdiscount_train_clean_stem.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Environment (conda_deel)",
   "language": "python",
   "name": "conda_deel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {
    "height": "279px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
